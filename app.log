INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
INFO:werkzeug:[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
INFO:werkzeug:[33mPress CTRL+C to quit[0m
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 524-771-621
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:12:38 KST)" (scheduled at 2025-01-14 16:12:38.458910+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:12:43 KST)" (scheduled at 2025-01-14 16:12:43.457574+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:12:58 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:werkzeug: * Detected change in 'C:\\Users\\soldesk\\AppData\\Roaming\\Python\\Python310\\site-packages\\apscheduler\\executors\\base.py', reloading
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:13:03 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:13:18 KST)" (scheduled at 2025-01-14 16:12:58.458910+09:00)
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 524-771-621
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:13:18 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:13:38 KST)" (scheduled at 2025-01-14 16:13:18.458910+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:13:20 KST)" (scheduled at 2025-01-14 16:13:20.208753+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:13:38 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:13:40 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:13:58 KST)" (scheduled at 2025-01-14 16:13:38.458910+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:14:00 KST)" (scheduled at 2025-01-14 16:13:40.208753+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:13:58 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:14:00 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:14:18 KST)" (scheduled at 2025-01-14 16:13:58.458910+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:14:20 KST)" (scheduled at 2025-01-14 16:14:00.208753+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:14:18 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:14:20 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:14:38 KST)" (scheduled at 2025-01-14 16:14:18.458910+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:14:40 KST)" (scheduled at 2025-01-14 16:14:20.208753+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:14:38 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:14:40 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:14:58 KST)" (scheduled at 2025-01-14 16:14:38.458910+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:15:00 KST)" (scheduled at 2025-01-14 16:14:40.208753+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:15:00 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:14:58 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:15:18 KST)" (scheduled at 2025-01-14 16:14:58.458910+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:15:20 KST)" (scheduled at 2025-01-14 16:15:00.208753+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:15:18 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:15:20 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:15:38 KST)" (scheduled at 2025-01-14 16:15:18.458910+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:15:40 KST)" (scheduled at 2025-01-14 16:15:20.208753+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:15:38 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:15:40 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:15:58 KST)" (scheduled at 2025-01-14 16:15:38.458910+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:16:00 KST)" (scheduled at 2025-01-14 16:15:40.208753+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:15:58 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:16:00 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:16:18 KST)" (scheduled at 2025-01-14 16:15:58.458910+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:16:20 KST)" (scheduled at 2025-01-14 16:16:00.208753+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:16:18 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:16:20 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:16:38 KST)" (scheduled at 2025-01-14 16:16:18.458910+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:16:40 KST)" (scheduled at 2025-01-14 16:16:20.208753+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:16:38 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:16:40 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:16:58 KST)" (scheduled at 2025-01-14 16:16:38.458910+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:17:00 KST)" (scheduled at 2025-01-14 16:16:40.208753+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:16:58 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:17:00 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:17:18 KST)" (scheduled at 2025-01-14 16:16:58.458910+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:17:20 KST)" (scheduled at 2025-01-14 16:17:00.208753+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:17:18 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:17:20 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:17:38 KST)" (scheduled at 2025-01-14 16:17:18.458910+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:17:40 KST)" (scheduled at 2025-01-14 16:17:20.208753+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:17:38 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:17:40 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:17:58 KST)" (scheduled at 2025-01-14 16:17:38.458910+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:18:00 KST)" (scheduled at 2025-01-14 16:17:40.208753+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:17:58 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:18:00 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:18:18 KST)" (scheduled at 2025-01-14 16:17:58.458910+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:18:20 KST)" (scheduled at 2025-01-14 16:18:00.208753+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:18:18 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:18:20 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:18:38 KST)" (scheduled at 2025-01-14 16:18:18.458910+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:18:40 KST)" (scheduled at 2025-01-14 16:18:20.208753+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:18:38 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:18:40 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\flask\\sessions.py', reloading
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\flask\\json\\tag.py', reloading
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\flask\\typing.py', reloading
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\werkzeug\\wrappers\\__init__.py', reloading
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\werkzeug\\wrappers\\response.py', reloading
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\werkzeug\\sansio\\response.py', reloading
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 524-771-621
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:18:58 KST)" (scheduled at 2025-01-14 16:18:38.458910+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:18:58 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:18:54 KST)" (scheduled at 2025-01-14 16:18:54.875978+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:19:18 KST)" (scheduled at 2025-01-14 16:18:58.458910+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:19:14 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:19:18 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:19:34 KST)" (scheduled at 2025-01-14 16:19:14.875978+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:19:38 KST)" (scheduled at 2025-01-14 16:19:18.458910+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:19:34 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:19:38 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:19:54 KST)" (scheduled at 2025-01-14 16:19:34.875978+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:19:58 KST)" (scheduled at 2025-01-14 16:19:38.458910+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:19:54 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:19:58 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:20:14 KST)" (scheduled at 2025-01-14 16:19:54.875978+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:20:18 KST)" (scheduled at 2025-01-14 16:19:58.458910+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:20:14 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:20:18 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:20:34 KST)" (scheduled at 2025-01-14 16:20:14.875978+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:20:38 KST)" (scheduled at 2025-01-14 16:20:18.458910+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:20:38 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:20:34 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:20:54 KST)" (scheduled at 2025-01-14 16:20:34.875978+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:20:58 KST)" (scheduled at 2025-01-14 16:20:38.458910+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:20:58 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:20:54 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:21:14 KST)" (scheduled at 2025-01-14 16:20:54.875978+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:21:18 KST)" (scheduled at 2025-01-14 16:20:58.458910+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:21:14 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:21:18 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:21:34 KST)" (scheduled at 2025-01-14 16:21:14.875978+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:21:34 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:21:38 KST)" (scheduled at 2025-01-14 16:21:18.458910+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:21:38 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:21:54 KST)" (scheduled at 2025-01-14 16:21:34.875978+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:21:54 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:21:58 KST)" (scheduled at 2025-01-14 16:21:38.458910+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:21:58 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:22:14 KST)" (scheduled at 2025-01-14 16:21:54.875978+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:22:18 KST)" (scheduled at 2025-01-14 16:21:58.458910+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:22:14 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:22:18 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:22:34 KST)" (scheduled at 2025-01-14 16:22:14.875978+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:22:38 KST)" (scheduled at 2025-01-14 16:22:18.458910+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:22:34 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:22:38 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:werkzeug: * Detected change in 'C:\\kd\\ws_java\\team4_v2sbm3c\\src\\main\\python\\pernews.py', reloading
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\logging\\__init__.py', reloading
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\sqlalchemy\\sql\\__init__.py', reloading
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\sqlalchemy\\sql\\expression.py', reloading
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\sqlalchemy\\sql\\selectable.py', reloading
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\sqlalchemy\\sql\\roles.py', reloading
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\sqlalchemy\\sql\\elements.py', reloading
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\sqlalchemy\\sql\\annotation.py', reloading
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\sqlalchemy\\sql\\traversals.py', reloading
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\sqlalchemy\\sql\\visitors.py', reloading
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\sqlalchemy\\inspection.py', reloading
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\sqlalchemy\\util\\_collections.py', reloading
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\sqlalchemy\\sql\\base.py', reloading
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\sqlalchemy\\orm\\__init__.py', reloading
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\sqlalchemy\\orm\\query.py', reloading
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\sqlalchemy\\sql\\operators.py', reloading
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\sqlalchemy\\engine\\interfaces.py', reloading
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\sqlalchemy\\exc.py', reloading
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:22:58 KST)" (scheduled at 2025-01-14 16:22:38.458910+09:00)
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 524-771-621
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:22:58 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:23:18 KST)" (scheduled at 2025-01-14 16:22:58.458910+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:22:59 KST)" (scheduled at 2025-01-14 16:22:59.955274+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:23:18 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:23:19 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:23:38 KST)" (scheduled at 2025-01-14 16:23:18.458910+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:23:39 KST)" (scheduled at 2025-01-14 16:23:19.955274+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:23:38 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:23:39 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:23:58 KST)" (scheduled at 2025-01-14 16:23:38.458910+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:23:59 KST)" (scheduled at 2025-01-14 16:23:39.955274+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:23:58 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:23:59 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:24:18 KST)" (scheduled at 2025-01-14 16:23:58.458910+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:24:19 KST)" (scheduled at 2025-01-14 16:23:59.955274+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:24:18 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:24:19 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:24:38 KST)" (scheduled at 2025-01-14 16:24:18.458910+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:24:39 KST)" (scheduled at 2025-01-14 16:24:19.955274+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:24:38 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:24:39 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:werkzeug: * Detected change in 'C:\\Users\\soldesk\\AppData\\Roaming\\Python\\Python310\\site-packages\\openai\\__init__.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\soldesk\\AppData\\Roaming\\Python\\Python310\\site-packages\\openai\\lib\\_old_api.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\soldesk\\AppData\\Roaming\\Python\\Python310\\site-packages\\openai\\_client.py', reloading
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\sqlalchemy\\__init__.py', reloading
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\sqlalchemy\\engine\\__init__.py', reloading
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\sqlalchemy\\engine\\create.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\soldesk\\AppData\\Roaming\\Python\\Python310\\site-packages\\selenium\\webdriver\\common\\by.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\soldesk\\AppData\\Roaming\\Python\\Python310\\site-packages\\apscheduler\\schedulers\\background.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\soldesk\\AppData\\Roaming\\Python\\Python310\\site-packages\\selenium\\webdriver\\remote\\webelement.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\soldesk\\AppData\\Roaming\\Python\\Python310\\site-packages\\apscheduler\\schedulers\\blocking.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\soldesk\\AppData\\Roaming\\Python\\Python310\\site-packages\\apscheduler\\schedulers\\base.py', reloading
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:24:58 KST)" (scheduled at 2025-01-14 16:24:38.458910+09:00)
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:24:58 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 524-771-621
INFO:werkzeug: * Detected change in 'C:\\kd\\ws_java\\team4_v2sbm3c\\src\\main\\python\\crawling.py', reloading
INFO:werkzeug: * Detected change in 'C:\\kd\\ws_java\\team4_v2sbm3c\\src\\main\\python\\crawling.py', reloading
INFO:werkzeug: * Detected change in 'C:\\kd\\ws_java\\team4_v2sbm3c\\src\\main\\python\\crawling.py', reloading
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:25:18 KST)" (scheduled at 2025-01-14 16:24:58.458910+09:00)
INFO:WDM:====== WebDriver manager ======
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:25:18 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 524-771-621
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
INFO:werkzeug:[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
INFO:werkzeug:[33mPress CTRL+C to quit[0m
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 524-771-621
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:25:43 KST)" (scheduled at 2025-01-14 16:25:43.154593+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:25:48 KST)" (scheduled at 2025-01-14 16:25:48.340187+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:26:03 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:26:08 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:26:23 KST)" (scheduled at 2025-01-14 16:26:03.154593+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:26:23 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:26:28 KST)" (scheduled at 2025-01-14 16:26:08.340187+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:26:28 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:26:43 KST)" (scheduled at 2025-01-14 16:26:23.154593+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:26:43 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:26:48 KST)" (scheduled at 2025-01-14 16:26:28.340187+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:26:48 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:27:03 KST)" (scheduled at 2025-01-14 16:26:43.154593+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:27:03 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:27:08 KST)" (scheduled at 2025-01-14 16:26:48.340187+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:27:08 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:27:23 KST)" (scheduled at 2025-01-14 16:27:03.154593+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:27:23 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:27:28 KST)" (scheduled at 2025-01-14 16:27:08.340187+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:27:28 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:27:43 KST)" (scheduled at 2025-01-14 16:27:23.154593+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:27:43 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:27:48 KST)" (scheduled at 2025-01-14 16:27:28.340187+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:27:48 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:28:03 KST)" (scheduled at 2025-01-14 16:27:43.154593+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:28:08 KST)" (scheduled at 2025-01-14 16:27:48.340187+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:28:03 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:28:08 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:28:23 KST)" (scheduled at 2025-01-14 16:28:03.154593+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:28:23 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:28:28 KST)" (scheduled at 2025-01-14 16:28:08.340187+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:28:28 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:28:43 KST)" (scheduled at 2025-01-14 16:28:23.154593+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:28:43 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:28:48 KST)" (scheduled at 2025-01-14 16:28:28.340187+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:28:48 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:29:03 KST)" (scheduled at 2025-01-14 16:28:43.154593+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:29:08 KST)" (scheduled at 2025-01-14 16:28:48.340187+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:29:08 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:29:03 KST)" skipped: maximum number of running instances reached (1)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:29:28 KST)" (scheduled at 2025-01-14 16:29:08.340187+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:29:28 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Scripts\\pip-script.py', reloading
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\pip\\_vendor\\pyparsing\\core.py', reloading
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\pip\\_vendor\\packaging\\markers.py', reloading
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:29:23 KST)" skipped: maximum number of running instances reached (1)
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 524-771-621
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:29:43 KST)" skipped: maximum number of running instances reached (1)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:29:51 KST)" (scheduled at 2025-01-14 16:29:51.913887+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:30:11 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:30:03 KST)" skipped: maximum number of running instances reached (1)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:30:31 KST)" (scheduled at 2025-01-14 16:30:11.913887+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:30:31 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:30:23 KST)" skipped: maximum number of running instances reached (1)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
INFO:werkzeug:[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
INFO:werkzeug:[33mPress CTRL+C to quit[0m
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 524-771-621
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:30:57 KST)" (scheduled at 2025-01-14 16:30:57.598397+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:31:03 KST)" (scheduled at 2025-01-14 16:31:03.202562+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:31:17 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:31:23 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:31:37 KST)" (scheduled at 2025-01-14 16:31:17.598397+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:31:37 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:31:43 KST)" (scheduled at 2025-01-14 16:31:23.202562+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:31:43 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:31:57 KST)" (scheduled at 2025-01-14 16:31:37.598397+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:31:57 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:32:03 KST)" (scheduled at 2025-01-14 16:31:43.202562+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:32:03 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:32:17 KST)" (scheduled at 2025-01-14 16:31:57.598397+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:32:17 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:32:23 KST)" (scheduled at 2025-01-14 16:32:03.202562+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:32:23 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:32:37 KST)" (scheduled at 2025-01-14 16:32:17.598397+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:32:37 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:32:43 KST)" (scheduled at 2025-01-14 16:32:23.202562+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:32:43 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:32:57 KST)" (scheduled at 2025-01-14 16:32:37.598397+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:32:57 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:33:03 KST)" (scheduled at 2025-01-14 16:32:43.202562+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:33:03 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:33:17 KST)" (scheduled at 2025-01-14 16:32:57.598397+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:33:17 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:33:23 KST)" (scheduled at 2025-01-14 16:33:03.202562+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:33:23 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:33:37 KST)" (scheduled at 2025-01-14 16:33:17.598397+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:33:37 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:33:43 KST)" (scheduled at 2025-01-14 16:33:23.202562+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:33:43 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:33:57 KST)" (scheduled at 2025-01-14 16:33:37.598397+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:33:57 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:34:03 KST)" (scheduled at 2025-01-14 16:33:43.202562+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:34:03 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:34:17 KST)" (scheduled at 2025-01-14 16:33:57.598397+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:34:17 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:34:23 KST)" (scheduled at 2025-01-14 16:34:03.202562+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:34:23 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:34:37 KST)" (scheduled at 2025-01-14 16:34:17.598397+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:34:37 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:34:43 KST)" (scheduled at 2025-01-14 16:34:23.202562+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:34:43 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:34:57 KST)" (scheduled at 2025-01-14 16:34:37.598397+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:34:57 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:35:03 KST)" (scheduled at 2025-01-14 16:34:43.202562+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:35:03 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:werkzeug: * Detected change in 'C:\\Users\\soldesk\\AppData\\Roaming\\Python\\Python310\\site-packages\\bs4\\element.py', reloading
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\bs4\\element.py', reloading
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:35:17 KST)" (scheduled at 2025-01-14 16:34:57.598397+09:00)
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:35:17 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 524-771-621
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:35:37 KST)" (scheduled at 2025-01-14 16:35:17.598397+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:35:37 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:35:29 KST)" (scheduled at 2025-01-14 16:35:29.175572+09:00)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
INFO:werkzeug:[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
INFO:werkzeug:[33mPress CTRL+C to quit[0m
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
INFO:werkzeug:[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
INFO:werkzeug:[33mPress CTRL+C to quit[0m
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 524-771-621
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:44:38 KST)" (scheduled at 2025-01-14 16:44:38.243761+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:44:58 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:44:43 KST)" (scheduled at 2025-01-14 16:44:43.655928+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:45:03 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:45:18 KST)" (scheduled at 2025-01-14 16:44:58.243761+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:45:18 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:45:23 KST)" (scheduled at 2025-01-14 16:45:03.655928+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:45:23 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:45:38 KST)" (scheduled at 2025-01-14 16:45:18.243761+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:45:38 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:45:43 KST)" (scheduled at 2025-01-14 16:45:23.655928+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:45:43 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:45:58 KST)" (scheduled at 2025-01-14 16:45:38.243761+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:45:58 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:46:03 KST)" (scheduled at 2025-01-14 16:45:43.655928+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:46:03 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:46:18 KST)" (scheduled at 2025-01-14 16:45:58.243761+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:46:18 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:46:23 KST)" (scheduled at 2025-01-14 16:46:03.655928+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:46:23 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:46:38 KST)" (scheduled at 2025-01-14 16:46:18.243761+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:46:38 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:46:43 KST)" (scheduled at 2025-01-14 16:46:23.655928+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:46:43 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:46:58 KST)" (scheduled at 2025-01-14 16:46:38.243761+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:46:58 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:47:03 KST)" (scheduled at 2025-01-14 16:46:43.655928+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:47:03 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:47:18 KST)" (scheduled at 2025-01-14 16:46:58.243761+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:47:18 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:47:23 KST)" (scheduled at 2025-01-14 16:47:03.655928+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:47:23 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:47:38 KST)" (scheduled at 2025-01-14 16:47:18.243761+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:47:38 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:47:43 KST)" (scheduled at 2025-01-14 16:47:23.655928+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:47:43 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:47:58 KST)" (scheduled at 2025-01-14 16:47:38.243761+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:47:58 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:48:03 KST)" (scheduled at 2025-01-14 16:47:43.655928+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:48:03 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:48:18 KST)" (scheduled at 2025-01-14 16:47:58.243761+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:48:23 KST)" (scheduled at 2025-01-14 16:48:03.655928+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:48:18 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:48:23 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:48:38 KST)" (scheduled at 2025-01-14 16:48:18.243761+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:48:38 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:48:43 KST)" (scheduled at 2025-01-14 16:48:23.655928+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:48:43 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:48:58 KST)" (scheduled at 2025-01-14 16:48:38.243761+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:48:58 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:49:03 KST)" (scheduled at 2025-01-14 16:48:43.655928+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:49:03 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:49:18 KST)" (scheduled at 2025-01-14 16:48:58.243761+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:49:18 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:49:23 KST)" (scheduled at 2025-01-14 16:49:03.655928+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:49:23 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:49:38 KST)" (scheduled at 2025-01-14 16:49:18.243761+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:49:38 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:49:43 KST)" (scheduled at 2025-01-14 16:49:23.655928+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:49:43 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:49:58 KST)" (scheduled at 2025-01-14 16:49:38.243761+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:49:58 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:50:03 KST)" (scheduled at 2025-01-14 16:49:43.655928+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:50:03 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:50:18 KST)" (scheduled at 2025-01-14 16:49:58.243761+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:50:18 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\os.py', reloading
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:50:23 KST)" (scheduled at 2025-01-14 16:50:03.655928+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:50:23 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 524-771-621
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:50:38 KST)" (scheduled at 2025-01-14 16:50:18.243761+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:50:38 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:50:34 KST)" (scheduled at 2025-01-14 16:50:34.193427+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:50:58 KST)" (scheduled at 2025-01-14 16:50:38.243761+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:50:54 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:50:58 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\sqlalchemy\\engine\\mock.py', reloading
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\sqlalchemy\\schema.py', reloading
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\sqlalchemy\\sql\\schema.py', reloading
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\sqlalchemy\\sql\\ddl.py', reloading
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\sqlalchemy\\sql\\functions.py', reloading
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\sqlalchemy\\sql\\lambdas.py', reloading
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\sqlalchemy\\types.py', reloading
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\sqlalchemy\\sql\\sqltypes.py', reloading
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\sqlalchemy\\sql\\type_api.py', reloading
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:51:18 KST)" (scheduled at 2025-01-14 16:50:58.243761+09:00)
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 524-771-621
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:51:18 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:51:38 KST)" (scheduled at 2025-01-14 16:51:18.243761+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:51:38 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:51:23 KST)" (scheduled at 2025-01-14 16:51:23.617375+09:00)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
INFO:werkzeug:[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
INFO:werkzeug:[33mPress CTRL+C to quit[0m
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 524-771-621
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:53:16 KST)" (scheduled at 2025-01-14 16:53:16.935733+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:53:22 KST)" (scheduled at 2025-01-14 16:53:22.490002+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:53:36 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:53:42 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:53:56 KST)" (scheduled at 2025-01-14 16:53:36.935733+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:53:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:54:02 KST)" (scheduled at 2025-01-14 16:53:42.490002+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:54:02 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:54:16 KST)" (scheduled at 2025-01-14 16:53:56.935733+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:54:16 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:54:22 KST)" (scheduled at 2025-01-14 16:54:02.490002+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:54:22 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:54:36 KST)" (scheduled at 2025-01-14 16:54:16.935733+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:54:36 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:54:42 KST)" (scheduled at 2025-01-14 16:54:22.490002+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:54:42 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:54:56 KST)" (scheduled at 2025-01-14 16:54:36.935733+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:54:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:55:02 KST)" (scheduled at 2025-01-14 16:54:42.490002+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:55:02 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:55:16 KST)" (scheduled at 2025-01-14 16:54:56.935733+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:55:16 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:55:22 KST)" (scheduled at 2025-01-14 16:55:02.490002+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:55:22 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:55:36 KST)" (scheduled at 2025-01-14 16:55:16.935733+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:55:36 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:55:42 KST)" (scheduled at 2025-01-14 16:55:22.490002+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:55:42 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:55:56 KST)" (scheduled at 2025-01-14 16:55:36.935733+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:55:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:56:02 KST)" (scheduled at 2025-01-14 16:55:42.490002+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:56:02 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:56:16 KST)" (scheduled at 2025-01-14 16:55:56.935733+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:56:16 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:56:22 KST)" (scheduled at 2025-01-14 16:56:02.490002+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:56:22 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:56:36 KST)" (scheduled at 2025-01-14 16:56:16.935733+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:56:36 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:56:42 KST)" (scheduled at 2025-01-14 16:56:22.490002+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:56:42 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:56:56 KST)" (scheduled at 2025-01-14 16:56:36.935733+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:56:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:57:02 KST)" (scheduled at 2025-01-14 16:56:42.490002+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:57:02 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:57:16 KST)" (scheduled at 2025-01-14 16:56:56.935733+09:00)
INFO:werkzeug: * Detected change in 'C:\\kd\\ws_java\\team4_v2sbm3c\\src\\main\\python\\crawling.py', reloading
INFO:werkzeug: * Detected change in 'C:\\kd\\ws_java\\team4_v2sbm3c\\src\\main\\python\\crawling.py', reloading
INFO:werkzeug: * Restarting with watchdog (windowsapi)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:57:16 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 524-771-621
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:57:36 KST)" (scheduled at 2025-01-14 16:57:16.935733+09:00)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
INFO:werkzeug:[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
INFO:werkzeug:[33mPress CTRL+C to quit[0m
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 524-771-621
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:57:49 KST)" (scheduled at 2025-01-14 16:57:49.750703+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:57:55 KST)" (scheduled at 2025-01-14 16:57:55.547632+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:58:15 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:58:09 KST)" skipped: maximum number of running instances reached (1)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:58:35 KST)" (scheduled at 2025-01-14 16:58:15.547632+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:58:35 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:58:29 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:58:49 KST)" (scheduled at 2025-01-14 16:58:29.750703+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:58:49 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:58:55 KST)" (scheduled at 2025-01-14 16:58:35.547632+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:58:55 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:59:09 KST)" (scheduled at 2025-01-14 16:58:49.750703+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:59:09 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:59:15 KST)" (scheduled at 2025-01-14 16:58:55.547632+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:59:15 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:59:29 KST)" (scheduled at 2025-01-14 16:59:09.750703+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:59:29 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:59:35 KST)" (scheduled at 2025-01-14 16:59:15.547632+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:59:35 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:59:49 KST)" (scheduled at 2025-01-14 16:59:29.750703+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:59:49 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:59:55 KST)" (scheduled at 2025-01-14 16:59:35.547632+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 16:59:55 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:00:09 KST)" (scheduled at 2025-01-14 16:59:49.750703+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:00:09 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:00:15 KST)" (scheduled at 2025-01-14 16:59:55.547632+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:00:15 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:00:29 KST)" (scheduled at 2025-01-14 17:00:09.750703+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:00:29 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:00:35 KST)" (scheduled at 2025-01-14 17:00:15.547632+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:00:35 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:00:49 KST)" (scheduled at 2025-01-14 17:00:29.750703+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:00:49 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:00:55 KST)" (scheduled at 2025-01-14 17:00:35.547632+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:00:55 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:01:09 KST)" (scheduled at 2025-01-14 17:00:49.750703+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:01:09 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:01:15 KST)" (scheduled at 2025-01-14 17:00:55.547632+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:01:15 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:01:29 KST)" (scheduled at 2025-01-14 17:01:09.750703+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:01:29 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:01:35 KST)" (scheduled at 2025-01-14 17:01:15.547632+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:01:35 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:01:49 KST)" (scheduled at 2025-01-14 17:01:29.750703+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:01:49 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:01:55 KST)" (scheduled at 2025-01-14 17:01:35.547632+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:01:55 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:02:09 KST)" (scheduled at 2025-01-14 17:01:49.750703+09:00)
INFO:werkzeug: * Detected change in 'C:\\kd\\ws_java\\team4_v2sbm3c\\src\\main\\python\\crawling.py', reloading
INFO:werkzeug: * Detected change in 'C:\\kd\\ws_java\\team4_v2sbm3c\\src\\main\\python\\crawling.py', reloading
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:02:09 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 524-771-621
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:02:29 KST)" (scheduled at 2025-01-14 17:02:09.750703+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:02:29 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
INFO:werkzeug:[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
INFO:werkzeug:[33mPress CTRL+C to quit[0m
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 524-771-621
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:02:50 KST)" (scheduled at 2025-01-14 17:02:50.800030+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:03:10 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:02:56 KST)" (scheduled at 2025-01-14 17:02:56.538863+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:03:16 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:03:30 KST)" (scheduled at 2025-01-14 17:03:10.800030+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:03:30 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:03:36 KST)" (scheduled at 2025-01-14 17:03:16.538863+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:03:36 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:03:50 KST)" (scheduled at 2025-01-14 17:03:30.800030+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:03:56 KST)" (scheduled at 2025-01-14 17:03:36.538863+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:03:50 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:03:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:04:10 KST)" (scheduled at 2025-01-14 17:03:50.800030+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:04:10 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:04:16 KST)" (scheduled at 2025-01-14 17:03:56.538863+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:04:16 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:04:30 KST)" (scheduled at 2025-01-14 17:04:10.800030+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:04:30 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:04:36 KST)" (scheduled at 2025-01-14 17:04:16.538863+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:04:36 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:04:50 KST)" (scheduled at 2025-01-14 17:04:30.800030+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:04:50 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:04:56 KST)" (scheduled at 2025-01-14 17:04:36.538863+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:04:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:05:10 KST)" (scheduled at 2025-01-14 17:04:50.800030+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:05:10 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:05:16 KST)" (scheduled at 2025-01-14 17:04:56.538863+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:05:16 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:05:30 KST)" (scheduled at 2025-01-14 17:05:10.800030+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:05:30 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:05:36 KST)" (scheduled at 2025-01-14 17:05:16.538863+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:05:36 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:05:50 KST)" (scheduled at 2025-01-14 17:05:30.800030+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:05:50 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:05:56 KST)" (scheduled at 2025-01-14 17:05:36.538863+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:05:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:06:10 KST)" (scheduled at 2025-01-14 17:05:50.800030+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:06:10 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:06:16 KST)" (scheduled at 2025-01-14 17:05:56.538863+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:06:16 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:06:30 KST)" (scheduled at 2025-01-14 17:06:10.800030+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:06:30 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:06:36 KST)" (scheduled at 2025-01-14 17:06:16.538863+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:06:36 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:06:50 KST)" (scheduled at 2025-01-14 17:06:30.800030+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:06:50 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:06:56 KST)" (scheduled at 2025-01-14 17:06:36.538863+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:06:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:07:10 KST)" (scheduled at 2025-01-14 17:06:50.800030+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:07:10 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:07:16 KST)" (scheduled at 2025-01-14 17:06:56.538863+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:07:16 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:07:30 KST)" (scheduled at 2025-01-14 17:07:10.800030+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:07:30 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:07:36 KST)" (scheduled at 2025-01-14 17:07:16.538863+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:07:36 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:07:50 KST)" (scheduled at 2025-01-14 17:07:30.800030+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:07:50 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:07:56 KST)" (scheduled at 2025-01-14 17:07:36.538863+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:07:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:08:10 KST)" (scheduled at 2025-01-14 17:07:50.800030+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:08:10 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:08:16 KST)" (scheduled at 2025-01-14 17:07:56.538863+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:08:16 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:08:30 KST)" (scheduled at 2025-01-14 17:08:10.800030+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:08:30 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:08:36 KST)" (scheduled at 2025-01-14 17:08:16.538863+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:08:36 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:08:50 KST)" (scheduled at 2025-01-14 17:08:30.800030+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:08:50 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:08:56 KST)" (scheduled at 2025-01-14 17:08:36.538863+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:08:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:09:10 KST)" (scheduled at 2025-01-14 17:08:50.800030+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:09:10 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:09:16 KST)" (scheduled at 2025-01-14 17:08:56.538863+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:09:16 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:09:30 KST)" (scheduled at 2025-01-14 17:09:10.800030+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:09:30 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:09:36 KST)" (scheduled at 2025-01-14 17:09:16.538863+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:09:36 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:09:50 KST)" (scheduled at 2025-01-14 17:09:30.800030+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:09:50 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:09:56 KST)" (scheduled at 2025-01-14 17:09:36.538863+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:09:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:10:10 KST)" (scheduled at 2025-01-14 17:09:50.800030+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:10:10 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:10:16 KST)" (scheduled at 2025-01-14 17:09:56.538863+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:10:16 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:10:30 KST)" (scheduled at 2025-01-14 17:10:10.800030+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:10:30 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:10:36 KST)" (scheduled at 2025-01-14 17:10:16.538863+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:10:36 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\envs\\ai\\Lib\\site-packages\\IPython\\core\\interactiveshell.py', reloading
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:10:50 KST)" (scheduled at 2025-01-14 17:10:30.800030+09:00)
INFO:werkzeug: * Restarting with watchdog (windowsapi)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:10:50 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 524-771-621
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:11:10 KST)" (scheduled at 2025-01-14 17:10:50.800030+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:11:10 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:11:01 KST)" (scheduled at 2025-01-14 17:11:01.175408+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:11:21 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:11:30 KST)" (scheduled at 2025-01-14 17:11:10.800030+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:11:30 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:11:41 KST)" (scheduled at 2025-01-14 17:11:21.175408+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:11:41 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:11:50 KST)" (scheduled at 2025-01-14 17:11:30.800030+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:11:50 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:12:01 KST)" (scheduled at 2025-01-14 17:11:41.175408+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:12:01 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:12:10 KST)" (scheduled at 2025-01-14 17:11:50.800030+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:12:10 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:12:21 KST)" (scheduled at 2025-01-14 17:12:01.175408+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:12:21 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:12:30 KST)" (scheduled at 2025-01-14 17:12:10.800030+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:12:30 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:12:41 KST)" (scheduled at 2025-01-14 17:12:21.175408+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:12:41 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:12:50 KST)" (scheduled at 2025-01-14 17:12:30.800030+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:12:50 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:13:01 KST)" (scheduled at 2025-01-14 17:12:41.175408+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:13:01 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:13:10 KST)" (scheduled at 2025-01-14 17:12:50.800030+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:13:10 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:13:21 KST)" (scheduled at 2025-01-14 17:13:01.175408+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:13:21 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:13:30 KST)" (scheduled at 2025-01-14 17:13:10.800030+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:13:30 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:13:41 KST)" (scheduled at 2025-01-14 17:13:21.175408+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:13:41 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:13:50 KST)" (scheduled at 2025-01-14 17:13:30.800030+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:13:50 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:14:01 KST)" (scheduled at 2025-01-14 17:13:41.175408+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:14:01 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:14:10 KST)" (scheduled at 2025-01-14 17:13:50.800030+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:14:10 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:14:21 KST)" (scheduled at 2025-01-14 17:14:01.175408+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:14:21 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:14:30 KST)" (scheduled at 2025-01-14 17:14:10.800030+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:14:30 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:14:41 KST)" (scheduled at 2025-01-14 17:14:21.175408+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:14:41 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:14:50 KST)" (scheduled at 2025-01-14 17:14:30.800030+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:14:50 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:15:01 KST)" (scheduled at 2025-01-14 17:14:41.175408+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:15:01 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:15:10 KST)" (scheduled at 2025-01-14 17:14:50.800030+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:15:10 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:15:21 KST)" (scheduled at 2025-01-14 17:15:01.175408+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:15:21 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:15:30 KST)" (scheduled at 2025-01-14 17:15:10.800030+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:15:30 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:15:41 KST)" (scheduled at 2025-01-14 17:15:21.175408+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:15:41 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:15:50 KST)" (scheduled at 2025-01-14 17:15:30.800030+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:15:50 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:16:01 KST)" (scheduled at 2025-01-14 17:15:41.175408+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:16:01 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:16:10 KST)" (scheduled at 2025-01-14 17:15:50.800030+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:16:10 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:16:21 KST)" (scheduled at 2025-01-14 17:16:01.175408+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:16:21 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:16:30 KST)" (scheduled at 2025-01-14 17:16:10.800030+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:16:30 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:16:41 KST)" (scheduled at 2025-01-14 17:16:21.175408+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:16:41 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:16:50 KST)" (scheduled at 2025-01-14 17:16:30.800030+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:16:50 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:17:01 KST)" (scheduled at 2025-01-14 17:16:41.175408+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:17:01 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:17:10 KST)" (scheduled at 2025-01-14 17:16:50.800030+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:17:10 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:17:21 KST)" (scheduled at 2025-01-14 17:17:01.175408+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:17:21 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:17:30 KST)" (scheduled at 2025-01-14 17:17:10.800030+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:17:30 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:17:41 KST)" (scheduled at 2025-01-14 17:17:21.175408+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:17:41 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:17:50 KST)" (scheduled at 2025-01-14 17:17:30.800030+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:17:50 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:18:01 KST)" (scheduled at 2025-01-14 17:17:41.175408+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:18:01 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:18:10 KST)" (scheduled at 2025-01-14 17:17:50.800030+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:18:10 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:18:21 KST)" (scheduled at 2025-01-14 17:18:01.175408+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:18:21 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:18:30 KST)" (scheduled at 2025-01-14 17:18:10.800030+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:18:30 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:18:41 KST)" (scheduled at 2025-01-14 17:18:21.175408+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:18:41 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:18:50 KST)" (scheduled at 2025-01-14 17:18:30.800030+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:18:50 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:19:01 KST)" (scheduled at 2025-01-14 17:18:41.175408+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:19:01 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:19:10 KST)" (scheduled at 2025-01-14 17:18:50.800030+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:19:10 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:19:21 KST)" (scheduled at 2025-01-14 17:19:01.175408+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:19:21 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:19:30 KST)" (scheduled at 2025-01-14 17:19:10.800030+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:19:30 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:19:41 KST)" (scheduled at 2025-01-14 17:19:21.175408+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:19:41 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:19:50 KST)" (scheduled at 2025-01-14 17:19:30.800030+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:19:50 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:20:01 KST)" (scheduled at 2025-01-14 17:19:41.175408+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:20:01 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:20:10 KST)" (scheduled at 2025-01-14 17:19:50.800030+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:20:10 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:20:21 KST)" (scheduled at 2025-01-14 17:20:01.175408+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:20:21 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:20:30 KST)" (scheduled at 2025-01-14 17:20:10.800030+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:20:30 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:20:41 KST)" (scheduled at 2025-01-14 17:20:21.175408+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:20:41 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:20:50 KST)" (scheduled at 2025-01-14 17:20:30.800030+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:20:50 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:21:01 KST)" (scheduled at 2025-01-14 17:20:41.175408+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:21:01 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:21:10 KST)" (scheduled at 2025-01-14 17:20:50.800030+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:21:10 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:21:21 KST)" (scheduled at 2025-01-14 17:21:01.175408+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:21:21 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:21:30 KST)" (scheduled at 2025-01-14 17:21:10.800030+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:21:30 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:21:41 KST)" (scheduled at 2025-01-14 17:21:21.175408+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:21:41 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:21:50 KST)" (scheduled at 2025-01-14 17:21:30.800030+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:21:50 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:22:01 KST)" (scheduled at 2025-01-14 17:21:41.175408+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:22:01 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:22:10 KST)" (scheduled at 2025-01-14 17:21:50.800030+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:22:10 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:22:21 KST)" (scheduled at 2025-01-14 17:22:01.175408+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:22:21 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:22:30 KST)" (scheduled at 2025-01-14 17:22:10.800030+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:22:30 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:22:41 KST)" (scheduled at 2025-01-14 17:22:21.175408+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:22:41 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:22:50 KST)" (scheduled at 2025-01-14 17:22:30.800030+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:22:50 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:23:01 KST)" (scheduled at 2025-01-14 17:22:41.175408+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:23:01 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:23:10 KST)" (scheduled at 2025-01-14 17:22:50.800030+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:23:10 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:23:21 KST)" (scheduled at 2025-01-14 17:23:01.175408+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:23:21 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:23:30 KST)" (scheduled at 2025-01-14 17:23:10.800030+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:23:30 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:23:41 KST)" (scheduled at 2025-01-14 17:23:21.175408+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:23:41 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:23:50 KST)" (scheduled at 2025-01-14 17:23:30.800030+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:23:50 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:24:01 KST)" (scheduled at 2025-01-14 17:23:41.175408+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:24:01 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:24:10 KST)" (scheduled at 2025-01-14 17:23:50.800030+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:24:10 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:24:21 KST)" (scheduled at 2025-01-14 17:24:01.175408+09:00)
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\pandas\\__init__.py', reloading
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:24:21 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:24:30 KST)" (scheduled at 2025-01-14 17:24:10.800030+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:24:30 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 524-771-621
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:24:50 KST)" (scheduled at 2025-01-14 17:24:30.800030+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:24:50 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:24:41 KST)" (scheduled at 2025-01-14 17:24:41.379071+09:00)
INFO:werkzeug: * Detected change in 'C:\\Users\\soldesk\\AppData\\Roaming\\Python\\Python310\\site-packages\\apscheduler\\executors\\base.py', reloading
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:25:01 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:25:10 KST)" (scheduled at 2025-01-14 17:24:50.800030+09:00)
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:25:10 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 524-771-621
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:25:30 KST)" (scheduled at 2025-01-14 17:25:10.800030+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:25:30 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:25:19 KST)" (scheduled at 2025-01-14 17:25:19.413694+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:25:39 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:25:50 KST)" (scheduled at 2025-01-14 17:25:30.800030+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:25:50 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 257, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 46: illegal multibyte sequence
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
INFO:werkzeug:[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
INFO:werkzeug:[33mPress CTRL+C to quit[0m
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 524-771-621
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
INFO:werkzeug:[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
INFO:werkzeug:[33mPress CTRL+C to quit[0m
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 524-771-621
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:38:45 KST)" (scheduled at 2025-01-14 17:38:45.007632+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:38:51 KST)" (scheduled at 2025-01-14 17:38:51.177741+09:00)
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:39:05 KST)" skipped: maximum number of running instances reached (1)
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:39:11 KST)" skipped: maximum number of running instances reached (1)
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:39:31 KST)" executed successfully
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:39:25 KST)" skipped: maximum number of running instances reached (1)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:39:51 KST)" (scheduled at 2025-01-14 17:39:31.177741+09:00)
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:39:45 KST)" skipped: maximum number of running instances reached (1)
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:40:05 KST)" executed successfully
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:39:51 KST)" skipped: maximum number of running instances reached (1)
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:40:11 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:40:25 KST)" (scheduled at 2025-01-14 17:40:05.007632+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:40:31 KST)" (scheduled at 2025-01-14 17:40:11.177741+09:00)
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:40:25 KST)" skipped: maximum number of running instances reached (1)
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:40:45 KST)" executed successfully
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:40:31 KST)" skipped: maximum number of running instances reached (1)
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:40:51 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:41:05 KST)" (scheduled at 2025-01-14 17:40:45.007632+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:41:11 KST)" (scheduled at 2025-01-14 17:40:51.177741+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:41:11 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 108, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 38, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 347, in execute
    self.error_handler.check_response(response)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchWindowException: Message: no such window: target window already closed
from unknown error: web view not found
  (Session info: chrome=131.0.6778.265)
Stacktrace:
	GetHandleVerifier [0x004EFD53+23747]
	(No symbol) [0x00477D54]
	(No symbol) [0x0034BE53]
	(No symbol) [0x0032D91B]
	(No symbol) [0x003B7EFF]
	(No symbol) [0x003CAD49]
	(No symbol) [0x003B1B96]
	(No symbol) [0x00383F3C]
	(No symbol) [0x00384EBD]
	GetHandleVerifier [0x007CAC73+3017699]
	GetHandleVerifier [0x007DB93B+3086507]
	GetHandleVerifier [0x007D40F2+3055714]
	GetHandleVerifier [0x00585AF0+637536]
	(No symbol) [0x00480A5D]
	(No symbol) [0x0047DA28]
	(No symbol) [0x0047DBC5]
	(No symbol) [0x004707F0]
	BaseThreadInitThunk [0x76E2FCC9+25]
	RtlGetAppContainerNamedObjectPath [0x77B2809E+286]
	RtlGetAppContainerNamedObjectPath [0x77B2806E+238]

INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:41:05 KST)" skipped: maximum number of running instances reached (1)
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
INFO:werkzeug:[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
INFO:werkzeug:[33mPress CTRL+C to quit[0m
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:41:31 KST)" (scheduled at 2025-01-14 17:41:11.177741+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:41:31 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 108, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 38, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 347, in execute
    self.error_handler.check_response(response)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchWindowException: Message: no such window: target window already closed
from unknown error: web view not found
  (Session info: chrome=131.0.6778.265)
Stacktrace:
	GetHandleVerifier [0x004EFD53+23747]
	(No symbol) [0x00477D54]
	(No symbol) [0x0034BE53]
	(No symbol) [0x0032D91B]
	(No symbol) [0x003B7EFF]
	(No symbol) [0x003CAD49]
	(No symbol) [0x003B1B96]
	(No symbol) [0x00383F3C]
	(No symbol) [0x00384EBD]
	GetHandleVerifier [0x007CAC73+3017699]
	GetHandleVerifier [0x007DB93B+3086507]
	GetHandleVerifier [0x007D40F2+3055714]
	GetHandleVerifier [0x00585AF0+637536]
	(No symbol) [0x00480A5D]
	(No symbol) [0x0047DA28]
	(No symbol) [0x0047DBC5]
	(No symbol) [0x004707F0]
	BaseThreadInitThunk [0x76E2FCC9+25]
	RtlGetAppContainerNamedObjectPath [0x77B2809E+286]
	RtlGetAppContainerNamedObjectPath [0x77B2806E+238]

INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 524-771-621
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:41:27 KST)" (scheduled at 2025-01-14 17:41:27.239420+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 17:41:47 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 108, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 38, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 347, in execute
    self.error_handler.check_response(response)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchWindowException: Message: no such window: target window already closed
from unknown error: web view not found
  (Session info: chrome=131.0.6778.265)
Stacktrace:
	GetHandleVerifier [0x004EFD53+23747]
	(No symbol) [0x00477D54]
	(No symbol) [0x0034BE53]
	(No symbol) [0x0032D91B]
	(No symbol) [0x003B7EFF]
	(No symbol) [0x003CAD49]
	(No symbol) [0x003B1B96]
	(No symbol) [0x00383F3C]
	(No symbol) [0x00384EBD]
	GetHandleVerifier [0x007CAC73+3017699]
	GetHandleVerifier [0x007DB93B+3086507]
	GetHandleVerifier [0x007D40F2+3055714]
	GetHandleVerifier [0x00585AF0+637536]
	(No symbol) [0x00480A5D]
	(No symbol) [0x0047DA28]
	(No symbol) [0x0047DBC5]
	(No symbol) [0x004707F0]
	BaseThreadInitThunk [0x76E2FCC9+25]
	RtlGetAppContainerNamedObjectPath [0x77B2809E+286]
	RtlGetAppContainerNamedObjectPath [0x77B2806E+238]

INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
<<<<<<< HEAD
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
=======
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:There is no [win64] chromedriver "131.0.6778.264" for browser google-chrome "131.0.6778" in cache
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:WebDriver version 131.0.6778.264 selected
INFO:WDM:Modern chrome version https://storage.googleapis.com/chrome-for-testing-public/131.0.6778.264/win32/chromedriver-win32.zip
INFO:WDM:About to download new driver from https://storage.googleapis.com/chrome-for-testing-public/131.0.6778.264/win32/chromedriver-win32.zip
INFO:WDM:Driver downloading response is 200
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver has been saved in cache [C:\Users\i\.wdm\drivers\chromedriver\win64\131.0.6778.264]
INFO:WDM:====== WebDriver manager ======
>>>>>>> a6bdad380237258af82a182dad4608eac81e0f1f
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
INFO:werkzeug:[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
INFO:werkzeug:[33mPress CTRL+C to quit[0m
INFO:werkzeug: * Restarting with watchdog (windowsapi)
<<<<<<< HEAD
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 524-771-621
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 18:54:18 KST)" (scheduled at 2025-01-14 18:54:18.794044+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 18:54:25 KST)" (scheduled at 2025-01-14 18:54:25.358887+09:00)
INFO:werkzeug: * Detected change in 'C:\\Users\\soldesk\\AppData\\Roaming\\Python\\Python310\\site-packages\\apscheduler\\executors\\base.py', reloading
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 18:54:45 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 148, in crawl_and_insert_data
    time.sleep(1)  # 잠시 대기 후 루프 계속
UnboundLocalError: local variable 'time' referenced before assignment
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 18:54:38 KST)" skipped: maximum number of running instances reached (1)
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
=======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\i\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
>>>>>>> a6bdad380237258af82a182dad4608eac81e0f1f
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
INFO:werkzeug:[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
INFO:werkzeug:[33mPress CTRL+C to quit[0m
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
<<<<<<< HEAD
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
=======
INFO:WDM:Driver [C:\Users\i\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:WDM:====== WebDriver manager ======
>>>>>>> a6bdad380237258af82a182dad4608eac81e0f1f
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
<<<<<<< HEAD
INFO:werkzeug: * Debugger PIN: 524-771-621
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 18:55:35 KST)" (scheduled at 2025-01-14 18:55:35.210182+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 18:55:43 KST)" (scheduled at 2025-01-14 18:55:43.963747+09:00)
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 18:55:55 KST)" skipped: maximum number of running instances reached (1)
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 18:56:03 KST)" skipped: maximum number of running instances reached (1)
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 18:56:15 KST)" skipped: maximum number of running instances reached (1)
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 18:56:23 KST)" skipped: maximum number of running instances reached (1)
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 18:56:35 KST)" skipped: maximum number of running instances reached (1)
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 18:56:43 KST)" skipped: maximum number of running instances reached (1)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 18:56:55 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 148, in crawl_and_insert_data
    time.sleep(1)  # 잠시 대기 후 루프 계속
UnboundLocalError: local variable 'time' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 18:57:15 KST)" (scheduled at 2025-01-14 18:56:55.210182+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 18:57:15 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 148, in crawl_and_insert_data
    time.sleep(1)  # 잠시 대기 후 루프 계속
UnboundLocalError: local variable 'time' referenced before assignment
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 18:57:03 KST)" skipped: maximum number of running instances reached (1)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 18:57:35 KST)" (scheduled at 2025-01-14 18:57:15.210182+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 18:57:35 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 148, in crawl_and_insert_data
    time.sleep(1)  # 잠시 대기 후 루프 계속
UnboundLocalError: local variable 'time' referenced before assignment
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 18:57:23 KST)" skipped: maximum number of running instances reached (1)
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\posixpath.py', reloading
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 18:57:55 KST)" (scheduled at 2025-01-14 18:57:35.210182+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 18:57:55 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 148, in crawl_and_insert_data
    time.sleep(1)  # 잠시 대기 후 루프 계속
UnboundLocalError: local variable 'time' referenced before assignment
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 18:57:43 KST)" skipped: maximum number of running instances reached (1)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 18:58:15 KST)" (scheduled at 2025-01-14 18:57:55.210182+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 18:58:15 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 148, in crawl_and_insert_data
    time.sleep(1)  # 잠시 대기 후 루프 계속
UnboundLocalError: local variable 'time' referenced before assignment
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 18:58:03 KST)" skipped: maximum number of running instances reached (1)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 18:58:35 KST)" (scheduled at 2025-01-14 18:58:15.210182+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 18:58:35 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 148, in crawl_and_insert_data
    time.sleep(1)  # 잠시 대기 후 루프 계속
UnboundLocalError: local variable 'time' referenced before assignment
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 18:58:23 KST)" skipped: maximum number of running instances reached (1)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 18:58:55 KST)" (scheduled at 2025-01-14 18:58:35.210182+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 18:58:55 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 148, in crawl_and_insert_data
    time.sleep(1)  # 잠시 대기 후 루프 계속
UnboundLocalError: local variable 'time' referenced before assignment
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 18:58:43 KST)" skipped: maximum number of running instances reached (1)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 18:59:15 KST)" (scheduled at 2025-01-14 18:58:55.210182+09:00)
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 18:59:03 KST)" skipped: maximum number of running instances reached (1)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 18:59:15 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 148, in crawl_and_insert_data
    time.sleep(1)  # 잠시 대기 후 루프 계속
UnboundLocalError: local variable 'time' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 18:59:35 KST)" (scheduled at 2025-01-14 18:59:15.210182+09:00)
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 18:59:23 KST)" skipped: maximum number of running instances reached (1)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 18:59:35 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 148, in crawl_and_insert_data
    time.sleep(1)  # 잠시 대기 후 루프 계속
UnboundLocalError: local variable 'time' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 18:59:55 KST)" (scheduled at 2025-01-14 18:59:35.210182+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 18:59:55 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 148, in crawl_and_insert_data
    time.sleep(1)  # 잠시 대기 후 루프 계속
UnboundLocalError: local variable 'time' referenced before assignment
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 18:59:43 KST)" skipped: maximum number of running instances reached (1)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:00:15 KST)" (scheduled at 2025-01-14 18:59:55.210182+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:00:15 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 148, in crawl_and_insert_data
    time.sleep(1)  # 잠시 대기 후 루프 계속
UnboundLocalError: local variable 'time' referenced before assignment
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:00:03 KST)" skipped: maximum number of running instances reached (1)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:00:35 KST)" (scheduled at 2025-01-14 19:00:15.210182+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:00:35 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 148, in crawl_and_insert_data
    time.sleep(1)  # 잠시 대기 후 루프 계속
UnboundLocalError: local variable 'time' referenced before assignment
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:00:23 KST)" skipped: maximum number of running instances reached (1)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:00:55 KST)" (scheduled at 2025-01-14 19:00:35.210182+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:00:55 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 148, in crawl_and_insert_data
    time.sleep(1)  # 잠시 대기 후 루프 계속
UnboundLocalError: local variable 'time' referenced before assignment
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:00:43 KST)" skipped: maximum number of running instances reached (1)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:01:03 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 108, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 40, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 347, in execute
    self.error_handler.check_response(response)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: target frame detached: received Inspector.detached event
  (Session info: chrome=131.0.6778.265)
Stacktrace:
	GetHandleVerifier [0x004EFD53+23747]
	(No symbol) [0x00477D54]
	(No symbol) [0x0034BE53]
	(No symbol) [0x0033E998]
	(No symbol) [0x0033DBA7]
	(No symbol) [0x0033D503]
	(No symbol) [0x0033D43D]
	(No symbol) [0x0033B8BB]
	(No symbol) [0x0033BF2D]
	(No symbol) [0x003481BA]
	(No symbol) [0x003578B5]
	(No symbol) [0x0035C726]
	(No symbol) [0x0033C535]
	(No symbol) [0x003574E1]
	(No symbol) [0x003CB778]
	(No symbol) [0x003B1B96]
	(No symbol) [0x00383F3C]
	(No symbol) [0x00384EBD]
	GetHandleVerifier [0x007CAC73+3017699]
	GetHandleVerifier [0x007DB93B+3086507]
	GetHandleVerifier [0x007D40F2+3055714]
	GetHandleVerifier [0x00585AF0+637536]
	(No symbol) [0x00480A5D]
	(No symbol) [0x0047DA28]
	(No symbol) [0x0047DBC5]
	(No symbol) [0x004707F0]
	BaseThreadInitThunk [0x76E2FCC9+25]
	RtlGetAppContainerNamedObjectPath [0x77B2809E+286]
	RtlGetAppContainerNamedObjectPath [0x77B2806E+238]

INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:01:15 KST)" (scheduled at 2025-01-14 19:00:55.210182+09:00)
INFO:werkzeug: * Restarting with watchdog (windowsapi)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:01:15 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 148, in crawl_and_insert_data
    time.sleep(1)  # 잠시 대기 후 루프 계속
UnboundLocalError: local variable 'time' referenced before assignment
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
=======
INFO:werkzeug: * Debugger PIN: 569-178-434
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\i\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
>>>>>>> a6bdad380237258af82a182dad4608eac81e0f1f
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
<<<<<<< HEAD
INFO:werkzeug: * Debugger PIN: 524-771-621
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:01:35 KST)" (scheduled at 2025-01-14 19:01:15.210182+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:01:35 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 148, in crawl_and_insert_data
    time.sleep(1)  # 잠시 대기 후 루프 계속
UnboundLocalError: local variable 'time' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:01:31 KST)" (scheduled at 2025-01-14 19:01:31.409257+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:01:55 KST)" (scheduled at 2025-01-14 19:01:35.210182+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:01:51 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 148, in crawl_and_insert_data
    time.sleep(1)  # 잠시 대기 후 루프 계속
UnboundLocalError: local variable 'time' referenced before assignment
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:01:55 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 148, in crawl_and_insert_data
    time.sleep(1)  # 잠시 대기 후 루프 계속
UnboundLocalError: local variable 'time' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:02:11 KST)" (scheduled at 2025-01-14 19:01:51.409257+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:02:11 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 148, in crawl_and_insert_data
    time.sleep(1)  # 잠시 대기 후 루프 계속
UnboundLocalError: local variable 'time' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:02:15 KST)" (scheduled at 2025-01-14 19:01:55.210182+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:02:15 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 148, in crawl_and_insert_data
    time.sleep(1)  # 잠시 대기 후 루프 계속
UnboundLocalError: local variable 'time' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:02:31 KST)" (scheduled at 2025-01-14 19:02:11.409257+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:02:35 KST)" (scheduled at 2025-01-14 19:02:15.210182+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:02:31 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 148, in crawl_and_insert_data
    time.sleep(1)  # 잠시 대기 후 루프 계속
UnboundLocalError: local variable 'time' referenced before assignment
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:02:35 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 148, in crawl_and_insert_data
    time.sleep(1)  # 잠시 대기 후 루프 계속
UnboundLocalError: local variable 'time' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:02:51 KST)" (scheduled at 2025-01-14 19:02:31.409257+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:02:55 KST)" (scheduled at 2025-01-14 19:02:35.210182+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:02:55 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 148, in crawl_and_insert_data
    time.sleep(1)  # 잠시 대기 후 루프 계속
UnboundLocalError: local variable 'time' referenced before assignment
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:02:51 KST)" skipped: maximum number of running instances reached (1)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:03:15 KST)" (scheduled at 2025-01-14 19:02:55.210182+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:03:15 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 148, in crawl_and_insert_data
    time.sleep(1)  # 잠시 대기 후 루프 계속
UnboundLocalError: local variable 'time' referenced before assignment
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:03:11 KST)" skipped: maximum number of running instances reached (1)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:03:35 KST)" (scheduled at 2025-01-14 19:03:15.210182+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:03:35 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 148, in crawl_and_insert_data
    time.sleep(1)  # 잠시 대기 후 루프 계속
UnboundLocalError: local variable 'time' referenced before assignment
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:03:31 KST)" skipped: maximum number of running instances reached (1)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:03:55 KST)" (scheduled at 2025-01-14 19:03:35.210182+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:03:55 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 148, in crawl_and_insert_data
    time.sleep(1)  # 잠시 대기 후 루프 계속
UnboundLocalError: local variable 'time' referenced before assignment
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:03:51 KST)" skipped: maximum number of running instances reached (1)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:04:15 KST)" (scheduled at 2025-01-14 19:03:55.210182+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:04:15 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 148, in crawl_and_insert_data
    time.sleep(1)  # 잠시 대기 후 루프 계속
UnboundLocalError: local variable 'time' referenced before assignment
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:04:11 KST)" skipped: maximum number of running instances reached (1)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:04:31 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 266, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2014' in position 80: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:04:35 KST)" (scheduled at 2025-01-14 19:04:15.210182+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:04:35 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 266, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2014' in position 80: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:04:51 KST)" (scheduled at 2025-01-14 19:04:31.409257+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:04:55 KST)" (scheduled at 2025-01-14 19:04:35.210182+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:04:51 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 266, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2014' in position 80: illegal multibyte sequence
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:04:55 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 266, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2014' in position 80: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:05:11 KST)" (scheduled at 2025-01-14 19:04:51.409257+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:05:15 KST)" (scheduled at 2025-01-14 19:04:55.210182+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:05:11 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 266, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2014' in position 80: illegal multibyte sequence
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:05:15 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 266, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2014' in position 80: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:05:31 KST)" (scheduled at 2025-01-14 19:05:11.409257+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:05:35 KST)" (scheduled at 2025-01-14 19:05:15.210182+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:05:31 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 266, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2014' in position 80: illegal multibyte sequence
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:05:35 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 266, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2014' in position 80: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:05:51 KST)" (scheduled at 2025-01-14 19:05:31.409257+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:05:55 KST)" (scheduled at 2025-01-14 19:05:35.210182+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:05:51 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 266, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2014' in position 80: illegal multibyte sequence
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:05:55 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 266, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2014' in position 80: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:06:11 KST)" (scheduled at 2025-01-14 19:05:51.409257+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:06:15 KST)" (scheduled at 2025-01-14 19:05:55.210182+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:06:11 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 266, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2014' in position 80: illegal multibyte sequence
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:06:15 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 266, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2014' in position 80: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:06:31 KST)" (scheduled at 2025-01-14 19:06:11.409257+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:06:35 KST)" (scheduled at 2025-01-14 19:06:15.210182+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:06:31 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 266, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2014' in position 80: illegal multibyte sequence
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:06:35 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 266, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2014' in position 80: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:06:51 KST)" (scheduled at 2025-01-14 19:06:31.409257+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:06:55 KST)" (scheduled at 2025-01-14 19:06:35.210182+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:06:51 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 266, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2014' in position 80: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:07:11 KST)" (scheduled at 2025-01-14 19:06:51.409257+09:00)
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:06:55 KST)" skipped: maximum number of running instances reached (1)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:07:15 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 266, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2014' in position 80: illegal multibyte sequence
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:07:11 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 266, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2014' in position 80: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:07:31 KST)" (scheduled at 2025-01-14 19:07:11.409257+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:07:35 KST)" (scheduled at 2025-01-14 19:07:15.210182+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:07:31 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 266, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2014' in position 80: illegal multibyte sequence
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:07:35 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 266, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2014' in position 80: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:07:51 KST)" (scheduled at 2025-01-14 19:07:31.409257+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:07:55 KST)" (scheduled at 2025-01-14 19:07:35.210182+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:07:51 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 266, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2014' in position 80: illegal multibyte sequence
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:07:55 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 266, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2014' in position 80: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:08:11 KST)" (scheduled at 2025-01-14 19:07:51.409257+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:08:15 KST)" (scheduled at 2025-01-14 19:07:55.210182+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:08:11 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 266, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2014' in position 80: illegal multibyte sequence
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:08:15 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 266, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2014' in position 80: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:08:31 KST)" (scheduled at 2025-01-14 19:08:11.409257+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:08:35 KST)" (scheduled at 2025-01-14 19:08:15.210182+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:08:31 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 266, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2014' in position 80: illegal multibyte sequence
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:08:35 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 266, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2014' in position 80: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:08:51 KST)" (scheduled at 2025-01-14 19:08:31.409257+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:08:55 KST)" (scheduled at 2025-01-14 19:08:35.210182+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:08:51 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 266, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2014' in position 80: illegal multibyte sequence
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:08:55 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 266, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2014' in position 80: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:09:11 KST)" (scheduled at 2025-01-14 19:08:51.409257+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:09:15 KST)" (scheduled at 2025-01-14 19:08:55.210182+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:09:11 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 266, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2014' in position 80: illegal multibyte sequence
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:09:15 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 266, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2014' in position 80: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:09:31 KST)" (scheduled at 2025-01-14 19:09:11.409257+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:09:35 KST)" (scheduled at 2025-01-14 19:09:15.210182+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:09:35 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 266, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2014' in position 80: illegal multibyte sequence
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:09:31 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 266, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2014' in position 80: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:09:51 KST)" (scheduled at 2025-01-14 19:09:31.409257+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:09:55 KST)" (scheduled at 2025-01-14 19:09:35.210182+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:09:51 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 266, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2014' in position 80: illegal multibyte sequence
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:09:55 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 266, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2014' in position 80: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:10:11 KST)" (scheduled at 2025-01-14 19:09:51.409257+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:10:15 KST)" (scheduled at 2025-01-14 19:09:55.210182+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:10:11 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 266, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2014' in position 80: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:10:31 KST)" (scheduled at 2025-01-14 19:10:11.409257+09:00)
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:10:15 KST)" skipped: maximum number of running instances reached (1)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:10:31 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 266, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2014' in position 80: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:10:51 KST)" (scheduled at 2025-01-14 19:10:31.409257+09:00)
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:10:35 KST)" skipped: maximum number of running instances reached (1)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:10:51 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 266, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2014' in position 80: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:11:11 KST)" (scheduled at 2025-01-14 19:10:51.409257+09:00)
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:10:55 KST)" skipped: maximum number of running instances reached (1)
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:11:11 KST)" skipped: maximum number of running instances reached (1)
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:11:15 KST)" skipped: maximum number of running instances reached (1)
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:11:31 KST)" skipped: maximum number of running instances reached (1)
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:11:35 KST)" skipped: maximum number of running instances reached (1)
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:11:51 KST)" skipped: maximum number of running instances reached (1)
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:11:55 KST)" skipped: maximum number of running instances reached (1)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:WebDriver version 131.0.6778.264 selected
INFO:WDM:Modern chrome version https://storage.googleapis.com/chrome-for-testing-public/131.0.6778.264/win32/chromedriver-win32.zip
INFO:WDM:About to download new driver from https://storage.googleapis.com/chrome-for-testing-public/131.0.6778.264/win32/chromedriver-win32.zip
INFO:WDM:Driver downloading response is 200
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver has been saved in cache [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264]
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
INFO:werkzeug:[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
INFO:werkzeug:[33mPress CTRL+C to quit[0m
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 524-771-621
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:00:45 KST)" (scheduled at 2025-01-15 10:00:45.666564+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:00:53 KST)" (scheduled at 2025-01-15 10:00:53.005672+09:00)
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:01:45 KST)" skipped: maximum number of running instances reached (1)
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:01:53 KST)" skipped: maximum number of running instances reached (1)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:02:45 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 263, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 34: illegal multibyte sequence
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:02:53 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 263, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 34: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:03:45 KST)" (scheduled at 2025-01-15 10:02:45.666564+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:03:53 KST)" (scheduled at 2025-01-15 10:02:53.005672+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:03:45 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 263, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 34: illegal multibyte sequence
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:03:53 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 263, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 34: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:04:45 KST)" (scheduled at 2025-01-15 10:03:45.666564+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:04:53 KST)" (scheduled at 2025-01-15 10:03:53.005672+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:04:45 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 263, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 34: illegal multibyte sequence
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:04:53 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 263, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 34: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:05:45 KST)" (scheduled at 2025-01-15 10:04:45.666564+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:05:53 KST)" (scheduled at 2025-01-15 10:04:53.005672+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:05:45 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 263, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 34: illegal multibyte sequence
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:05:53 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 263, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 34: illegal multibyte sequence
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
INFO:werkzeug:[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
INFO:werkzeug:[33mPress CTRL+C to quit[0m
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 524-771-621
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:08:14 KST)" (scheduled at 2025-01-15 10:08:14.799011+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:08:26 KST)" (scheduled at 2025-01-15 10:08:26.339649+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:09:14 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 263, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 34: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:10:14 KST)" (scheduled at 2025-01-15 10:09:14.799011+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:09:26 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 263, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 34: illegal multibyte sequence
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:10:14 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 263, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 34: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:10:26 KST)" (scheduled at 2025-01-15 10:09:26.339649+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:10:26 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 263, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 34: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:11:14 KST)" (scheduled at 2025-01-15 10:10:14.799011+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:11:14 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 263, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 34: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:11:26 KST)" (scheduled at 2025-01-15 10:10:26.339649+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:11:26 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 263, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 34: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:12:14 KST)" (scheduled at 2025-01-15 10:11:14.799011+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:12:26 KST)" (scheduled at 2025-01-15 10:11:26.339649+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:12:14 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 263, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 34: illegal multibyte sequence
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:12:26 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 263, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 34: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:13:14 KST)" (scheduled at 2025-01-15 10:12:14.799011+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:13:14 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 263, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 34: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:13:26 KST)" (scheduled at 2025-01-15 10:12:26.339649+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:13:26 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 263, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 34: illegal multibyte sequence
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
INFO:werkzeug:[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
INFO:werkzeug:[33mPress CTRL+C to quit[0m
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 524-771-621
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:29:56 KST)" (scheduled at 2025-01-15 10:29:56.912050+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:30:02 KST)" (scheduled at 2025-01-15 10:30:02.741557+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:30:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 263, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 34: illegal multibyte sequence
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:31:02 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 263, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 34: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:31:56 KST)" (scheduled at 2025-01-15 10:30:56.912050+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:32:02 KST)" (scheduled at 2025-01-15 10:31:02.741557+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:31:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 263, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 34: illegal multibyte sequence
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:32:02 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 263, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 34: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:32:56 KST)" (scheduled at 2025-01-15 10:31:56.912050+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:33:02 KST)" (scheduled at 2025-01-15 10:32:02.741557+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:32:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 263, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 34: illegal multibyte sequence
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:33:02 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 263, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 34: illegal multibyte sequence
INFO:werkzeug: * Detected change in 'C:\\Users\\soldesk\\AppData\\Roaming\\Python\\Python310\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py', reloading
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 524-771-621
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:33:56 KST)" (scheduled at 2025-01-15 10:32:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:33:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 263, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2013' in position 34: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:33:46 KST)" (scheduled at 2025-01-15 10:33:46.939971+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:34:56 KST)" (scheduled at 2025-01-15 10:33:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:34:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:34:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:35:46 KST)" (scheduled at 2025-01-15 10:34:46.939971+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:35:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:35:56 KST)" (scheduled at 2025-01-15 10:34:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:35:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:36:46 KST)" (scheduled at 2025-01-15 10:35:46.939971+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:36:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:36:56 KST)" (scheduled at 2025-01-15 10:35:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:36:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:37:46 KST)" (scheduled at 2025-01-15 10:36:46.939971+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:37:56 KST)" (scheduled at 2025-01-15 10:36:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:37:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:37:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:38:46 KST)" (scheduled at 2025-01-15 10:37:46.939971+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:38:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:38:56 KST)" (scheduled at 2025-01-15 10:37:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:38:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:39:46 KST)" (scheduled at 2025-01-15 10:38:46.939971+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:39:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:39:56 KST)" (scheduled at 2025-01-15 10:38:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:39:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:40:46 KST)" (scheduled at 2025-01-15 10:39:46.939971+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:40:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:40:56 KST)" (scheduled at 2025-01-15 10:39:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:40:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:41:46 KST)" (scheduled at 2025-01-15 10:40:46.939971+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:41:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:41:56 KST)" (scheduled at 2025-01-15 10:40:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:41:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:42:46 KST)" (scheduled at 2025-01-15 10:41:46.939971+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:42:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:42:56 KST)" (scheduled at 2025-01-15 10:41:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:42:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:43:46 KST)" (scheduled at 2025-01-15 10:42:46.939971+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:43:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:43:56 KST)" (scheduled at 2025-01-15 10:42:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:43:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:44:46 KST)" (scheduled at 2025-01-15 10:43:46.939971+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:44:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:44:56 KST)" (scheduled at 2025-01-15 10:43:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:44:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:45:46 KST)" (scheduled at 2025-01-15 10:44:46.939971+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:45:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:45:56 KST)" (scheduled at 2025-01-15 10:44:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:45:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:46:46 KST)" (scheduled at 2025-01-15 10:45:46.939971+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:46:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:46:56 KST)" (scheduled at 2025-01-15 10:45:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:46:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:47:46 KST)" (scheduled at 2025-01-15 10:46:46.939971+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:47:56 KST)" (scheduled at 2025-01-15 10:46:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:47:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:47:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:48:46 KST)" (scheduled at 2025-01-15 10:47:46.939971+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:48:56 KST)" (scheduled at 2025-01-15 10:47:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:48:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:48:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:49:46 KST)" (scheduled at 2025-01-15 10:48:46.939971+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:49:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:49:56 KST)" (scheduled at 2025-01-15 10:48:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:49:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:50:46 KST)" (scheduled at 2025-01-15 10:49:46.939971+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:50:56 KST)" (scheduled at 2025-01-15 10:49:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:50:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:50:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:51:46 KST)" (scheduled at 2025-01-15 10:50:46.939971+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:51:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:51:56 KST)" (scheduled at 2025-01-15 10:50:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:51:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:52:46 KST)" (scheduled at 2025-01-15 10:51:46.939971+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:52:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:52:56 KST)" (scheduled at 2025-01-15 10:51:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:52:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:53:46 KST)" (scheduled at 2025-01-15 10:52:46.939971+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:53:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:53:56 KST)" (scheduled at 2025-01-15 10:52:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:53:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:54:46 KST)" (scheduled at 2025-01-15 10:53:46.939971+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:54:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:54:56 KST)" (scheduled at 2025-01-15 10:53:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:54:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:55:46 KST)" (scheduled at 2025-01-15 10:54:46.939971+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:55:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:55:56 KST)" (scheduled at 2025-01-15 10:54:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:55:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:56:46 KST)" (scheduled at 2025-01-15 10:55:46.939971+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:56:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:56:56 KST)" (scheduled at 2025-01-15 10:55:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:56:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:57:46 KST)" (scheduled at 2025-01-15 10:56:46.939971+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:57:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:57:56 KST)" (scheduled at 2025-01-15 10:56:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:57:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:58:46 KST)" (scheduled at 2025-01-15 10:57:46.939971+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:58:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:58:56 KST)" (scheduled at 2025-01-15 10:57:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:58:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:59:46 KST)" (scheduled at 2025-01-15 10:58:46.939971+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:59:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:59:56 KST)" (scheduled at 2025-01-15 10:58:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 10:59:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:00:46 KST)" (scheduled at 2025-01-15 10:59:46.939971+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:00:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:00:56 KST)" (scheduled at 2025-01-15 10:59:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:00:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:01:46 KST)" (scheduled at 2025-01-15 11:00:46.939971+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:01:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:01:56 KST)" (scheduled at 2025-01-15 11:00:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:01:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:02:46 KST)" (scheduled at 2025-01-15 11:01:46.939971+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:02:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:02:56 KST)" (scheduled at 2025-01-15 11:01:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:02:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:03:46 KST)" (scheduled at 2025-01-15 11:02:46.939971+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:03:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:03:56 KST)" (scheduled at 2025-01-15 11:02:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:03:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:04:46 KST)" (scheduled at 2025-01-15 11:03:46.939971+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:04:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:04:56 KST)" (scheduled at 2025-01-15 11:03:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:04:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:05:46 KST)" (scheduled at 2025-01-15 11:04:46.939971+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:05:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:05:56 KST)" (scheduled at 2025-01-15 11:04:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:05:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:06:46 KST)" (scheduled at 2025-01-15 11:05:46.939971+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:06:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:06:56 KST)" (scheduled at 2025-01-15 11:05:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:06:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:07:46 KST)" (scheduled at 2025-01-15 11:06:46.939971+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:07:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:07:56 KST)" (scheduled at 2025-01-15 11:06:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:07:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:08:46 KST)" (scheduled at 2025-01-15 11:07:46.939971+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:08:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:08:56 KST)" (scheduled at 2025-01-15 11:07:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:08:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:09:46 KST)" (scheduled at 2025-01-15 11:08:46.939971+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:09:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:09:56 KST)" (scheduled at 2025-01-15 11:08:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:09:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:10:46 KST)" (scheduled at 2025-01-15 11:09:46.939971+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:10:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:10:56 KST)" (scheduled at 2025-01-15 11:09:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:10:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:11:46 KST)" (scheduled at 2025-01-15 11:10:46.939971+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:11:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:11:56 KST)" (scheduled at 2025-01-15 11:10:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:11:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:12:46 KST)" (scheduled at 2025-01-15 11:11:46.939971+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:12:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:12:56 KST)" (scheduled at 2025-01-15 11:11:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:12:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:13:46 KST)" (scheduled at 2025-01-15 11:12:46.939971+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:13:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:13:56 KST)" (scheduled at 2025-01-15 11:12:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:13:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:14:46 KST)" (scheduled at 2025-01-15 11:13:46.939971+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:14:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:14:56 KST)" (scheduled at 2025-01-15 11:13:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:14:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:15:46 KST)" (scheduled at 2025-01-15 11:14:46.939971+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:15:56 KST)" (scheduled at 2025-01-15 11:14:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:15:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:15:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:16:46 KST)" (scheduled at 2025-01-15 11:15:46.939971+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:16:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:16:56 KST)" (scheduled at 2025-01-15 11:15:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:16:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:17:46 KST)" (scheduled at 2025-01-15 11:16:46.939971+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:17:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:17:56 KST)" (scheduled at 2025-01-15 11:16:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:17:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:18:46 KST)" (scheduled at 2025-01-15 11:17:46.939971+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:18:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:18:56 KST)" (scheduled at 2025-01-15 11:17:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:18:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:19:46 KST)" (scheduled at 2025-01-15 11:18:46.939971+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:19:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:19:56 KST)" (scheduled at 2025-01-15 11:18:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:19:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:20:46 KST)" (scheduled at 2025-01-15 11:19:46.939971+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:20:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:20:56 KST)" (scheduled at 2025-01-15 11:19:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:20:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:21:46 KST)" (scheduled at 2025-01-15 11:20:46.939971+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:21:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:21:56 KST)" (scheduled at 2025-01-15 11:20:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:21:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:22:46 KST)" (scheduled at 2025-01-15 11:21:46.939971+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:22:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:22:56 KST)" (scheduled at 2025-01-15 11:21:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:22:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:23:46 KST)" (scheduled at 2025-01-15 11:22:46.939971+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:23:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:23:56 KST)" (scheduled at 2025-01-15 11:22:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:23:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:24:46 KST)" (scheduled at 2025-01-15 11:23:46.939971+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:24:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:24:56 KST)" (scheduled at 2025-01-15 11:23:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:24:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:25:46 KST)" (scheduled at 2025-01-15 11:24:46.939971+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:25:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:25:56 KST)" (scheduled at 2025-01-15 11:24:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:25:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:26:46 KST)" (scheduled at 2025-01-15 11:25:46.939971+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:26:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:26:56 KST)" (scheduled at 2025-01-15 11:25:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:26:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:27:46 KST)" (scheduled at 2025-01-15 11:26:46.939971+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:27:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:27:56 KST)" (scheduled at 2025-01-15 11:26:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:27:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:28:46 KST)" (scheduled at 2025-01-15 11:27:46.939971+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:28:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:28:56 KST)" (scheduled at 2025-01-15 11:27:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:28:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:29:46 KST)" (scheduled at 2025-01-15 11:28:46.939971+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:29:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:29:56 KST)" (scheduled at 2025-01-15 11:28:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:29:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:30:46 KST)" (scheduled at 2025-01-15 11:29:46.939971+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:30:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:30:56 KST)" (scheduled at 2025-01-15 11:29:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:30:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:31:46 KST)" (scheduled at 2025-01-15 11:30:46.939971+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:31:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:31:56 KST)" (scheduled at 2025-01-15 11:30:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:31:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:32:46 KST)" (scheduled at 2025-01-15 11:31:46.939971+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:32:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:32:56 KST)" (scheduled at 2025-01-15 11:31:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:32:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:33:46 KST)" (scheduled at 2025-01-15 11:32:46.939971+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:33:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:33:56 KST)" (scheduled at 2025-01-15 11:32:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:33:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:34:46 KST)" (scheduled at 2025-01-15 11:33:46.939971+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:34:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:34:56 KST)" (scheduled at 2025-01-15 11:33:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:34:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:35:46 KST)" (scheduled at 2025-01-15 11:34:46.939971+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:35:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:35:56 KST)" (scheduled at 2025-01-15 11:34:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:35:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:36:46 KST)" (scheduled at 2025-01-15 11:35:46.939971+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:36:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:36:56 KST)" (scheduled at 2025-01-15 11:35:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:36:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:37:46 KST)" (scheduled at 2025-01-15 11:36:46.939971+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:37:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:37:56 KST)" (scheduled at 2025-01-15 11:36:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:37:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:38:46 KST)" (scheduled at 2025-01-15 11:37:46.939971+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:38:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:38:56 KST)" (scheduled at 2025-01-15 11:37:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:38:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:39:46 KST)" (scheduled at 2025-01-15 11:38:46.939971+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:39:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:39:56 KST)" (scheduled at 2025-01-15 11:38:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:39:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:40:46 KST)" (scheduled at 2025-01-15 11:39:46.939971+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:40:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:40:56 KST)" (scheduled at 2025-01-15 11:39:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:40:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:41:46 KST)" (scheduled at 2025-01-15 11:40:46.939971+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:41:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:41:56 KST)" (scheduled at 2025-01-15 11:40:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:41:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:42:46 KST)" (scheduled at 2025-01-15 11:41:46.939971+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:42:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:42:56 KST)" (scheduled at 2025-01-15 11:41:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:42:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:43:46 KST)" (scheduled at 2025-01-15 11:42:46.939971+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:43:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:43:56 KST)" (scheduled at 2025-01-15 11:42:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:43:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:44:46 KST)" (scheduled at 2025-01-15 11:43:46.939971+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:44:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:44:56 KST)" (scheduled at 2025-01-15 11:43:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:44:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:45:46 KST)" (scheduled at 2025-01-15 11:44:46.939971+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:45:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:45:56 KST)" (scheduled at 2025-01-15 11:44:56.912050+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:45:56 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:46:46 KST)" (scheduled at 2025-01-15 11:45:46.939971+09:00)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
INFO:werkzeug:[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
INFO:werkzeug:[33mPress CTRL+C to quit[0m
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 524-771-621
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:48:17 KST)" (scheduled at 2025-01-15 11:48:17.023587+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:48:23 KST)" (scheduled at 2025-01-15 11:48:23.134604+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:49:23 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:werkzeug: * Detected change in 'C:\\Users\\soldesk\\AppData\\Roaming\\Python\\Python310\\site-packages\\apscheduler\\executors\\base.py', reloading
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 524-771-621
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:49:17 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:50:17 KST)" (scheduled at 2025-01-15 11:49:17.023587+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:50:17 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 263, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\xa3' in position 112: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:49:51 KST)" (scheduled at 2025-01-15 11:49:51.016673+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:50:51 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:51:17 KST)" (scheduled at 2025-01-15 11:50:17.023587+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:51:17 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:51:51 KST)" (scheduled at 2025-01-15 11:50:51.016673+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:51:51 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:werkzeug: * Detected change in 'C:\\Users\\soldesk\\AppData\\Roaming\\Python\\Python310\\site-packages\\openai\\resources\\chat\\__init__.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\soldesk\\AppData\\Roaming\\Python\\Python310\\site-packages\\openai\\resources\\chat\\chat.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\soldesk\\AppData\\Roaming\\Python\\Python310\\site-packages\\openai\\_resource.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\soldesk\\AppData\\Roaming\\Python\\Python310\\site-packages\\openai\\_compat.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\soldesk\\AppData\\Roaming\\Python\\Python310\\site-packages\\openai\\resources\\chat\\completions.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\soldesk\\AppData\\Roaming\\Python\\Python310\\site-packages\\openai\\_utils\\_utils.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\soldesk\\AppData\\Roaming\\Python\\Python310\\site-packages\\openai\\types\\chat\\chat_completion_message_param.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\soldesk\\AppData\\Roaming\\Python\\Python310\\site-packages\\openai\\types\\chat\\chat_completion_developer_message_param.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\soldesk\\AppData\\Roaming\\Python\\Python310\\site-packages\\openai\\types\\chat\\chat_completion_content_part_text_param.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\soldesk\\AppData\\Roaming\\Python\\Python310\\site-packages\\openai\\types\\chat\\chat_completion_system_message_param.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\soldesk\\AppData\\Roaming\\Python\\Python310\\site-packages\\openai\\types\\chat\\chat_completion_user_message_param.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\soldesk\\AppData\\Roaming\\Python\\Python310\\site-packages\\openai\\types\\chat\\chat_completion_content_part_param.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\soldesk\\AppData\\Roaming\\Python\\Python310\\site-packages\\openai\\types\\chat\\chat_completion_content_part_image_param.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\soldesk\\AppData\\Roaming\\Python\\Python310\\site-packages\\openai\\types\\chat\\chat_completion_content_part_input_audio_param.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\soldesk\\AppData\\Roaming\\Python\\Python310\\site-packages\\openai\\types\\chat\\chat_completion_assistant_message_param.py', reloading
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:52:17 KST)" (scheduled at 2025-01-15 11:51:17.023587+09:00)
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:52:17 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 524-771-621
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:53:17 KST)" (scheduled at 2025-01-15 11:52:17.023587+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:53:17 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:52:28 KST)" (scheduled at 2025-01-15 11:52:28.146296+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:53:28 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\envs\\ai\\Lib\\site-packages\\tornado\\web.py', reloading
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\envs\\ai\\Lib\\site-packages\\jupyter_server\\services\\kernels\\websocket.py', reloading
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\envs\\ai\\Lib\\site-packages\\tornado\\websocket.py', reloading
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 524-771-621
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:54:17 KST)" (scheduled at 2025-01-15 11:53:17.023587+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:54:17 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:54:07 KST)" (scheduled at 2025-01-15 11:54:07.246180+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:55:17 KST)" (scheduled at 2025-01-15 11:54:17.023587+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:55:07 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:55:17 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\envs\\ai\\Lib\\site-packages\\IPython\\core\\interactiveshell.py', reloading
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 524-771-621
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:56:17 KST)" (scheduled at 2025-01-15 11:55:17.023587+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:56:17 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py', reloading
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 524-771-621
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:57:17 KST)" (scheduled at 2025-01-15 11:56:17.023587+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:57:17 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:werkzeug: * Detected change in 'C:\\Users\\soldesk\\AppData\\Roaming\\Python\\Python310\\site-packages\\selenium\\webdriver\\remote\\webdriver.py', reloading
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 524-771-621
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:58:17 KST)" (scheduled at 2025-01-15 11:57:17.023587+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:58:17 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:werkzeug: * Detected change in 'C:\\Users\\soldesk\\AppData\\Roaming\\Python\\Python310\\site-packages\\selenium\\webdriver\\common\\by.py', reloading
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 524-771-621
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\_pytest\\doctest.py', reloading
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 524-771-621
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:59:17 KST)" (scheduled at 2025-01-15 11:58:17.023587+09:00)
INFO:werkzeug: * Detected change in 'C:\\kd\\ws_java\\team4_v2sbm3c\\src\\main\\python\\crawling.py', reloading
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:59:17 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    VALUES (newscrawling_seq.NEXTVAL, :title, :content, sysdate, 'BBC', :url, 'main')
UnboundLocalError: local variable 'title' referenced before assignment
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 524-771-621
INFO:werkzeug: * Detected change in 'C:\\Users\\soldesk\\AppData\\Roaming\\Python\\Python310\\site-packages\\webdriver_manager\\chrome.py', reloading
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\os.py', reloading
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\posixpath.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\soldesk\\AppData\\Roaming\\Python\\Python310\\site-packages\\selenium\\webdriver\\chrome\\service.py', reloading
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 524-771-621
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:00:17 KST)" (scheduled at 2025-01-15 11:59:17.023587+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:00:17 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 263, in crawl_and_insert_data
    # 조건을 만족하지 않으면 텍스트 출력
UnicodeEncodeError: 'cp949' codec can't encode character '\xa3' in position 112: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 11:59:54 KST)" (scheduled at 2025-01-15 11:59:54.856404+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:00:54 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 319, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:01:17 KST)" (scheduled at 2025-01-15 12:00:17.023587+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:01:17 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    VALUES (newscrawling_seq.NEXTVAL, :title, :content, sysdate, 'BBC', :url, 'main')
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:01:54 KST)" (scheduled at 2025-01-15 12:00:54.856404+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:01:54 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 319, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:02:17 KST)" (scheduled at 2025-01-15 12:01:17.023587+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:02:54 KST)" (scheduled at 2025-01-15 12:01:54.856404+09:00)
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:02:17 KST)" executed successfully
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:02:54 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 319, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:03:17 KST)" (scheduled at 2025-01-15 12:02:17.023587+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:03:17 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    VALUES (newscrawling_seq.NEXTVAL, :title, :content, sysdate, 'BBC', :url, 'main')
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:03:54 KST)" (scheduled at 2025-01-15 12:02:54.856404+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:03:54 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 319, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:04:17 KST)" (scheduled at 2025-01-15 12:03:17.023587+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:04:54 KST)" (scheduled at 2025-01-15 12:03:54.856404+09:00)
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:04:17 KST)" executed successfully
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:04:54 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 319, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:05:17 KST)" (scheduled at 2025-01-15 12:04:17.023587+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:05:17 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    VALUES (newscrawling_seq.NEXTVAL, :title, :content, sysdate, 'BBC', :url, 'main')
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:05:54 KST)" (scheduled at 2025-01-15 12:04:54.856404+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:05:54 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 319, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:06:17 KST)" (scheduled at 2025-01-15 12:05:17.023587+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:06:17 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    VALUES (newscrawling_seq.NEXTVAL, :title, :content, sysdate, 'BBC', :url, 'main')
UnboundLocalError: local variable 'title' referenced before assignment
INFO:werkzeug: * Detected change in 'C:\\kd\\ws_java\\team4_v2sbm3c\\src\\main\\python\\crawling.py', reloading
INFO:werkzeug: * Detected change in 'C:\\kd\\ws_java\\team4_v2sbm3c\\src\\main\\python\\crawling.py', reloading
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 524-771-621
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:07:17 KST)" (scheduled at 2025-01-15 12:06:17.023587+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:07:17 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:07:02 KST)" (scheduled at 2025-01-15 12:07:02.709880+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:08:02 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:08:17 KST)" (scheduled at 2025-01-15 12:07:17.023587+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:08:17 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:09:02 KST)" (scheduled at 2025-01-15 12:08:02.709880+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:09:17 KST)" (scheduled at 2025-01-15 12:08:17.023587+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:09:17 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:09:02 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:10:02 KST)" (scheduled at 2025-01-15 12:09:02.709880+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:10:02 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:10:17 KST)" (scheduled at 2025-01-15 12:09:17.023587+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:10:17 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:11:02 KST)" (scheduled at 2025-01-15 12:10:02.709880+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:11:02 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:werkzeug: * Detected change in 'C:\\kd\\ws_java\\team4_v2sbm3c\\src\\main\\python\\crawling.py', reloading
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:11:17 KST)" (scheduled at 2025-01-15 12:10:17.023587+09:00)
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 524-771-621
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:11:17 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:12:17 KST)" (scheduled at 2025-01-15 12:11:17.023587+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:11:18 KST)" (scheduled at 2025-01-15 12:11:18.957414+09:00)
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:12:17 KST)" executed successfully
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:12:18 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:13:17 KST)" (scheduled at 2025-01-15 12:12:17.023587+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:13:18 KST)" (scheduled at 2025-01-15 12:12:18.957414+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:13:17 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:13:18 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:14:17 KST)" (scheduled at 2025-01-15 12:13:17.023587+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:14:18 KST)" (scheduled at 2025-01-15 12:13:18.957414+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:14:17 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:14:18 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:15:17 KST)" (scheduled at 2025-01-15 12:14:17.023587+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:15:18 KST)" (scheduled at 2025-01-15 12:14:18.957414+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:15:17 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:15:18 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:16:17 KST)" (scheduled at 2025-01-15 12:15:17.023587+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:16:18 KST)" (scheduled at 2025-01-15 12:15:18.957414+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:16:17 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:16:18 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:17:17 KST)" (scheduled at 2025-01-15 12:16:17.023587+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:17:18 KST)" (scheduled at 2025-01-15 12:16:18.957414+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:17:17 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:17:18 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:18:17 KST)" (scheduled at 2025-01-15 12:17:17.023587+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:18:18 KST)" (scheduled at 2025-01-15 12:17:18.957414+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:18:17 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:18:18 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:19:17 KST)" (scheduled at 2025-01-15 12:18:17.023587+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:19:18 KST)" (scheduled at 2025-01-15 12:18:18.957414+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:19:17 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:20:17 KST)" (scheduled at 2025-01-15 12:19:17.023587+09:00)
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:19:18 KST)" skipped: maximum number of running instances reached (1)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:20:17 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\warnings.py', reloading
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:20:18 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 354, in crawl_and_insert_data
    translated_data = json.loads(response.choices[0].message.content)
  File "C:\ProgramData\Anaconda3\lib\json\__init__.py", line 346, in loads
    return _default_decoder.decode(s)
  File "C:\ProgramData\Anaconda3\lib\json\decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "C:\ProgramData\Anaconda3\lib\json\decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
json.decoder.JSONDecodeError: Unterminated string starting at: line 3 column 11 (char 73)
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 524-771-621
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:21:17 KST)" (scheduled at 2025-01-15 12:20:17.023587+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:21:17 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:21:10 KST)" (scheduled at 2025-01-15 12:21:10.317409+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:22:17 KST)" (scheduled at 2025-01-15 12:21:17.023587+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:22:17 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:22:10 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:23:10 KST)" (scheduled at 2025-01-15 12:22:10.317409+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:23:17 KST)" (scheduled at 2025-01-15 12:22:17.023587+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:23:17 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:23:10 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:24:10 KST)" (scheduled at 2025-01-15 12:23:10.317409+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:24:17 KST)" (scheduled at 2025-01-15 12:23:17.023587+09:00)
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:24:10 KST)" executed successfully
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:24:17 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:25:10 KST)" (scheduled at 2025-01-15 12:24:10.317409+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:25:17 KST)" (scheduled at 2025-01-15 12:24:17.023587+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:25:17 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:25:10 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:26:10 KST)" (scheduled at 2025-01-15 12:25:10.317409+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:26:17 KST)" (scheduled at 2025-01-15 12:25:17.023587+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:26:17 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:26:10 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:27:10 KST)" (scheduled at 2025-01-15 12:26:10.317409+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:27:17 KST)" (scheduled at 2025-01-15 12:26:17.023587+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:27:17 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 318, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:27:10 KST)" executed successfully
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
INFO:werkzeug:[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
INFO:werkzeug:[33mPress CTRL+C to quit[0m
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 524-771-621
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:41:36 KST)" (scheduled at 2025-01-15 12:41:36.223100+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:42:36 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 138, in crawl_and_insert_data
    last_url = str(df.iloc[0]['URL']).strip()
  File "C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\indexing.py", line 1073, in __getitem__
    return self._getitem_axis(maybe_callable, axis=axis)
  File "C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\indexing.py", line 1625, in _getitem_axis
    self._validate_integer(key, axis)
  File "C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\indexing.py", line 1557, in _validate_integer
    raise IndexError("single positional indexer is out-of-bounds")
IndexError: single positional indexer is out-of-bounds
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:41:42 KST)" (scheduled at 2025-01-15 12:41:42.276908+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:42:42 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 138, in crawl_and_insert_data
    last_url = str(df.iloc[0]['URL']).strip()
  File "C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\indexing.py", line 1073, in __getitem__
    return self._getitem_axis(maybe_callable, axis=axis)
  File "C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\indexing.py", line 1625, in _getitem_axis
    self._validate_integer(key, axis)
  File "C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\indexing.py", line 1557, in _validate_integer
    raise IndexError("single positional indexer is out-of-bounds")
IndexError: single positional indexer is out-of-bounds
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:43:36 KST)" (scheduled at 2025-01-15 12:42:36.223100+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:43:36 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 138, in crawl_and_insert_data
    last_url = str(df.iloc[0]['URL']).strip()
  File "C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\indexing.py", line 1073, in __getitem__
    return self._getitem_axis(maybe_callable, axis=axis)
  File "C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\indexing.py", line 1625, in _getitem_axis
    self._validate_integer(key, axis)
  File "C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\indexing.py", line 1557, in _validate_integer
    raise IndexError("single positional indexer is out-of-bounds")
IndexError: single positional indexer is out-of-bounds
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:43:42 KST)" (scheduled at 2025-01-15 12:42:42.276908+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:43:42 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 138, in crawl_and_insert_data
    last_url = str(df.iloc[0]['URL']).strip()
  File "C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\indexing.py", line 1073, in __getitem__
    return self._getitem_axis(maybe_callable, axis=axis)
  File "C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\indexing.py", line 1625, in _getitem_axis
    self._validate_integer(key, axis)
  File "C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\indexing.py", line 1557, in _validate_integer
    raise IndexError("single positional indexer is out-of-bounds")
IndexError: single positional indexer is out-of-bounds
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:44:36 KST)" (scheduled at 2025-01-15 12:43:36.223100+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:44:36 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 138, in crawl_and_insert_data
    last_url = str(df.iloc[0]['URL']).strip()
  File "C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\indexing.py", line 1073, in __getitem__
    return self._getitem_axis(maybe_callable, axis=axis)
  File "C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\indexing.py", line 1625, in _getitem_axis
    self._validate_integer(key, axis)
  File "C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\indexing.py", line 1557, in _validate_integer
    raise IndexError("single positional indexer is out-of-bounds")
IndexError: single positional indexer is out-of-bounds
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:44:42 KST)" (scheduled at 2025-01-15 12:43:42.276908+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:44:42 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 138, in crawl_and_insert_data
    last_url = str(df.iloc[0]['URL']).strip()
  File "C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\indexing.py", line 1073, in __getitem__
    return self._getitem_axis(maybe_callable, axis=axis)
  File "C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\indexing.py", line 1625, in _getitem_axis
    self._validate_integer(key, axis)
  File "C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\indexing.py", line 1557, in _validate_integer
    raise IndexError("single positional indexer is out-of-bounds")
IndexError: single positional indexer is out-of-bounds
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:45:36 KST)" (scheduled at 2025-01-15 12:44:36.223100+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:45:36 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 138, in crawl_and_insert_data
    last_url = str(df.iloc[0]['URL']).strip()
  File "C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\indexing.py", line 1073, in __getitem__
    return self._getitem_axis(maybe_callable, axis=axis)
  File "C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\indexing.py", line 1625, in _getitem_axis
    self._validate_integer(key, axis)
  File "C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\indexing.py", line 1557, in _validate_integer
    raise IndexError("single positional indexer is out-of-bounds")
IndexError: single positional indexer is out-of-bounds
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:45:42 KST)" (scheduled at 2025-01-15 12:44:42.276908+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:45:42 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 138, in crawl_and_insert_data
    last_url = str(df.iloc[0]['URL']).strip()
  File "C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\indexing.py", line 1073, in __getitem__
    return self._getitem_axis(maybe_callable, axis=axis)
  File "C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\indexing.py", line 1625, in _getitem_axis
    self._validate_integer(key, axis)
  File "C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\indexing.py", line 1557, in _validate_integer
    raise IndexError("single positional indexer is out-of-bounds")
IndexError: single positional indexer is out-of-bounds
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:46:36 KST)" (scheduled at 2025-01-15 12:45:36.223100+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:46:36 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 138, in crawl_and_insert_data
    last_url = str(df.iloc[0]['URL']).strip()
  File "C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\indexing.py", line 1073, in __getitem__
    return self._getitem_axis(maybe_callable, axis=axis)
  File "C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\indexing.py", line 1625, in _getitem_axis
    self._validate_integer(key, axis)
  File "C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\indexing.py", line 1557, in _validate_integer
    raise IndexError("single positional indexer is out-of-bounds")
IndexError: single positional indexer is out-of-bounds
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:46:42 KST)" (scheduled at 2025-01-15 12:45:42.276908+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:46:42 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 138, in crawl_and_insert_data
    last_url = str(df.iloc[0]['URL']).strip()
  File "C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\indexing.py", line 1073, in __getitem__
    return self._getitem_axis(maybe_callable, axis=axis)
  File "C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\indexing.py", line 1625, in _getitem_axis
    self._validate_integer(key, axis)
  File "C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\indexing.py", line 1557, in _validate_integer
    raise IndexError("single positional indexer is out-of-bounds")
IndexError: single positional indexer is out-of-bounds
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:47:36 KST)" (scheduled at 2025-01-15 12:46:36.223100+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:47:36 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 138, in crawl_and_insert_data
    last_url = str(df.iloc[0]['URL']).strip()
  File "C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\indexing.py", line 1073, in __getitem__
    return self._getitem_axis(maybe_callable, axis=axis)
  File "C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\indexing.py", line 1625, in _getitem_axis
    self._validate_integer(key, axis)
  File "C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\indexing.py", line 1557, in _validate_integer
    raise IndexError("single positional indexer is out-of-bounds")
IndexError: single positional indexer is out-of-bounds
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:47:42 KST)" (scheduled at 2025-01-15 12:46:42.276908+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:47:42 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 138, in crawl_and_insert_data
    last_url = str(df.iloc[0]['URL']).strip()
  File "C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\indexing.py", line 1073, in __getitem__
    return self._getitem_axis(maybe_callable, axis=axis)
  File "C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\indexing.py", line 1625, in _getitem_axis
    self._validate_integer(key, axis)
  File "C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\indexing.py", line 1557, in _validate_integer
    raise IndexError("single positional indexer is out-of-bounds")
IndexError: single positional indexer is out-of-bounds
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:48:36 KST)" (scheduled at 2025-01-15 12:47:36.223100+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:48:36 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 138, in crawl_and_insert_data
    last_url = str(df.iloc[0]['URL']).strip()
  File "C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\indexing.py", line 1073, in __getitem__
    return self._getitem_axis(maybe_callable, axis=axis)
  File "C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\indexing.py", line 1625, in _getitem_axis
    self._validate_integer(key, axis)
  File "C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\indexing.py", line 1557, in _validate_integer
    raise IndexError("single positional indexer is out-of-bounds")
IndexError: single positional indexer is out-of-bounds
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:48:42 KST)" (scheduled at 2025-01-15 12:47:42.276908+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:48:42 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 138, in crawl_and_insert_data
    last_url = str(df.iloc[0]['URL']).strip()
  File "C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\indexing.py", line 1073, in __getitem__
    return self._getitem_axis(maybe_callable, axis=axis)
  File "C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\indexing.py", line 1625, in _getitem_axis
    self._validate_integer(key, axis)
  File "C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\indexing.py", line 1557, in _validate_integer
    raise IndexError("single positional indexer is out-of-bounds")
IndexError: single positional indexer is out-of-bounds
INFO:werkzeug: * Detected change in 'C:\\kd\\ws_java\\team4_v2sbm3c\\src\\main\\python\\pernews.py', reloading
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\numpy\\__config__.py', reloading
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 524-771-621
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:49:36 KST)" (scheduled at 2025-01-15 12:48:36.223100+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:49:36 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:49:29 KST)" (scheduled at 2025-01-15 12:49:29.026249+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:50:29 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:50:36 KST)" (scheduled at 2025-01-15 12:49:36.223100+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:50:36 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:51:29 KST)" (scheduled at 2025-01-15 12:50:29.026249+09:00)
INFO:werkzeug: * Detected change in 'C:\\kd\\ws_java\\team4_v2sbm3c\\src\\main\\python\\crawling.py', reloading
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:51:29 KST)" executed successfully
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:51:36 KST)" (scheduled at 2025-01-15 12:50:36.223100+09:00)
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:51:36 KST)" executed successfully
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 524-771-621
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:52:36 KST)" (scheduled at 2025-01-15 12:51:36.223100+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:52:36 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:51:43 KST)" (scheduled at 2025-01-15 12:51:43.178728+09:00)
INFO:werkzeug: * Detected change in 'C:\\kd\\ws_java\\team4_v2sbm3c\\src\\main\\python\\crawling.py', reloading
INFO:werkzeug: * Detected change in 'C:\\kd\\ws_java\\team4_v2sbm3c\\src\\main\\python\\crawling.py', reloading
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:52:43 KST)" executed successfully
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 524-771-621
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:53:36 KST)" (scheduled at 2025-01-15 12:52:36.223100+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:53:36 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 330, in crawl_and_insert_data
    """, {'title': title, 'content': content_text, 'url': url})
UnboundLocalError: local variable 'title' referenced before assignment
INFO:werkzeug: * Detected change in 'C:\\kd\\ws_java\\team4_v2sbm3c\\src\\main\\python\\crawling.py', reloading
INFO:werkzeug: * Detected change in 'C:\\kd\\ws_java\\team4_v2sbm3c\\src\\main\\python\\crawling.py', reloading
INFO:werkzeug: * Detected change in 'C:\\kd\\ws_java\\team4_v2sbm3c\\src\\main\\python\\crawling.py', reloading
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 524-771-621
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:54:36 KST)" (scheduled at 2025-01-15 12:53:36.223100+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:54:36 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:54:04 KST)" (scheduled at 2025-01-15 12:54:04.586382+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:55:04 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:55:36 KST)" (scheduled at 2025-01-15 12:54:36.223100+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:55:36 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:56:04 KST)" (scheduled at 2025-01-15 12:55:04.586382+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:56:04 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:56:36 KST)" (scheduled at 2025-01-15 12:55:36.223100+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:56:36 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:57:04 KST)" (scheduled at 2025-01-15 12:56:04.586382+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:57:04 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:57:36 KST)" (scheduled at 2025-01-15 12:56:36.223100+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:57:36 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:58:04 KST)" (scheduled at 2025-01-15 12:57:04.586382+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 12:58:04 KST)" executed successfully
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
INFO:werkzeug:[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
INFO:werkzeug:[33mPress CTRL+C to quit[0m
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 524-771-621
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:12:01 KST)" (scheduled at 2025-01-15 15:12:01.757268+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:12:10 KST)" (scheduled at 2025-01-15 15:12:10.812229+09:00)
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:13:01 KST)" skipped: maximum number of running instances reached (1)
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:13:10 KST)" skipped: maximum number of running instances reached (1)
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:14:01 KST)" skipped: maximum number of running instances reached (1)
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:14:10 KST)" skipped: maximum number of running instances reached (1)
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:15:01 KST)" skipped: maximum number of running instances reached (1)
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:15:10 KST)" skipped: maximum number of running instances reached (1)
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:16:01 KST)" skipped: maximum number of running instances reached (1)
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:16:10 KST)" skipped: maximum number of running instances reached (1)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
INFO:werkzeug:[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
INFO:werkzeug:[33mPress CTRL+C to quit[0m
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 524-771-621
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:17:45 KST)" (scheduled at 2025-01-15 15:17:45.477794+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:17:52 KST)" (scheduled at 2025-01-15 15:17:52.146942+09:00)
INFO:werkzeug: * Detected change in 'C:\\Users\\soldesk\\AppData\\Roaming\\Python\\Python310\\site-packages\\openai\\types\\__init__.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\soldesk\\AppData\\Roaming\\Python\\Python310\\site-packages\\openai\\types\\batch.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\soldesk\\AppData\\Roaming\\Python\\Python310\\site-packages\\pydantic\\_internal\\_core_utils.py', reloading
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:18:45 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 328, in crawl_and_insert_data
    from openai import OpenAI # 1.x~
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\openai\__init__.py", line 8, in <module>
    from . import types
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\openai\types\__init__.py", line 5, in <module>
    from .batch import Batch as Batch
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\openai\types\batch.py", line 7, in <module>
    from .._models import BaseModel
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\openai\_models.py", line 37, in <module>
    from ._utils import (
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\openai\_utils\__init__.py", line 1, in <module>
    from ._logs import SensitiveHeadersFilter as SensitiveHeadersFilter
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\openai\_utils\_logs.py", line 5, in <module>
    from ._utils import is_dict
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\openai\_utils\_utils.py", line 26, in <module>
    from .._compat import parse_date as parse_date, parse_datetime as parse_datetime
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\openai\_compat.py", line 189, in <module>
    class GenericModel(pydantic.BaseModel): ...
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\pydantic\_internal\_model_construction.py", line 224, in __new__
    complete_model_class(
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\pydantic\_internal\_model_construction.py", line 587, in complete_model_class
    schema = gen_schema.clean_schema(schema)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\pydantic\_internal\_generate_schema.py", line 595, in clean_schema
    schema = validate_core_schema(schema)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\pydantic\_internal\_core_utils.py", line 570, in validate_core_schema
    return _validate_core_schema(schema)
pyo3_runtime.PanicException: Error building schema validator:
  MemoryError: 
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:19:45 KST)" (scheduled at 2025-01-15 15:18:45.477794+09:00)
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:18:52 KST)" skipped: maximum number of running instances reached (1)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
INFO:werkzeug:[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
INFO:werkzeug:[33mPress CTRL+C to quit[0m
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 524-771-621
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:27:23 KST)" (scheduled at 2025-01-15 15:27:23.838309+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:27:29 KST)" (scheduled at 2025-01-15 15:27:29.297744+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:28:23 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:28:29 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:29:23 KST)" (scheduled at 2025-01-15 15:28:23.838309+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:29:23 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:29:29 KST)" (scheduled at 2025-01-15 15:28:29.297744+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:29:29 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:30:23 KST)" (scheduled at 2025-01-15 15:29:23.838309+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:30:29 KST)" (scheduled at 2025-01-15 15:29:29.297744+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:30:29 KST)" executed successfully
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:30:23 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:31:23 KST)" (scheduled at 2025-01-15 15:30:23.838309+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:31:23 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:31:29 KST)" (scheduled at 2025-01-15 15:30:29.297744+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:31:29 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:32:23 KST)" (scheduled at 2025-01-15 15:31:23.838309+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:32:23 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:32:29 KST)" (scheduled at 2025-01-15 15:31:29.297744+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:32:29 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:33:23 KST)" (scheduled at 2025-01-15 15:32:23.838309+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:33:23 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:33:29 KST)" (scheduled at 2025-01-15 15:32:29.297744+09:00)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:33:29 KST)" executed successfully
INFO:WDM:====== WebDriver manager ======
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:WDM:====== WebDriver manager ======
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
INFO:werkzeug:[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
INFO:werkzeug:[33mPress CTRL+C to quit[0m
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
INFO:werkzeug:[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
INFO:werkzeug:[33mPress CTRL+C to quit[0m
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:WDM:====== WebDriver manager ======
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 524-771-621
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 524-771-621
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:34:23 KST)" (scheduled at 2025-01-15 15:33:23.838309+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:34:29 KST)" (scheduled at 2025-01-15 15:33:29.297744+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:34:29 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:33:41 KST)" (scheduled at 2025-01-15 15:33:41.118004+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:33:43 KST)" (scheduled at 2025-01-15 15:33:43.684480+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:34:41 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:33:48 KST)" (scheduled at 2025-01-15 15:33:48.944863+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:33:51 KST)" (scheduled at 2025-01-15 15:33:51.502609+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:34:43 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:34:48 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:34:51 KST)" executed successfully
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:34:23 KST)" skipped: maximum number of running instances reached (1)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:35:29 KST)" (scheduled at 2025-01-15 15:34:29.297744+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:35:29 KST)" executed successfully
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\json\\__init__.py', reloading
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\json\\__init__.py', reloading
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\json\\decoder.py', reloading
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\json\\__init__.py', reloading
at: 2025-01-15 15:35:23 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 353, in crawl_and_insert_data
    translated_data = json.loads(response.choices[0].message.content)
  File "C:\ProgramData\Anaconda3\lib\json\__init__.py", line 346, in loads
    return _default_decoder.decode(s)
  File "C:\ProgramData\Anaconda3\lib\json\decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "C:\ProgramData\Anaconda3\lib\json\decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 3 column 4300 (char 4337)
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\json\\decoder.py', reloading
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\json\\decoder.py', reloading
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:====== WebDriver manager ======
INFO:WDM:====== WebDriver manager ======
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:35:41 KST)" (scheduled at 2025-01-15 15:34:41.118004+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:35:43 KST)" (scheduled at 2025-01-15 15:34:43.684480+09:00)
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 524-771-621
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:35:43 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:35:41 KST)" executed successfully
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 524-771-621
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 524-771-621
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:36:23 KST)" (scheduled at 2025-01-15 15:35:23.838309+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:36:23 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:36:41 KST)" (scheduled at 2025-01-15 15:35:41.118004+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:36:43 KST)" (scheduled at 2025-01-15 15:35:43.684480+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:36:41 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:36:43 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:35:49 KST)" (scheduled at 2025-01-15 15:35:49.295589+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:36:49 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:35:59 KST)" (scheduled at 2025-01-15 15:35:59.134623+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:35:59 KST)" (scheduled at 2025-01-15 15:35:59.470723+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:37:23 KST)" (scheduled at 2025-01-15 15:36:23.838309+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:37:23 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:36:59 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:37:41 KST)" (scheduled at 2025-01-15 15:36:41.118004+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:37:43 KST)" (scheduled at 2025-01-15 15:36:43.684480+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:37:49 KST)" (scheduled at 2025-01-15 15:36:49.295589+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:37:43 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:37:41 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:37:49 KST)" executed successfully
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:37:59 KST)" (scheduled at 2025-01-15 15:36:59.134623+09:00)
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:36:59 KST)" skipped: maximum number of running instances reached (1)
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:37:59 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:37:59 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:38:23 KST)" (scheduled at 2025-01-15 15:37:23.838309+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:38:41 KST)" (scheduled at 2025-01-15 15:37:41.118004+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:38:43 KST)" (scheduled at 2025-01-15 15:37:43.684480+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:38:41 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:38:43 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:38:49 KST)" (scheduled at 2025-01-15 15:37:49.295589+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:38:49 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:38:59 KST)" (scheduled at 2025-01-15 15:37:59.134623+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:38:59 KST)" (scheduled at 2025-01-15 15:37:59.470723+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:38:59 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:38:59 KST)" executed successfully
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:38:23 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 353, in crawl_and_insert_data
    translated_data = json.loads(response.choices[0].message.content)
  File "C:\ProgramData\Anaconda3\lib\json\__init__.py", line 346, in loads
    return _default_decoder.decode(s)
  File "C:\ProgramData\Anaconda3\lib\json\decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "C:\ProgramData\Anaconda3\lib\json\decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 3 column 4297 (char 4334)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:39:23 KST)" (scheduled at 2025-01-15 15:38:23.838309+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:39:41 KST)" (scheduled at 2025-01-15 15:38:41.118004+09:00)
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:39:43 KST)" (scheduled at 2025-01-15 15:38:43.684480+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:39:41 KST)" executed successfully
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:39:23 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:39:43 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:39:49 KST)" (scheduled at 2025-01-15 15:38:49.295589+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:39:49 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:39:59 KST)" (scheduled at 2025-01-15 15:38:59.134623+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:39:59 KST)" (scheduled at 2025-01-15 15:38:59.470723+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:39:59 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:39:59 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:40:23 KST)" (scheduled at 2025-01-15 15:39:23.838309+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:40:23 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:40:41 KST)" (scheduled at 2025-01-15 15:39:41.118004+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:40:43 KST)" (scheduled at 2025-01-15 15:39:43.684480+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:40:41 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:40:43 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:40:49 KST)" (scheduled at 2025-01-15 15:39:49.295589+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:40:49 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:40:59 KST)" (scheduled at 2025-01-15 15:39:59.134623+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:40:59 KST)" (scheduled at 2025-01-15 15:39:59.470723+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:40:59 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:40:59 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:41:23 KST)" (scheduled at 2025-01-15 15:40:23.838309+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:41:23 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:41:41 KST)" (scheduled at 2025-01-15 15:40:41.118004+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:41:43 KST)" (scheduled at 2025-01-15 15:40:43.684480+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:41:41 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:41:43 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:41:49 KST)" (scheduled at 2025-01-15 15:40:49.295589+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:41:49 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:41:59 KST)" (scheduled at 2025-01-15 15:40:59.134623+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:41:59 KST)" (scheduled at 2025-01-15 15:40:59.470723+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:41:59 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:41:59 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:42:23 KST)" (scheduled at 2025-01-15 15:41:23.838309+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:42:23 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:42:41 KST)" (scheduled at 2025-01-15 15:41:41.118004+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:42:43 KST)" (scheduled at 2025-01-15 15:41:43.684480+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:42:41 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:42:43 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:42:49 KST)" (scheduled at 2025-01-15 15:41:49.295589+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:42:49 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:42:59 KST)" (scheduled at 2025-01-15 15:41:59.134623+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:42:59 KST)" (scheduled at 2025-01-15 15:41:59.470723+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:42:59 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:42:59 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:43:23 KST)" (scheduled at 2025-01-15 15:42:23.838309+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:43:23 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:43:41 KST)" (scheduled at 2025-01-15 15:42:41.118004+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:43:43 KST)" (scheduled at 2025-01-15 15:42:43.684480+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:43:41 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:43:43 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:43:49 KST)" (scheduled at 2025-01-15 15:42:49.295589+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:43:49 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:43:59 KST)" (scheduled at 2025-01-15 15:42:59.134623+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:43:59 KST)" (scheduled at 2025-01-15 15:42:59.470723+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:43:59 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:43:59 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:44:23 KST)" (scheduled at 2025-01-15 15:43:23.838309+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:44:23 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:44:41 KST)" (scheduled at 2025-01-15 15:43:41.118004+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:44:43 KST)" (scheduled at 2025-01-15 15:43:43.684480+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:44:43 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:44:49 KST)" (scheduled at 2025-01-15 15:43:49.295589+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:44:41 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:44:49 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:44:59 KST)" (scheduled at 2025-01-15 15:43:59.134623+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:44:59 KST)" (scheduled at 2025-01-15 15:43:59.470723+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:44:59 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:44:59 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:45:23 KST)" (scheduled at 2025-01-15 15:44:23.838309+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:45:23 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:45:41 KST)" (scheduled at 2025-01-15 15:44:41.118004+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:45:43 KST)" (scheduled at 2025-01-15 15:44:43.684480+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:45:41 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:45:49 KST)" (scheduled at 2025-01-15 15:44:49.295589+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:45:43 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:45:49 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:45:59 KST)" (scheduled at 2025-01-15 15:44:59.134623+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:45:59 KST)" (scheduled at 2025-01-15 15:44:59.470723+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:45:59 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:45:59 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:46:23 KST)" (scheduled at 2025-01-15 15:45:23.838309+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:46:23 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:46:41 KST)" (scheduled at 2025-01-15 15:45:41.118004+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:46:43 KST)" (scheduled at 2025-01-15 15:45:43.684480+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:46:41 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:46:49 KST)" (scheduled at 2025-01-15 15:45:49.295589+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:46:43 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:46:49 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:46:59 KST)" (scheduled at 2025-01-15 15:45:59.134623+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:46:59 KST)" (scheduled at 2025-01-15 15:45:59.470723+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:46:59 KST)" executed successfully
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:47:23 KST)" (scheduled at 2025-01-15 15:46:23.838309+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:47:23 KST)" executed successfully
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:46:59 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:47:41 KST)" (scheduled at 2025-01-15 15:46:41.118004+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:47:43 KST)" (scheduled at 2025-01-15 15:46:43.684480+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:47:43 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:47:49 KST)" (scheduled at 2025-01-15 15:46:49.295589+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:47:41 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:47:49 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:47:59 KST)" (scheduled at 2025-01-15 15:46:59.134623+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:47:59 KST)" (scheduled at 2025-01-15 15:46:59.470723+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:47:59 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:47:59 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:48:23 KST)" (scheduled at 2025-01-15 15:47:23.838309+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:48:23 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:48:41 KST)" (scheduled at 2025-01-15 15:47:41.118004+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:48:43 KST)" (scheduled at 2025-01-15 15:47:43.684480+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:48:41 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:48:43 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:48:49 KST)" (scheduled at 2025-01-15 15:47:49.295589+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:48:49 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:48:59 KST)" (scheduled at 2025-01-15 15:47:59.134623+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:48:59 KST)" (scheduled at 2025-01-15 15:47:59.470723+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:48:59 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:48:59 KST)" executed successfully
INFO:werkzeug: * Detected change in 'C:\\kd\\ws_java\\team4_v2sbm3c\\src\\main\\python\\pernews.py', reloading
INFO:werkzeug: * Detected change in 'C:\\kd\\ws_java\\team4_v2sbm3c\\src\\main\\python\\pernews.py', reloading
INFO:werkzeug: * Detected change in 'C:\\kd\\ws_java\\team4_v2sbm3c\\src\\main\\python\\pernews.py', reloading
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:====== WebDriver manager ======
INFO:WDM:====== WebDriver manager ======
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:49:23 KST)" (scheduled at 2025-01-15 15:48:23.838309+09:00)
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 524-771-621
INFO:werkzeug: * Debugger PIN: 524-771-621
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 524-771-621
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:49:23 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:49:41 KST)" (scheduled at 2025-01-15 15:48:41.118004+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:49:43 KST)" (scheduled at 2025-01-15 15:48:43.684480+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:49:43 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:49:41 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:50:23 KST)" (scheduled at 2025-01-15 15:49:23.838309+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:50:23 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:49:31 KST)" (scheduled at 2025-01-15 15:49:31.674273+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:49:31 KST)" (scheduled at 2025-01-15 15:49:31.678263+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:50:41 KST)" (scheduled at 2025-01-15 15:49:41.118004+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:50:43 KST)" (scheduled at 2025-01-15 15:49:43.684480+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:50:43 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:50:41 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:51:23 KST)" (scheduled at 2025-01-15 15:50:23.838309+09:00)
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:50:31 KST)" skipped: maximum number of running instances reached (1)
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:50:31 KST)" skipped: maximum number of running instances reached (1)
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:50:31 KST)" skipped: maximum number of running instances reached (1)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:51:41 KST)" (scheduled at 2025-01-15 15:50:41.118004+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:51:43 KST)" (scheduled at 2025-01-15 15:50:43.684480+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:51:23 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:52:23 KST)" (scheduled at 2025-01-15 15:51:23.838309+09:00)
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:51:31 KST)" skipped: maximum number of running instances reached (1)
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:51:31 KST)" skipped: maximum number of running instances reached (1)
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:51:31 KST)" skipped: maximum number of running instances reached (1)
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:51:41 KST)" skipped: maximum number of running instances reached (1)
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:51:43 KST)" skipped: maximum number of running instances reached (1)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:52:41 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:52:31 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:52:31 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:52:43 KST)" executed successfully
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:52:23 KST)" skipped: maximum number of running instances reached (1)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:53:31 KST)" (scheduled at 2025-01-15 15:52:31.673276+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:53:31 KST)" (scheduled at 2025-01-15 15:52:31.674273+09:00)
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:52:31 KST)" skipped: maximum number of running instances reached (1)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:53:41 KST)" (scheduled at 2025-01-15 15:52:41.118004+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:53:43 KST)" (scheduled at 2025-01-15 15:52:43.684480+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:53:23 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:54:23 KST)" (scheduled at 2025-01-15 15:53:23.838309+09:00)
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:53:31 KST)" skipped: maximum number of running instances reached (1)
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:53:31 KST)" skipped: maximum number of running instances reached (1)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:54:31 KST)" executed successfully
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:53:41 KST)" skipped: maximum number of running instances reached (1)
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:53:43 KST)" skipped: maximum number of running instances reached (1)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:54:43 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:54:41 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:54:31 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:54:31 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:54:23 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:55:23 KST)" (scheduled at 2025-01-15 15:54:23.838309+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:55:23 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:55:31 KST)" (scheduled at 2025-01-15 15:54:31.674273+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:55:31 KST)" (scheduled at 2025-01-15 15:54:31.678263+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:55:31 KST)" (scheduled at 2025-01-15 15:54:31.673276+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:55:31 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:55:41 KST)" (scheduled at 2025-01-15 15:54:41.118004+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:55:43 KST)" (scheduled at 2025-01-15 15:54:43.684480+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:55:31 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:55:31 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:55:41 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:55:43 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:56:23 KST)" (scheduled at 2025-01-15 15:55:23.838309+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:56:23 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:56:31 KST)" (scheduled at 2025-01-15 15:55:31.673276+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:56:31 KST)" (scheduled at 2025-01-15 15:55:31.678263+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:56:31 KST)" (scheduled at 2025-01-15 15:55:31.674273+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:56:31 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:56:31 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:56:31 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:56:41 KST)" (scheduled at 2025-01-15 15:55:41.118004+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:56:43 KST)" (scheduled at 2025-01-15 15:55:43.684480+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:56:41 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:56:43 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:57:23 KST)" (scheduled at 2025-01-15 15:56:23.838309+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:57:23 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:57:31 KST)" (scheduled at 2025-01-15 15:56:31.674273+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:57:31 KST)" (scheduled at 2025-01-15 15:56:31.678263+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:57:31 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:57:31 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:57:31 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:57:41 KST)" (scheduled at 2025-01-15 15:56:41.118004+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:57:43 KST)" (scheduled at 2025-01-15 15:56:43.684480+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:57:41 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:57:43 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:58:23 KST)" (scheduled at 2025-01-15 15:57:23.838309+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:58:23 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:58:31 KST)" (scheduled at 2025-01-15 15:57:31.678263+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:58:31 KST)" (scheduled at 2025-01-15 15:57:31.674273+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:58:31 KST)" (scheduled at 2025-01-15 15:57:31.673276+09:00)
INFO:werkzeug: * Detected change in 'C:\\Users\\soldesk\\AppData\\Roaming\\Python\\Python310\\site-packages\\selenium\\webdriver\\remote\\webdriver.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\soldesk\\AppData\\Roaming\\Python\\Python310\\site-packages\\selenium\\webdriver\\remote\\webdriver.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\soldesk\\AppData\\Roaming\\Python\\Python310\\site-packages\\selenium\\webdriver\\remote\\webdriver.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\soldesk\\AppData\\Roaming\\Python\\Python310\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py', reloading
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:58:31 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 347, in execute
    self.error_handler.check_response(response)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: unknown error: session deleted because of page crash
from disconnected: not connected to DevTools
  (Session info: chrome=131.0.6778.265)
Stacktrace:
	GetHandleVerifier [0x00FFFD53+23747]
	(No symbol) [0x00F87D54]
	(No symbol) [0x00E5BE53]
	(No symbol) [0x00E4C408]
	(No symbol) [0x00E4C329]
	(No symbol) [0x00E5E2E0]
	(No symbol) [0x00EDAD69]
	(No symbol) [0x00EC1B96]
	(No symbol) [0x00E93F3C]
	(No symbol) [0x00E94EBD]
	GetHandleVerifier [0x012DAC73+3017699]
	GetHandleVerifier [0x012EB93B+3086507]
	GetHandleVerifier [0x012E40F2+3055714]
	GetHandleVerifier [0x01095AF0+637536]
	(No symbol) [0x00F90A5D]
	(No symbol) [0x00F8DA28]
	(No symbol) [0x00F8DBC5]
	(No symbol) [0x00F807F0]
	BaseThreadInitThunk [0x76E2FCC9+25]
	RtlGetAppContainerNamedObjectPath [0x77B2809E+286]
	RtlGetAppContainerNamedObjectPath [0x77B2806E+238]

INFO:werkzeug: * Detected change in 'C:\\Users\\soldesk\\AppData\\Roaming\\Python\\Python310\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py', reloading
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:58:31 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 347, in execute
    self.error_handler.check_response(response)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: unknown error: session deleted because of page crash
from unknown error: cannot determine loading status
from tab crashed
  (Session info: chrome=131.0.6778.265)
Stacktrace:
	GetHandleVerifier [0x00FFFD53+23747]
	(No symbol) [0x00F87D54]
	(No symbol) [0x00E5BCCD]
	(No symbol) [0x00E4D3E0]
	(No symbol) [0x00E4B8BB]
	(No symbol) [0x00E4BF2D]
	(No symbol) [0x00E581BA]
	(No symbol) [0x00E678B5]
	(No symbol) [0x00E6C726]
	(No symbol) [0x00E4C535]
	(No symbol) [0x00E674E1]
	(No symbol) [0x00EDB33D]
	(No symbol) [0x00EC1B96]
	(No symbol) [0x00E93F3C]
	(No symbol) [0x00E94EBD]
	GetHandleVerifier [0x012DAC73+3017699]
	GetHandleVerifier [0x012EB93B+3086507]
	GetHandleVerifier [0x012E40F2+3055714]
	GetHandleVerifier [0x01095AF0+637536]
	(No symbol) [0x00F90A5D]
	(No symbol) [0x00F8DA28]
	(No symbol) [0x00F8DBC5]
	(No symbol) [0x00F807F0]
	BaseThreadInitThunk [0x76E2FCC9+25]
	RtlGetAppContainerNamedObjectPath [0x77B2809E+286]
	RtlGetAppContainerNamedObjectPath [0x77B2806E+238]

INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:58:31 KST)" executed successfully
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:====== WebDriver manager ======
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:58:41 KST)" (scheduled at 2025-01-15 15:57:41.118004+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:58:41 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 347, in execute
    self.error_handler.check_response(response)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: unknown error: session deleted because of page crash
from unknown error: cannot determine loading status
from tab crashed
  (Session info: chrome=131.0.6778.265)
Stacktrace:
	GetHandleVerifier [0x00FFFD53+23747]
	(No symbol) [0x00F87D54]
	(No symbol) [0x00E5BCCD]
	(No symbol) [0x00E4D3E0]
	(No symbol) [0x00E4B8BB]
	(No symbol) [0x00E4BF2D]
	(No symbol) [0x00E581BA]
	(No symbol) [0x00E678B5]
	(No symbol) [0x00E6C726]
	(No symbol) [0x00E4C535]
	(No symbol) [0x00E674E1]
	(No symbol) [0x00EDB33D]
	(No symbol) [0x00EC1B96]
	(No symbol) [0x00E93F3C]
	(No symbol) [0x00E94EBD]
	GetHandleVerifier [0x012DAC73+3017699]
	GetHandleVerifier [0x012EB93B+3086507]
	GetHandleVerifier [0x012E40F2+3055714]
	GetHandleVerifier [0x01095AF0+637536]
	(No symbol) [0x00F90A5D]
	(No symbol) [0x00F8DA28]
	(No symbol) [0x00F8DBC5]
	(No symbol) [0x00F807F0]
	BaseThreadInitThunk [0x76E2FCC9+25]
	RtlGetAppContainerNamedObjectPath [0x77B2809E+286]
	RtlGetAppContainerNamedObjectPath [0x77B2806E+238]

INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:58:43 KST)" (scheduled at 2025-01-15 15:57:43.684480+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:58:43 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 347, in execute
    self.error_handler.check_response(response)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: disconnected: not connected to DevTools
  (failed to check if window was closed: disconnected: not connected to DevTools)
  (Session info: chrome=131.0.6778.265)
Stacktrace:
	GetHandleVerifier [0x00FFFD53+23747]
	(No symbol) [0x00F87D54]
	(No symbol) [0x00E5BE53]
	(No symbol) [0x00E4C408]
	(No symbol) [0x00E4C329]
	(No symbol) [0x00E5E2E0]
	(No symbol) [0x00EDAD69]
	(No symbol) [0x00EC1B96]
	(No symbol) [0x00E93F3C]
	(No symbol) [0x00E94EBD]
	GetHandleVerifier [0x012DAC73+3017699]
	GetHandleVerifier [0x012EB93B+3086507]
	GetHandleVerifier [0x012E40F2+3055714]
	GetHandleVerifier [0x01095AF0+637536]
	(No symbol) [0x00F90A5D]
	(No symbol) [0x00F8DA28]
	(No symbol) [0x00F8DBC5]
	(No symbol) [0x00F807F0]
	BaseThreadInitThunk [0x76E2FCC9+25]
	RtlGetAppContainerNamedObjectPath [0x77B2809E+286]
	RtlGetAppContainerNamedObjectPath [0x77B2806E+238]

INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
INFO:werkzeug: * Debugger PIN: 524-771-621
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 524-771-621
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 524-771-621
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:59:23 KST)" (scheduled at 2025-01-15 15:58:23.838309+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 15:59:23 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 347, in execute
    self.error_handler.check_response(response)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: unknown error: session deleted because of page crash
from unknown error: cannot determine loading status
from tab crashed
  (Session info: chrome=131.0.6778.265)
Stacktrace:
	GetHandleVerifier [0x00FFFD53+23747]
	(No symbol) [0x00F87D54]
	(No symbol) [0x00E5BCCD]
	(No symbol) [0x00E4D3E0]
	(No symbol) [0x00E4B8BB]
	(No symbol) [0x00E4BF2D]
	(No symbol) [0x00E581BA]
	(No symbol) [0x00E678B5]
	(No symbol) [0x00E6C726]
	(No symbol) [0x00E4C535]
	(No symbol) [0x00E674E1]
	(No symbol) [0x00EDB33D]
	(No symbol) [0x00EC1B96]
	(No symbol) [0x00E93F3C]
	(No symbol) [0x00E94EBD]
	GetHandleVerifier [0x012DAC73+3017699]
	GetHandleVerifier [0x012EB93B+3086507]
	GetHandleVerifier [0x012E40F2+3055714]
	GetHandleVerifier [0x01095AF0+637536]
	(No symbol) [0x00F90A5D]
	(No symbol) [0x00F8DA28]
	(No symbol) [0x00F8DBC5]
	(No symbol) [0x00F807F0]
	BaseThreadInitThunk [0x76E2FCC9+25]
	RtlGetAppContainerNamedObjectPath [0x77B2809E+286]
	RtlGetAppContainerNamedObjectPath [0x77B2806E+238]

INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
INFO:werkzeug:[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
INFO:werkzeug:[33mPress CTRL+C to quit[0m
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 524-771-621
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:09:39 KST)" (scheduled at 2025-01-15 16:09:39.190602+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:09:45 KST)" (scheduled at 2025-01-15 16:09:45.650620+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:10:45 KST)" executed successfully
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:10:39 KST)" skipped: maximum number of running instances reached (1)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:11:45 KST)" (scheduled at 2025-01-15 16:10:45.650620+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:11:45 KST)" executed successfully
INFO:werkzeug: * Detected change in 'C:\\Users\\soldesk\\AppData\\Roaming\\Python\\Python310\\site-packages\\selenium\\webdriver\\__init__.py', reloading
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:11:39 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 353, in crawl_and_insert_data
    translated_data = json.loads(response.choices[0].message.content)
  File "C:\ProgramData\Anaconda3\lib\json\__init__.py", line 346, in loads
    return _default_decoder.decode(s)
  File "C:\ProgramData\Anaconda3\lib\json\decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "C:\ProgramData\Anaconda3\lib\json\decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 3 column 4314 (char 4361)
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 524-771-621
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:12:39 KST)" (scheduled at 2025-01-15 16:11:39.190602+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:12:39 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:12:16 KST)" (scheduled at 2025-01-15 16:12:16.770922+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:13:16 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:13:39 KST)" (scheduled at 2025-01-15 16:12:39.190602+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:13:39 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:14:16 KST)" (scheduled at 2025-01-15 16:13:16.770922+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:14:16 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:14:39 KST)" (scheduled at 2025-01-15 16:13:39.190602+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:14:39 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:15:16 KST)" (scheduled at 2025-01-15 16:14:16.770922+09:00)
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:15:39 KST)" (scheduled at 2025-01-15 16:14:39.190602+09:00)
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:15:39 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:15:16 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:16:16 KST)" (scheduled at 2025-01-15 16:15:16.770922+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000252795EB9D0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/f980f0202b5372a21880bbae269ea68f/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000252795EA6E0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/f980f0202b5372a21880bbae269ea68f/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000252795E9960>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/f980f0202b5372a21880bbae269ea68f/url
INFO:werkzeug: * Detected change in 'C:\\Users\\soldesk\\AppData\\Roaming\\Python\\Python310\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py', reloading
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\urllib3\\request.py', reloading
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\urllib3\\poolmanager.py', reloading
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py', reloading
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:16:16 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x00000252795EABC0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=55939): Max retries exceeded with url: /session/f980f0202b5372a21880bbae269ea68f/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000252795EABC0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\urllib3\\util\\retry.py', reloading
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\urllib3\\connection.py', reloading
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\http\\client.py', reloading
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\urllib3\\util\\connection.py', reloading
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:16:39 KST)" (scheduled at 2025-01-15 16:15:39.190602+09:00)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 524-771-621
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:16:39 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:17:39 KST)" (scheduled at 2025-01-15 16:16:39.190602+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F7ED2CE0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:16:46 KST)" (scheduled at 2025-01-15 16:16:46.027289+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F7ED3760>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F80A93C0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:17:39 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000002C9F80A95A0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=55482): Max retries exceeded with url: /session/c09b198de9cf0751cb997bc8dc24cabf/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F80A95A0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:17:46 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:18:39 KST)" (scheduled at 2025-01-15 16:17:39.190602+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F80A97B0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:18:46 KST)" (scheduled at 2025-01-15 16:17:46.027289+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F80A9960>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC66F970>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F80A99F0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC66FA60>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:18:39 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000002C9F80A9B40>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=55482): Max retries exceeded with url: /session/c09b198de9cf0751cb997bc8dc24cabf/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F80A9B40>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC66FD30>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:18:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001FDEC6940D0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=56465): Max retries exceeded with url: /session/936ea9e03903ed08ade1ba8ec16186b7/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC6940D0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:19:39 KST)" (scheduled at 2025-01-15 16:18:39.190602+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F80A9DB0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:19:46 KST)" (scheduled at 2025-01-15 16:18:46.027289+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F80A9ED0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC66FF70>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F80A9FF0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC66FD60>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:19:39 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000002C9F80AA140>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=55482): Max retries exceeded with url: /session/c09b198de9cf0751cb997bc8dc24cabf/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F80AA140>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC66F550>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:19:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001FDEC66EFE0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=56465): Max retries exceeded with url: /session/936ea9e03903ed08ade1ba8ec16186b7/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC66EFE0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:20:39 KST)" (scheduled at 2025-01-15 16:19:39.190602+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F5B73310>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:20:46 KST)" (scheduled at 2025-01-15 16:19:46.027289+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F8065AB0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC694EB0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F80935B0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC66D720>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:20:39 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000002C9F8090970>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=55482): Max retries exceeded with url: /session/c09b198de9cf0751cb997bc8dc24cabf/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F8090970>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC66FE20>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:20:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001FDEC66E5F0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=56465): Max retries exceeded with url: /session/936ea9e03903ed08ade1ba8ec16186b7/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC66E5F0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:21:39 KST)" (scheduled at 2025-01-15 16:20:39.190602+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F80909A0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:21:46 KST)" (scheduled at 2025-01-15 16:20:46.027289+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F8090C40>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEA36C0A0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F8090D60>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEA36C070>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:21:39 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000002C9F80A9F00>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=55482): Max retries exceeded with url: /session/c09b198de9cf0751cb997bc8dc24cabf/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F80A9F00>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC694D00>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:21:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001FDEC694C70>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=56465): Max retries exceeded with url: /session/936ea9e03903ed08ade1ba8ec16186b7/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC694C70>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
INFO:werkzeug:[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
INFO:werkzeug:[33mPress CTRL+C to quit[0m
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 524-771-621
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:22:39 KST)" (scheduled at 2025-01-15 16:21:39.190602+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F80A9C00>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:22:46 KST)" (scheduled at 2025-01-15 16:21:46.027289+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F80A9AB0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEA36D6C0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F80A9930>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC60F6A0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:22:39 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000002C9F80A98D0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=55482): Max retries exceeded with url: /session/c09b198de9cf0751cb997bc8dc24cabf/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F80A98D0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC60E2F0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:22:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001FDEC60F3A0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=56465): Max retries exceeded with url: /session/936ea9e03903ed08ade1ba8ec16186b7/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC60F3A0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:22:15 KST)" (scheduled at 2025-01-15 16:22:15.152624+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:22:20 KST)" (scheduled at 2025-01-15 16:22:20.462567+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:23:39 KST)" (scheduled at 2025-01-15 16:22:39.190602+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F80A9480>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:23:46 KST)" (scheduled at 2025-01-15 16:22:46.027289+09:00)
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F80A8520>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:23:15 KST)" executed successfully
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC695690>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F80A9270>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:23:20 KST)" executed successfully
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC695780>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:23:39 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000002C9F80A9240>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=55482): Max retries exceeded with url: /session/c09b198de9cf0751cb997bc8dc24cabf/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F80A9240>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC695300>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:23:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001FDEC695090>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=56465): Max retries exceeded with url: /session/936ea9e03903ed08ade1ba8ec16186b7/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC695090>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:24:15 KST)" (scheduled at 2025-01-15 16:23:15.152624+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:24:15 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:24:20 KST)" (scheduled at 2025-01-15 16:23:20.462567+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D7BE20>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/426563f9d82aac489ee423afb6c3217d/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D7BF10>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/426563f9d82aac489ee423afb6c3217d/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D9C130>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/426563f9d82aac489ee423afb6c3217d/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:24:20 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001ED39D9C0A0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=56764): Max retries exceeded with url: /session/426563f9d82aac489ee423afb6c3217d/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D9C0A0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:24:39 KST)" (scheduled at 2025-01-15 16:23:39.190602+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F7ED35E0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:24:46 KST)" (scheduled at 2025-01-15 16:23:46.027289+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F5D84250>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC66FC10>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F5B73C40>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC66F100>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:24:39 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000002C9F5B73700>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=55482): Max retries exceeded with url: /session/c09b198de9cf0751cb997bc8dc24cabf/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F5B73700>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC66FF10>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:24:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001FDEC60F400>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=56465): Max retries exceeded with url: /session/936ea9e03903ed08ade1ba8ec16186b7/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC60F400>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:25:15 KST)" (scheduled at 2025-01-15 16:24:15.152624+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:25:15 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:25:20 KST)" (scheduled at 2025-01-15 16:24:20.462567+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D780A0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/426563f9d82aac489ee423afb6c3217d/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D7B6A0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/426563f9d82aac489ee423afb6c3217d/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D7A770>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/426563f9d82aac489ee423afb6c3217d/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:25:20 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001ED39D7B730>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=56764): Max retries exceeded with url: /session/426563f9d82aac489ee423afb6c3217d/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D7B730>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:25:39 KST)" (scheduled at 2025-01-15 16:24:39.190602+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F5D3FD30>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:25:46 KST)" (scheduled at 2025-01-15 16:24:46.027289+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F5D3FF10>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC694A00>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F5D3F730>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC694DC0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:25:39 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000002C9F545DD20>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=55482): Max retries exceeded with url: /session/c09b198de9cf0751cb997bc8dc24cabf/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F545DD20>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC694940>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:25:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001FDEC695090>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=56465): Max retries exceeded with url: /session/936ea9e03903ed08ade1ba8ec16186b7/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC695090>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:26:15 KST)" (scheduled at 2025-01-15 16:25:15.152624+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:26:15 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:26:20 KST)" (scheduled at 2025-01-15 16:25:20.462567+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D9CFD0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/426563f9d82aac489ee423afb6c3217d/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D9CE50>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/426563f9d82aac489ee423afb6c3217d/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D9CBE0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/426563f9d82aac489ee423afb6c3217d/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:26:20 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001ED39D9C880>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=56764): Max retries exceeded with url: /session/426563f9d82aac489ee423afb6c3217d/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D9C880>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:26:39 KST)" (scheduled at 2025-01-15 16:25:39.190602+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F8090910>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:26:46 KST)" (scheduled at 2025-01-15 16:25:46.027289+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F8090CD0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC66FE20>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F80918D0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC66FB50>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:26:39 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000002C9F8090B50>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=55482): Max retries exceeded with url: /session/c09b198de9cf0751cb997bc8dc24cabf/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F8090B50>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC66F370>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:26:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001FDEC66EF20>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=56465): Max retries exceeded with url: /session/936ea9e03903ed08ade1ba8ec16186b7/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC66EF20>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:27:15 KST)" (scheduled at 2025-01-15 16:26:15.152624+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:27:20 KST)" (scheduled at 2025-01-15 16:26:20.462567+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:27:15 KST)" executed successfully
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D7A8C0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/426563f9d82aac489ee423afb6c3217d/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D7B4F0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/426563f9d82aac489ee423afb6c3217d/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D7B2E0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/426563f9d82aac489ee423afb6c3217d/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:27:20 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001ED39D7BF10>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=56764): Max retries exceeded with url: /session/426563f9d82aac489ee423afb6c3217d/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D7BF10>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:27:39 KST)" (scheduled at 2025-01-15 16:26:39.190602+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F80906A0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:27:46 KST)" (scheduled at 2025-01-15 16:26:46.027289+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F8093430>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC6958A0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F8093340>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC695990>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:27:39 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000002C9F8092260>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=55482): Max retries exceeded with url: /session/c09b198de9cf0751cb997bc8dc24cabf/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F8092260>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC66E830>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:27:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001FDEC66EFB0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=56465): Max retries exceeded with url: /session/936ea9e03903ed08ade1ba8ec16186b7/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC66EFB0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:28:15 KST)" (scheduled at 2025-01-15 16:27:15.152624+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:28:15 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:28:20 KST)" (scheduled at 2025-01-15 16:27:20.462567+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D9D0C0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/426563f9d82aac489ee423afb6c3217d/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D9D1B0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/426563f9d82aac489ee423afb6c3217d/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED37A5D3C0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/426563f9d82aac489ee423afb6c3217d/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:28:20 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001ED37A5D960>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=56764): Max retries exceeded with url: /session/426563f9d82aac489ee423afb6c3217d/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED37A5D960>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:28:39 KST)" (scheduled at 2025-01-15 16:27:39.190602+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F8091D50>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:28:46 KST)" (scheduled at 2025-01-15 16:27:46.027289+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F8092F80>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC60F370>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F8091DE0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEA36C2B0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:28:39 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000002C9F8092CB0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=55482): Max retries exceeded with url: /session/c09b198de9cf0751cb997bc8dc24cabf/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F8092CB0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEA36D6F0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:28:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001FDEC6946A0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=56465): Max retries exceeded with url: /session/936ea9e03903ed08ade1ba8ec16186b7/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC6946A0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:29:15 KST)" (scheduled at 2025-01-15 16:28:15.152624+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:29:15 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:29:20 KST)" (scheduled at 2025-01-15 16:28:20.462567+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D1B310>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/426563f9d82aac489ee423afb6c3217d/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D1B280>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/426563f9d82aac489ee423afb6c3217d/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D1BEB0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/426563f9d82aac489ee423afb6c3217d/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:29:20 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001ED39D1B5B0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=56764): Max retries exceeded with url: /session/426563f9d82aac489ee423afb6c3217d/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D1B5B0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:29:39 KST)" (scheduled at 2025-01-15 16:28:39.190602+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F5D3FD00>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:29:46 KST)" (scheduled at 2025-01-15 16:28:46.027289+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F5D3FB50>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC6962C0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F5B73DC0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEA36D630>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:29:39 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000002C9F5B73D00>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=55482): Max retries exceeded with url: /session/c09b198de9cf0751cb997bc8dc24cabf/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F5B73D00>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEA36D6F0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:29:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001FDEC60E650>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=56465): Max retries exceeded with url: /session/936ea9e03903ed08ade1ba8ec16186b7/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC60E650>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:30:15 KST)" (scheduled at 2025-01-15 16:29:15.152624+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:30:15 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:30:20 KST)" (scheduled at 2025-01-15 16:29:20.462567+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D9DA20>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/426563f9d82aac489ee423afb6c3217d/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D1A1D0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/426563f9d82aac489ee423afb6c3217d/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D1B100>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/426563f9d82aac489ee423afb6c3217d/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:30:20 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001ED39D1B2B0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=56764): Max retries exceeded with url: /session/426563f9d82aac489ee423afb6c3217d/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D1B2B0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:30:39 KST)" (scheduled at 2025-01-15 16:29:39.190602+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F5B73760>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:30:46 KST)" (scheduled at 2025-01-15 16:29:46.027289+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F7ED2DA0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC66E830>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F7ED2080>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC696110>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:30:39 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000002C9F7ED2AA0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=55482): Max retries exceeded with url: /session/c09b198de9cf0751cb997bc8dc24cabf/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F7ED2AA0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC696620>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:30:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001FDEC695F30>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=56465): Max retries exceeded with url: /session/936ea9e03903ed08ade1ba8ec16186b7/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC695F30>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:31:15 KST)" (scheduled at 2025-01-15 16:30:15.152624+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:31:15 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:31:20 KST)" (scheduled at 2025-01-15 16:30:20.462567+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D7B610>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/426563f9d82aac489ee423afb6c3217d/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED37A5C970>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/426563f9d82aac489ee423afb6c3217d/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D9DAB0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/426563f9d82aac489ee423afb6c3217d/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:31:20 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001ED39D9D840>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=56764): Max retries exceeded with url: /session/426563f9d82aac489ee423afb6c3217d/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D9D840>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:31:39 KST)" (scheduled at 2025-01-15 16:30:39.190602+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F80909D0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:31:46 KST)" (scheduled at 2025-01-15 16:30:46.027289+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F8093100>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC696800>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F8093220>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC66EDD0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:31:39 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000002C9F8092B90>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=55482): Max retries exceeded with url: /session/c09b198de9cf0751cb997bc8dc24cabf/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F8092B90>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC66EEC0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:31:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001FDEC66F220>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=56465): Max retries exceeded with url: /session/936ea9e03903ed08ade1ba8ec16186b7/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC66F220>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:32:15 KST)" (scheduled at 2025-01-15 16:31:15.152624+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:32:15 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:32:20 KST)" (scheduled at 2025-01-15 16:31:20.462567+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D9DFC0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/426563f9d82aac489ee423afb6c3217d/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED37A5CA00>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/426563f9d82aac489ee423afb6c3217d/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D7B400>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/426563f9d82aac489ee423afb6c3217d/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:32:20 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001ED39D7B2E0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=56764): Max retries exceeded with url: /session/426563f9d82aac489ee423afb6c3217d/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D7B2E0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:32:39 KST)" (scheduled at 2025-01-15 16:31:39.190602+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F8090970>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:32:46 KST)" (scheduled at 2025-01-15 16:31:46.027289+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F8091AE0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC696710>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F8090E20>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC696A40>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:32:39 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000002C9F8090D90>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=55482): Max retries exceeded with url: /session/c09b198de9cf0751cb997bc8dc24cabf/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F8090D90>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC695CC0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:32:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001FDEC6946A0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=56465): Max retries exceeded with url: /session/936ea9e03903ed08ade1ba8ec16186b7/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC6946A0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:33:15 KST)" (scheduled at 2025-01-15 16:32:15.152624+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:33:15 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:33:20 KST)" (scheduled at 2025-01-15 16:32:20.462567+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D1B070>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/426563f9d82aac489ee423afb6c3217d/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D1B670>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/426563f9d82aac489ee423afb6c3217d/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D9E050>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/426563f9d82aac489ee423afb6c3217d/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:33:20 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001ED39D9D240>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=56764): Max retries exceeded with url: /session/426563f9d82aac489ee423afb6c3217d/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D9D240>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:33:39 KST)" (scheduled at 2025-01-15 16:32:39.190602+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F8092AD0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:33:46 KST)" (scheduled at 2025-01-15 16:32:46.027289+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F8092A70>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC696CB0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F80928C0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEA36CD00>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:33:39 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000002C9F8092740>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=55482): Max retries exceeded with url: /session/c09b198de9cf0751cb997bc8dc24cabf/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F8092740>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC60E3B0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:33:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001FDEC60DE40>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=56465): Max retries exceeded with url: /session/936ea9e03903ed08ade1ba8ec16186b7/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC60DE40>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:34:15 KST)" (scheduled at 2025-01-15 16:33:15.152624+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:34:15 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:34:20 KST)" (scheduled at 2025-01-15 16:33:20.462567+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D9E350>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/426563f9d82aac489ee423afb6c3217d/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D1B0A0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/426563f9d82aac489ee423afb6c3217d/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D1AA70>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/426563f9d82aac489ee423afb6c3217d/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:34:20 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001ED39D1B100>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=56764): Max retries exceeded with url: /session/426563f9d82aac489ee423afb6c3217d/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D1B100>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:34:39 KST)" (scheduled at 2025-01-15 16:33:39.190602+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F7ED2DA0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:34:46 KST)" (scheduled at 2025-01-15 16:33:46.027289+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F5D842B0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC6962F0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F5B73130>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC696320>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:34:39 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000002C9F5B731F0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=55482): Max retries exceeded with url: /session/c09b198de9cf0751cb997bc8dc24cabf/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F5B731F0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC695D20>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:34:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001FDEC6949D0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=56465): Max retries exceeded with url: /session/936ea9e03903ed08ade1ba8ec16186b7/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC6949D0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:35:15 KST)" (scheduled at 2025-01-15 16:34:15.152624+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:35:15 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:35:20 KST)" (scheduled at 2025-01-15 16:34:20.462567+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED37A5D8D0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/426563f9d82aac489ee423afb6c3217d/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D9E1A0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/426563f9d82aac489ee423afb6c3217d/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D9DA80>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/426563f9d82aac489ee423afb6c3217d/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:35:20 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001ED39D9DC60>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=56764): Max retries exceeded with url: /session/426563f9d82aac489ee423afb6c3217d/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D9DC60>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:35:39 KST)" (scheduled at 2025-01-15 16:34:39.190602+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F5D3FF10>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:35:46 KST)" (scheduled at 2025-01-15 16:34:46.027289+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F5D3FD90>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC697040>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F5D3FFA0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC66FD90>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:35:39 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000002C9F80925F0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=55482): Max retries exceeded with url: /session/c09b198de9cf0751cb997bc8dc24cabf/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F80925F0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC66F790>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:35:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001FDEC66F0D0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=56465): Max retries exceeded with url: /session/936ea9e03903ed08ade1ba8ec16186b7/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC66F0D0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:36:15 KST)" (scheduled at 2025-01-15 16:35:15.152624+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:36:15 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:36:20 KST)" (scheduled at 2025-01-15 16:35:20.462567+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D9E680>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/426563f9d82aac489ee423afb6c3217d/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED37A5D990>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/426563f9d82aac489ee423afb6c3217d/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D78040>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/426563f9d82aac489ee423afb6c3217d/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:36:20 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001ED39D7BE80>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=56764): Max retries exceeded with url: /session/426563f9d82aac489ee423afb6c3217d/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D7BE80>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:36:39 KST)" (scheduled at 2025-01-15 16:35:39.190602+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F8091990>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:36:46 KST)" (scheduled at 2025-01-15 16:35:46.027289+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F8091960>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC695990>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F8090C40>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC695AE0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:36:39 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000002C9F8090820>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=55482): Max retries exceeded with url: /session/c09b198de9cf0751cb997bc8dc24cabf/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F8090820>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC695DB0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:36:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001FDEC694C70>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=56465): Max retries exceeded with url: /session/936ea9e03903ed08ade1ba8ec16186b7/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC694C70>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:37:15 KST)" (scheduled at 2025-01-15 16:36:15.152624+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:37:15 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:37:20 KST)" (scheduled at 2025-01-15 16:36:20.462567+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D9E710>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/426563f9d82aac489ee423afb6c3217d/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D9E080>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/426563f9d82aac489ee423afb6c3217d/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D9DC00>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/426563f9d82aac489ee423afb6c3217d/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:37:20 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001ED39D9C250>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=56764): Max retries exceeded with url: /session/426563f9d82aac489ee423afb6c3217d/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D9C250>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:37:39 KST)" (scheduled at 2025-01-15 16:36:39.190602+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F8090AF0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:37:46 KST)" (scheduled at 2025-01-15 16:36:46.027289+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F80932E0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC697370>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F8093100>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEA36C9D0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:37:39 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000002C9F8093190>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=55482): Max retries exceeded with url: /session/c09b198de9cf0751cb997bc8dc24cabf/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F8093190>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC60F730>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:37:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001FDEC60E680>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=56465): Max retries exceeded with url: /session/936ea9e03903ed08ade1ba8ec16186b7/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC60E680>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:38:15 KST)" (scheduled at 2025-01-15 16:37:15.152624+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:38:15 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:38:20 KST)" (scheduled at 2025-01-15 16:37:20.462567+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D9E9B0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/426563f9d82aac489ee423afb6c3217d/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D1AFE0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/426563f9d82aac489ee423afb6c3217d/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D1B8B0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/426563f9d82aac489ee423afb6c3217d/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:38:20 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001ED39D1A170>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=56764): Max retries exceeded with url: /session/426563f9d82aac489ee423afb6c3217d/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D1A170>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:38:39 KST)" (scheduled at 2025-01-15 16:37:39.190602+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F8092560>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:38:46 KST)" (scheduled at 2025-01-15 16:37:46.027289+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F8092470>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC6962F0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F8091BD0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC695D50>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:38:39 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000002C9F80920B0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=55482): Max retries exceeded with url: /session/c09b198de9cf0751cb997bc8dc24cabf/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F80920B0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC6963E0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:38:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001FDEC696080>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=56465): Max retries exceeded with url: /session/936ea9e03903ed08ade1ba8ec16186b7/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC696080>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:39:15 KST)" (scheduled at 2025-01-15 16:38:15.152624+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:39:15 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:39:20 KST)" (scheduled at 2025-01-15 16:38:20.462567+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D9E410>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/426563f9d82aac489ee423afb6c3217d/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D9DA20>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/426563f9d82aac489ee423afb6c3217d/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D9E020>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/426563f9d82aac489ee423afb6c3217d/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:39:20 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001ED39D9D540>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=56764): Max retries exceeded with url: /session/426563f9d82aac489ee423afb6c3217d/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D9D540>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:39:39 KST)" (scheduled at 2025-01-15 16:38:39.190602+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F5D3FB80>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:39:46 KST)" (scheduled at 2025-01-15 16:38:46.027289+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F5D3FE80>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC6976A0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F5B73C40>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC60ED10>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:39:39 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000002C9F5B73D00>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=55482): Max retries exceeded with url: /session/c09b198de9cf0751cb997bc8dc24cabf/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F5B73D00>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC60E110>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:39:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001FDEC60F6D0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=56465): Max retries exceeded with url: /session/936ea9e03903ed08ade1ba8ec16186b7/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC60F6D0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:40:15 KST)" (scheduled at 2025-01-15 16:39:15.152624+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:40:15 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:40:20 KST)" (scheduled at 2025-01-15 16:39:20.462567+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D9ECE0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/426563f9d82aac489ee423afb6c3217d/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED37A5CAC0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/426563f9d82aac489ee423afb6c3217d/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D7B4F0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/426563f9d82aac489ee423afb6c3217d/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:40:20 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001ED39D7B610>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=56764): Max retries exceeded with url: /session/426563f9d82aac489ee423afb6c3217d/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D7B610>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:40:39 KST)" (scheduled at 2025-01-15 16:39:39.190602+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F5D842B0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:40:46 KST)" (scheduled at 2025-01-15 16:39:46.027289+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F7ED3550>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC66F4C0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F7ED2DA0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC66F2B0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:40:39 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000002C9F8092080>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=55482): Max retries exceeded with url: /session/c09b198de9cf0751cb997bc8dc24cabf/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F8092080>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC66D8D0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:40:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001FDEC66FB20>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=56465): Max retries exceeded with url: /session/936ea9e03903ed08ade1ba8ec16186b7/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC66FB20>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:41:15 KST)" (scheduled at 2025-01-15 16:40:15.152624+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:41:15 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:41:20 KST)" (scheduled at 2025-01-15 16:40:20.462567+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D9DBD0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/426563f9d82aac489ee423afb6c3217d/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D9D3F0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/426563f9d82aac489ee423afb6c3217d/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D9C2E0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/426563f9d82aac489ee423afb6c3217d/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:41:20 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001ED39D9C520>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=56764): Max retries exceeded with url: /session/426563f9d82aac489ee423afb6c3217d/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D9C520>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:41:39 KST)" (scheduled at 2025-01-15 16:40:39.190602+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F8092EF0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:41:46 KST)" (scheduled at 2025-01-15 16:40:46.027289+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F8093340>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC696260>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F80932E0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC66FCD0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:41:39 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000002C9F8090C10>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=55482): Max retries exceeded with url: /session/c09b198de9cf0751cb997bc8dc24cabf/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F8090C10>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC66E6B0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:41:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001FDEC66FDC0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=56465): Max retries exceeded with url: /session/936ea9e03903ed08ade1ba8ec16186b7/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC66FDC0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:42:15 KST)" (scheduled at 2025-01-15 16:41:15.152624+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:42:15 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:42:20 KST)" (scheduled at 2025-01-15 16:41:20.462567+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D9F070>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/426563f9d82aac489ee423afb6c3217d/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D1AF80>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/426563f9d82aac489ee423afb6c3217d/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D1B0A0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/426563f9d82aac489ee423afb6c3217d/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:42:20 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001ED39D1B160>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=56764): Max retries exceeded with url: /session/426563f9d82aac489ee423afb6c3217d/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D1B160>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:42:39 KST)" (scheduled at 2025-01-15 16:41:39.190602+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F80933D0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:42:46 KST)" (scheduled at 2025-01-15 16:41:46.027289+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F8091AE0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC60E3E0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F80906D0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC60F4F0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:42:39 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000002C9F80927D0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=55482): Max retries exceeded with url: /session/c09b198de9cf0751cb997bc8dc24cabf/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F80927D0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC60F730>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:42:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001FDEC60F490>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=56465): Max retries exceeded with url: /session/936ea9e03903ed08ade1ba8ec16186b7/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC60F490>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:43:15 KST)" (scheduled at 2025-01-15 16:42:15.152624+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:43:15 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:43:20 KST)" (scheduled at 2025-01-15 16:42:20.462567+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D9E380>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/426563f9d82aac489ee423afb6c3217d/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D9DB70>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/426563f9d82aac489ee423afb6c3217d/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D9C970>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/426563f9d82aac489ee423afb6c3217d/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:43:20 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001ED39D9CE80>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=56764): Max retries exceeded with url: /session/426563f9d82aac489ee423afb6c3217d/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D9CE80>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:43:39 KST)" (scheduled at 2025-01-15 16:42:39.190602+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F8091E40>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:43:46 KST)" (scheduled at 2025-01-15 16:42:46.027289+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F8090A90>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC696770>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F8090EB0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC6949D0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:43:39 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000002C9F80910F0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=55482): Max retries exceeded with url: /session/c09b198de9cf0751cb997bc8dc24cabf/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F80910F0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC60F490>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:43:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001FDEC60F3A0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=56465): Max retries exceeded with url: /session/936ea9e03903ed08ade1ba8ec16186b7/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC60F3A0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:44:15 KST)" (scheduled at 2025-01-15 16:43:15.152624+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:44:20 KST)" (scheduled at 2025-01-15 16:43:20.462567+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:44:15 KST)" executed successfully
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D9F340>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/426563f9d82aac489ee423afb6c3217d/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED37A5CE50>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/426563f9d82aac489ee423afb6c3217d/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D7B4C0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/426563f9d82aac489ee423afb6c3217d/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:44:20 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001ED39D7B400>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=56764): Max retries exceeded with url: /session/426563f9d82aac489ee423afb6c3217d/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D7B400>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:44:39 KST)" (scheduled at 2025-01-15 16:43:39.190602+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F7ED2BC0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:44:46 KST)" (scheduled at 2025-01-15 16:43:46.027289+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F5B73EB0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC66F640>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F5B73C10>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC66EEF0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:44:39 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000002C9F5B73250>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=55482): Max retries exceeded with url: /session/c09b198de9cf0751cb997bc8dc24cabf/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F5B73250>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC66FA30>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:44:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001FDEC696C20>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=56465): Max retries exceeded with url: /session/936ea9e03903ed08ade1ba8ec16186b7/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC696C20>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:45:15 KST)" (scheduled at 2025-01-15 16:44:15.152624+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:45:15 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:45:20 KST)" (scheduled at 2025-01-15 16:44:20.462567+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D9DC90>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/426563f9d82aac489ee423afb6c3217d/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D9DC00>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/426563f9d82aac489ee423afb6c3217d/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D9D480>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/426563f9d82aac489ee423afb6c3217d/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:45:20 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001ED39D9C610>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=56764): Max retries exceeded with url: /session/426563f9d82aac489ee423afb6c3217d/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D9C610>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:45:39 KST)" (scheduled at 2025-01-15 16:44:39.190602+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F5D3FF40>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:45:46 KST)" (scheduled at 2025-01-15 16:44:46.027289+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F5D3FE80>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC694670>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F5D3FDF0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC696530>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:45:39 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000002C9F8090940>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=55482): Max retries exceeded with url: /session/c09b198de9cf0751cb997bc8dc24cabf/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F8090940>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC66F2E0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/936ea9e03903ed08ade1ba8ec16186b7/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:45:46 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001FDEC66D6F0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=56465): Max retries exceeded with url: /session/936ea9e03903ed08ade1ba8ec16186b7/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FDEC66D6F0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:46:15 KST)" (scheduled at 2025-01-15 16:45:15.152624+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:46:15 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:46:20 KST)" (scheduled at 2025-01-15 16:45:20.462567+09:00)
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D9F670>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/426563f9d82aac489ee423afb6c3217d/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED37A5D870>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/426563f9d82aac489ee423afb6c3217d/url
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED37A5D360>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/426563f9d82aac489ee423afb6c3217d/url
INFO:werkzeug: * Detected change in 'C:\\Users\\soldesk\\AppData\\Roaming\\Python\\Python310\\site-packages\\webdriver_manager\\chrome.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\soldesk\\AppData\\Roaming\\Python\\Python310\\site-packages\\webdriver_manager\\core\\manager.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\soldesk\\AppData\\Roaming\\Python\\Python310\\site-packages\\webdriver_manager\\core\\manager.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\soldesk\\AppData\\Roaming\\Python\\Python310\\site-packages\\selenium\\webdriver\\chrome\\options.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\soldesk\\AppData\\Roaming\\Python\\Python310\\site-packages\\selenium\\webdriver\\chromium\\options.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\soldesk\\AppData\\Roaming\\Python\\Python310\\site-packages\\selenium\\webdriver\\chromium\\options.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\soldesk\\AppData\\Roaming\\Python\\Python310\\site-packages\\selenium\\webdriver\\common\\options.py', reloading
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\flask\\__init__.py', reloading
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\flask\\__init__.py', reloading
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\flask\\app.py', reloading
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\flask\\app.py', reloading
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\flask\\scaffold.py', reloading
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\flask\\scaffold.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\soldesk\\AppData\\Roaming\\Python\\Python310\\site-packages\\webdriver_manager\\core\\os_manager.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\soldesk\\AppData\\Roaming\\Python\\Python310\\site-packages\\webdriver_manager\\core\\download_manager.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\soldesk\\AppData\\Roaming\\Python\\Python310\\site-packages\\webdriver_manager\\core\\driver_cache.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\soldesk\\AppData\\Roaming\\Python\\Python310\\site-packages\\selenium\\webdriver\\chrome\\webdriver.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\soldesk\\AppData\\Roaming\\Python\\Python310\\site-packages\\selenium\\webdriver\\chrome\\webdriver.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\soldesk\\AppData\\Roaming\\Python\\Python310\\site-packages\\selenium\\webdriver\\chromium\\webdriver.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\soldesk\\AppData\\Roaming\\Python\\Python310\\site-packages\\selenium\\webdriver\\common\\timeouts.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\soldesk\\AppData\\Roaming\\Python\\Python310\\site-packages\\selenium\\webdriver\\common\\timeouts.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\soldesk\\AppData\\Roaming\\Python\\Python310\\site-packages\\selenium\\webdriver\\remote\\file_detector.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\soldesk\\AppData\\Roaming\\Python\\Python310\\site-packages\\selenium\\webdriver\\common\\virtual_authenticator.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\soldesk\\AppData\\Roaming\\Python\\Python310\\site-packages\\selenium\\webdriver\\chrome\\service.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\soldesk\\AppData\\Roaming\\Python\\Python310\\site-packages\\selenium\\webdriver\\chromium\\__init__.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\soldesk\\AppData\\Roaming\\Python\\Python310\\site-packages\\selenium\\webdriver\\chromium\\service.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\soldesk\\AppData\\Roaming\\Python\\Python310\\site-packages\\selenium\\webdriver\\common\\__init__.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\soldesk\\AppData\\Roaming\\Python\\Python310\\site-packages\\selenium\\webdriver\\common\\service.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\soldesk\\AppData\\Roaming\\Python\\Python310\\site-packages\\selenium\\webdriver\\common\\service.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\soldesk\\AppData\\Roaming\\Python\\Python310\\site-packages\\selenium\\types.py', reloading
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:46:20 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001ED39D78070>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=56764): Max retries exceeded with url: /session/426563f9d82aac489ee423afb6c3217d/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ED39D78070>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:46:39 KST)" (scheduled at 2025-01-15 16:45:39.190602+09:00)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F80925F0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:WDM:====== WebDriver manager ======
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 524-771-621
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\soldesk\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F8090BB0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
=======
INFO:werkzeug: * Debugger PIN: 569-178-434
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 18:59:28 KST)" (scheduled at 2025-01-14 18:59:28.975517+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 18:59:30 KST)" (scheduled at 2025-01-14 18:59:30.966502+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 18:59:34 KST)" (scheduled at 2025-01-14 18:59:34.380568+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 18:59:36 KST)" (scheduled at 2025-01-14 18:59:36.166765+09:00)
INFO:werkzeug: * Detected change in 'C:\\Users\\i\\AppData\\Roaming\\Python\\Python310\\site-packages\\openai\\types\\__init__.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\i\\AppData\\Roaming\\Python\\Python310\\site-packages\\openai\\types\\__init__.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\i\\AppData\\Roaming\\Python\\Python310\\site-packages\\openai\\types\\batch.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\i\\AppData\\Roaming\\Python\\Python310\\site-packages\\openai\\types\\batch.py', reloading
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 18:59:50 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 288, in crawl_and_insert_data
    from openai import OpenAI # 1.x~
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\openai\__init__.py", line 8, in <module>
    from . import types
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\openai\types\__init__.py", line 5, in <module>
    from .batch import Batch as Batch
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\openai\types\batch.py", line 7, in <module>
    from .._models import BaseModel
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\openai\_models.py", line 26, in <module>
    from ._types import (
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\openai\_types.py", line 21, in <module>
    import httpx
ModuleNotFoundError: No module named 'httpx'
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 18:59:48 KST)" skipped: maximum number of running instances reached (1)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:00:10 KST)" (scheduled at 2025-01-14 18:59:50.966502+09:00)
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 18:59:54 KST)" skipped: maximum number of running instances reached (1)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:00:10 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 288, in crawl_and_insert_data
    from openai import OpenAI # 1.x~
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\openai\__init__.py", line 8, in <module>
    from . import types
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\openai\types\__init__.py", line 5, in <module>
    from .batch import Batch as Batch
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\openai\types\batch.py", line 7, in <module>
    from .._models import BaseModel
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\openai\_models.py", line 26, in <module>
    from ._types import (
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\openai\_types.py", line 21, in <module>
    import httpx
ModuleNotFoundError: No module named 'httpx'
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 18:59:56 KST)" skipped: maximum number of running instances reached (1)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:00:08 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 288, in crawl_and_insert_data
    from openai import OpenAI # 1.x~
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\openai\__init__.py", line 8, in <module>
    from . import types
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\openai\types\__init__.py", line 5, in <module>
    from .batch import Batch as Batch
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\openai\types\batch.py", line 7, in <module>
    from .._models import BaseModel
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\openai\_models.py", line 26, in <module>
    from ._types import (
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\openai\_types.py", line 21, in <module>
    import httpx
ModuleNotFoundError: No module named 'httpx'
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:00:28 KST)" (scheduled at 2025-01-14 19:00:08.975517+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:00:30 KST)" (scheduled at 2025-01-14 19:00:10.966502+09:00)
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:00:14 KST)" skipped: maximum number of running instances reached (1)
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:00:16 KST)" skipped: maximum number of running instances reached (1)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:00:36 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 129, in crawl_and_insert_data
    url = driver.current_url
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 437, in current_url
    return self.execute(Command.GET_CURRENT_URL)["value"]
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 347, in execute
    self.error_handler.check_response(response)
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: disconnected: not connected to DevTools
  (failed to check if window was closed: disconnected: not connected to DevTools)
  (Session info: chrome=131.0.6778.265)
Stacktrace:
	GetHandleVerifier [0x007BFD53+23747]
	(No symbol) [0x00747D54]
	(No symbol) [0x0061BE53]
	(No symbol) [0x0060C408]
	(No symbol) [0x0060C329]
	(No symbol) [0x0061E2E0]
	(No symbol) [0x0069AD69]
	(No symbol) [0x00681B96]
	(No symbol) [0x00653F3C]
	(No symbol) [0x00654EBD]
	GetHandleVerifier [0x00A9AC73+3017699]
	GetHandleVerifier [0x00AAB93B+3086507]
	GetHandleVerifier [0x00AA40F2+3055714]
	GetHandleVerifier [0x00855AF0+637536]
	(No symbol) [0x00750A5D]
	(No symbol) [0x0074DA28]
	(No symbol) [0x0074DBC5]
	(No symbol) [0x007407F0]
	BaseThreadInitThunk [0x754CFCC9+25]
	RtlGetAppContainerNamedObjectPath [0x76F4809E+286]
	RtlGetAppContainerNamedObjectPath [0x76F4806E+238]

ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:00:34 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 129, in crawl_and_insert_data
    url = driver.current_url
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 437, in current_url
    return self.execute(Command.GET_CURRENT_URL)["value"]
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 347, in execute
    self.error_handler.check_response(response)
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: disconnected: not connected to DevTools
  (failed to check if window was closed: disconnected: not connected to DevTools)
  (Session info: chrome=131.0.6778.265)
Stacktrace:
	GetHandleVerifier [0x007BFD53+23747]
	(No symbol) [0x00747D54]
	(No symbol) [0x0061BE53]
	(No symbol) [0x0060C408]
	(No symbol) [0x0060C329]
	(No symbol) [0x0061E2E0]
	(No symbol) [0x0069AD69]
	(No symbol) [0x00681B96]
	(No symbol) [0x00653F3C]
	(No symbol) [0x00654EBD]
	GetHandleVerifier [0x00A9AC73+3017699]
	GetHandleVerifier [0x00AAB93B+3086507]
	GetHandleVerifier [0x00AA40F2+3055714]
	GetHandleVerifier [0x00855AF0+637536]
	(No symbol) [0x00750A5D]
	(No symbol) [0x0074DA28]
	(No symbol) [0x0074DBC5]
	(No symbol) [0x007407F0]
	BaseThreadInitThunk [0x754CFCC9+25]
	RtlGetAppContainerNamedObjectPath [0x76F4809E+286]
	RtlGetAppContainerNamedObjectPath [0x76F4806E+238]

INFO:werkzeug: * Restarting with watchdog (windowsapi)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:00:30 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 129, in crawl_and_insert_data
    url = driver.current_url
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 437, in current_url
    return self.execute(Command.GET_CURRENT_URL)["value"]
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 347, in execute
    self.error_handler.check_response(response)
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: disconnected: not connected to DevTools
  (failed to check if window was closed: disconnected: not connected to DevTools)
  (Session info: chrome=131.0.6778.265)
Stacktrace:
	GetHandleVerifier [0x007BFD53+23747]
	(No symbol) [0x00747D54]
	(No symbol) [0x0061BE53]
	(No symbol) [0x0060C408]
	(No symbol) [0x0060C329]
	(No symbol) [0x0061E2E0]
	(No symbol) [0x0069AD69]
	(No symbol) [0x00681B96]
	(No symbol) [0x00653F3C]
	(No symbol) [0x00654EBD]
	GetHandleVerifier [0x00A9AC73+3017699]
	GetHandleVerifier [0x00AAB93B+3086507]
	GetHandleVerifier [0x00AA40F2+3055714]
	GetHandleVerifier [0x00855AF0+637536]
	(No symbol) [0x00750A5D]
	(No symbol) [0x0074DA28]
	(No symbol) [0x0074DBC5]
	(No symbol) [0x007407F0]
	BaseThreadInitThunk [0x754CFCC9+25]
	RtlGetAppContainerNamedObjectPath [0x76F4809E+286]
	RtlGetAppContainerNamedObjectPath [0x76F4806E+238]

ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:00:28 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 129, in crawl_and_insert_data
    url = driver.current_url
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 437, in current_url
    return self.execute(Command.GET_CURRENT_URL)["value"]
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 347, in execute
    self.error_handler.check_response(response)
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: disconnected: not connected to DevTools
  (failed to check if window was closed: disconnected: not connected to DevTools)
  (Session info: chrome=131.0.6778.265)
Stacktrace:
	GetHandleVerifier [0x007BFD53+23747]
	(No symbol) [0x00747D54]
	(No symbol) [0x0061BE53]
	(No symbol) [0x0060C408]
	(No symbol) [0x0060C329]
	(No symbol) [0x0061E2E0]
	(No symbol) [0x0069AD69]
	(No symbol) [0x00681B96]
	(No symbol) [0x00653F3C]
	(No symbol) [0x00654EBD]
	GetHandleVerifier [0x00A9AC73+3017699]
	GetHandleVerifier [0x00AAB93B+3086507]
	GetHandleVerifier [0x00AA40F2+3055714]
	GetHandleVerifier [0x00855AF0+637536]
	(No symbol) [0x00750A5D]
	(No symbol) [0x0074DA28]
	(No symbol) [0x0074DBC5]
	(No symbol) [0x007407F0]
	BaseThreadInitThunk [0x754CFCC9+25]
	RtlGetAppContainerNamedObjectPath [0x76F4809E+286]
	RtlGetAppContainerNamedObjectPath [0x76F4806E+238]

INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:00:48 KST)" (scheduled at 2025-01-14 19:00:28.975517+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:00:48 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 108, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 38, in load
    driver.get(url) # url 접속
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 347, in execute
    self.error_handler.check_response(response)
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: disconnected: not connected to DevTools
  (failed to check if window was closed: disconnected: not connected to DevTools)
  (Session info: chrome=131.0.6778.265)
Stacktrace:
	GetHandleVerifier [0x007BFD53+23747]
	(No symbol) [0x00747D54]
	(No symbol) [0x0061BE53]
	(No symbol) [0x0060C408]
	(No symbol) [0x0060C329]
	(No symbol) [0x0061E2E0]
	(No symbol) [0x0069AD69]
	(No symbol) [0x00681B96]
	(No symbol) [0x00653F3C]
	(No symbol) [0x00654EBD]
	GetHandleVerifier [0x00A9AC73+3017699]
	GetHandleVerifier [0x00AAB93B+3086507]
	GetHandleVerifier [0x00AA40F2+3055714]
	GetHandleVerifier [0x00855AF0+637536]
	(No symbol) [0x00750A5D]
	(No symbol) [0x0074DA28]
	(No symbol) [0x0074DBC5]
	(No symbol) [0x007407F0]
	BaseThreadInitThunk [0x754CFCC9+25]
	RtlGetAppContainerNamedObjectPath [0x76F4809E+286]
	RtlGetAppContainerNamedObjectPath [0x76F4806E+238]

INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:00:50 KST)" (scheduled at 2025-01-14 19:00:30.966502+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:00:50 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 108, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 38, in load
    driver.get(url) # url 접속
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 347, in execute
    self.error_handler.check_response(response)
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: disconnected: not connected to DevTools
  (failed to check if window was closed: disconnected: not connected to DevTools)
  (Session info: chrome=131.0.6778.265)
Stacktrace:
	GetHandleVerifier [0x007BFD53+23747]
	(No symbol) [0x00747D54]
	(No symbol) [0x0061BE53]
	(No symbol) [0x0060C408]
	(No symbol) [0x0060C329]
	(No symbol) [0x0061E2E0]
	(No symbol) [0x0069AD69]
	(No symbol) [0x00681B96]
	(No symbol) [0x00653F3C]
	(No symbol) [0x00654EBD]
	GetHandleVerifier [0x00A9AC73+3017699]
	GetHandleVerifier [0x00AAB93B+3086507]
	GetHandleVerifier [0x00AA40F2+3055714]
	GetHandleVerifier [0x00855AF0+637536]
	(No symbol) [0x00750A5D]
	(No symbol) [0x0074DA28]
	(No symbol) [0x0074DBC5]
	(No symbol) [0x007407F0]
	BaseThreadInitThunk [0x754CFCC9+25]
	RtlGetAppContainerNamedObjectPath [0x76F4809E+286]
	RtlGetAppContainerNamedObjectPath [0x76F4806E+238]

INFO:WDM:====== WebDriver manager ======
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\i\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\i\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 569-178-434
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 569-178-434
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:01:08 KST)" (scheduled at 2025-01-14 19:00:48.975517+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:01:08 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 108, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 38, in load
    driver.get(url) # url 접속
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 347, in execute
    self.error_handler.check_response(response)
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: disconnected: not connected to DevTools
  (failed to check if window was closed: disconnected: not connected to DevTools)
  (Session info: chrome=131.0.6778.265)
Stacktrace:
	GetHandleVerifier [0x007BFD53+23747]
	(No symbol) [0x00747D54]
	(No symbol) [0x0061BE53]
	(No symbol) [0x0060C408]
	(No symbol) [0x0060C329]
	(No symbol) [0x0061E2E0]
	(No symbol) [0x0069AD69]
	(No symbol) [0x00681B96]
	(No symbol) [0x00653F3C]
	(No symbol) [0x00654EBD]
	GetHandleVerifier [0x00A9AC73+3017699]
	GetHandleVerifier [0x00AAB93B+3086507]
	GetHandleVerifier [0x00AA40F2+3055714]
	GetHandleVerifier [0x00855AF0+637536]
	(No symbol) [0x00750A5D]
	(No symbol) [0x0074DA28]
	(No symbol) [0x0074DBC5]
	(No symbol) [0x007407F0]
	BaseThreadInitThunk [0x754CFCC9+25]
	RtlGetAppContainerNamedObjectPath [0x76F4809E+286]
	RtlGetAppContainerNamedObjectPath [0x76F4806E+238]

INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:01:10 KST)" (scheduled at 2025-01-14 19:00:50.966502+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:01:10 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 108, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 38, in load
    driver.get(url) # url 접속
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 347, in execute
    self.error_handler.check_response(response)
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: disconnected: not connected to DevTools
  (failed to check if window was closed: disconnected: not connected to DevTools)
  (Session info: chrome=131.0.6778.265)
Stacktrace:
	GetHandleVerifier [0x007BFD53+23747]
	(No symbol) [0x00747D54]
	(No symbol) [0x0061BE53]
	(No symbol) [0x0060C408]
	(No symbol) [0x0060C329]
	(No symbol) [0x0061E2E0]
	(No symbol) [0x0069AD69]
	(No symbol) [0x00681B96]
	(No symbol) [0x00653F3C]
	(No symbol) [0x00654EBD]
	GetHandleVerifier [0x00A9AC73+3017699]
	GetHandleVerifier [0x00AAB93B+3086507]
	GetHandleVerifier [0x00AA40F2+3055714]
	GetHandleVerifier [0x00855AF0+637536]
	(No symbol) [0x00750A5D]
	(No symbol) [0x0074DA28]
	(No symbol) [0x0074DBC5]
	(No symbol) [0x007407F0]
	BaseThreadInitThunk [0x754CFCC9+25]
	RtlGetAppContainerNamedObjectPath [0x76F4809E+286]
	RtlGetAppContainerNamedObjectPath [0x76F4806E+238]

INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:00:57 KST)" (scheduled at 2025-01-14 19:00:57.912732+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:00:57 KST)" (scheduled at 2025-01-14 19:00:57.979729+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:01:17 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 129, in crawl_and_insert_data
    url = driver.current_url
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 437, in current_url
    return self.execute(Command.GET_CURRENT_URL)["value"]
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 347, in execute
    self.error_handler.check_response(response)
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchWindowException: Message: no such window: target window already closed
from unknown error: web view not found
  (Session info: chrome=131.0.6778.265)
Stacktrace:
	GetHandleVerifier [0x007BFD53+23747]
	(No symbol) [0x00747D54]
	(No symbol) [0x0061BE53]
	(No symbol) [0x005FD91B]
	(No symbol) [0x00687EFF]
	(No symbol) [0x0069AD49]
	(No symbol) [0x00681B96]
	(No symbol) [0x00653F3C]
	(No symbol) [0x00654EBD]
	GetHandleVerifier [0x00A9AC73+3017699]
	GetHandleVerifier [0x00AAB93B+3086507]
	GetHandleVerifier [0x00AA40F2+3055714]
	GetHandleVerifier [0x00855AF0+637536]
	(No symbol) [0x00750A5D]
	(No symbol) [0x0074DA28]
	(No symbol) [0x0074DBC5]
	(No symbol) [0x007407F0]
	BaseThreadInitThunk [0x754CFCC9+25]
	RtlGetAppContainerNamedObjectPath [0x76F4809E+286]
	RtlGetAppContainerNamedObjectPath [0x76F4806E+238]

ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:01:17 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 288, in crawl_and_insert_data
    from openai import OpenAI # 1.x~
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\openai\__init__.py", line 8, in <module>
    from . import types
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\openai\types\__init__.py", line 5, in <module>
    from .batch import Batch as Batch
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\openai\types\batch.py", line 7, in <module>
    from .._models import BaseModel
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\openai\_models.py", line 26, in <module>
    from ._types import (
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\openai\_types.py", line 21, in <module>
    import httpx
ModuleNotFoundError: No module named 'httpx'
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:01:28 KST)" (scheduled at 2025-01-14 19:01:08.975517+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:01:28 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 108, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 38, in load
    driver.get(url) # url 접속
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 347, in execute
    self.error_handler.check_response(response)
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: disconnected: not connected to DevTools
  (failed to check if window was closed: disconnected: not connected to DevTools)
  (Session info: chrome=131.0.6778.265)
Stacktrace:
	GetHandleVerifier [0x007BFD53+23747]
	(No symbol) [0x00747D54]
	(No symbol) [0x0061BE53]
	(No symbol) [0x0060C408]
	(No symbol) [0x0060C329]
	(No symbol) [0x0061E2E0]
	(No symbol) [0x0069AD69]
	(No symbol) [0x00681B96]
	(No symbol) [0x00653F3C]
	(No symbol) [0x00654EBD]
	GetHandleVerifier [0x00A9AC73+3017699]
	GetHandleVerifier [0x00AAB93B+3086507]
	GetHandleVerifier [0x00AA40F2+3055714]
	GetHandleVerifier [0x00855AF0+637536]
	(No symbol) [0x00750A5D]
	(No symbol) [0x0074DA28]
	(No symbol) [0x0074DBC5]
	(No symbol) [0x007407F0]
	BaseThreadInitThunk [0x754CFCC9+25]
	RtlGetAppContainerNamedObjectPath [0x76F4809E+286]
	RtlGetAppContainerNamedObjectPath [0x76F4806E+238]

INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:01:30 KST)" (scheduled at 2025-01-14 19:01:10.966502+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:01:30 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 108, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 38, in load
    driver.get(url) # url 접속
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 347, in execute
    self.error_handler.check_response(response)
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: disconnected: not connected to DevTools
  (failed to check if window was closed: disconnected: not connected to DevTools)
  (Session info: chrome=131.0.6778.265)
Stacktrace:
	GetHandleVerifier [0x007BFD53+23747]
	(No symbol) [0x00747D54]
	(No symbol) [0x0061BE53]
	(No symbol) [0x0060C408]
	(No symbol) [0x0060C329]
	(No symbol) [0x0061E2E0]
	(No symbol) [0x0069AD69]
	(No symbol) [0x00681B96]
	(No symbol) [0x00653F3C]
	(No symbol) [0x00654EBD]
	GetHandleVerifier [0x00A9AC73+3017699]
	GetHandleVerifier [0x00AAB93B+3086507]
	GetHandleVerifier [0x00AA40F2+3055714]
	GetHandleVerifier [0x00855AF0+637536]
	(No symbol) [0x00750A5D]
	(No symbol) [0x0074DA28]
	(No symbol) [0x0074DBC5]
	(No symbol) [0x007407F0]
	BaseThreadInitThunk [0x754CFCC9+25]
	RtlGetAppContainerNamedObjectPath [0x76F4809E+286]
	RtlGetAppContainerNamedObjectPath [0x76F4806E+238]

INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:01:37 KST)" (scheduled at 2025-01-14 19:01:17.912732+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:01:37 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 108, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 38, in load
    driver.get(url) # url 접속
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 347, in execute
    self.error_handler.check_response(response)
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchWindowException: Message: no such window: target window already closed
from unknown error: web view not found
  (Session info: chrome=131.0.6778.265)
Stacktrace:
	GetHandleVerifier [0x007BFD53+23747]
	(No symbol) [0x00747D54]
	(No symbol) [0x0061BE53]
	(No symbol) [0x005FD91B]
	(No symbol) [0x00687EFF]
	(No symbol) [0x0069AD49]
	(No symbol) [0x00681B96]
	(No symbol) [0x00653F3C]
	(No symbol) [0x00654EBD]
	GetHandleVerifier [0x00A9AC73+3017699]
	GetHandleVerifier [0x00AAB93B+3086507]
	GetHandleVerifier [0x00AA40F2+3055714]
	GetHandleVerifier [0x00855AF0+637536]
	(No symbol) [0x00750A5D]
	(No symbol) [0x0074DA28]
	(No symbol) [0x0074DBC5]
	(No symbol) [0x007407F0]
	BaseThreadInitThunk [0x754CFCC9+25]
	RtlGetAppContainerNamedObjectPath [0x76F4809E+286]
	RtlGetAppContainerNamedObjectPath [0x76F4806E+238]

INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:01:37 KST)" (scheduled at 2025-01-14 19:01:17.979729+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:01:37 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 108, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 38, in load
    driver.get(url) # url 접속
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 347, in execute
    self.error_handler.check_response(response)
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchWindowException: Message: no such window: target window already closed
from unknown error: web view not found
  (Session info: chrome=131.0.6778.265)
Stacktrace:
	GetHandleVerifier [0x007BFD53+23747]
	(No symbol) [0x00747D54]
	(No symbol) [0x0061BE53]
	(No symbol) [0x005FD91B]
	(No symbol) [0x00687EFF]
	(No symbol) [0x0069AD49]
	(No symbol) [0x00681B96]
	(No symbol) [0x00653F3C]
	(No symbol) [0x00654EBD]
	GetHandleVerifier [0x00A9AC73+3017699]
	GetHandleVerifier [0x00AAB93B+3086507]
	GetHandleVerifier [0x00AA40F2+3055714]
	GetHandleVerifier [0x00855AF0+637536]
	(No symbol) [0x00750A5D]
	(No symbol) [0x0074DA28]
	(No symbol) [0x0074DBC5]
	(No symbol) [0x007407F0]
	BaseThreadInitThunk [0x754CFCC9+25]
	RtlGetAppContainerNamedObjectPath [0x76F4809E+286]
	RtlGetAppContainerNamedObjectPath [0x76F4806E+238]

INFO:werkzeug: * Detected change in 'C:\\kd\\ws_java\\team4_v2sbm3c\\src\\main\\python\\exc.py', reloading
INFO:werkzeug: * Detected change in 'C:\\kd\\ws_java\\team4_v2sbm3c\\src\\main\\python\\exc.py', reloading
INFO:werkzeug: * Detected change in 'C:\\kd\\ws_java\\team4_v2sbm3c\\src\\main\\python\\exc.py', reloading
INFO:werkzeug: * Detected change in 'C:\\kd\\ws_java\\team4_v2sbm3c\\src\\main\\python\\exc.py', reloading
INFO:werkzeug: * Detected change in 'C:\\kd\\ws_java\\team4_v2sbm3c\\src\\main\\python\\exc.py', reloading
INFO:werkzeug: * Detected change in 'C:\\kd\\ws_java\\team4_v2sbm3c\\src\\main\\python\\exc.py', reloading
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\i\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:WDM:Driver [C:\Users\i\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:01:48 KST)" (scheduled at 2025-01-14 19:01:28.975517+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:01:48 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 108, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 38, in load
    driver.get(url) # url 접속
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 347, in execute
    self.error_handler.check_response(response)
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: disconnected: not connected to DevTools
  (failed to check if window was closed: disconnected: not connected to DevTools)
  (Session info: chrome=131.0.6778.265)
Stacktrace:
	GetHandleVerifier [0x007BFD53+23747]
	(No symbol) [0x00747D54]
	(No symbol) [0x0061BE53]
	(No symbol) [0x0060C408]
	(No symbol) [0x0060C329]
	(No symbol) [0x0061E2E0]
	(No symbol) [0x0069AD69]
	(No symbol) [0x00681B96]
	(No symbol) [0x00653F3C]
	(No symbol) [0x00654EBD]
	GetHandleVerifier [0x00A9AC73+3017699]
	GetHandleVerifier [0x00AAB93B+3086507]
	GetHandleVerifier [0x00AA40F2+3055714]
	GetHandleVerifier [0x00855AF0+637536]
	(No symbol) [0x00750A5D]
	(No symbol) [0x0074DA28]
	(No symbol) [0x0074DBC5]
	(No symbol) [0x007407F0]
	BaseThreadInitThunk [0x754CFCC9+25]
	RtlGetAppContainerNamedObjectPath [0x76F4809E+286]
	RtlGetAppContainerNamedObjectPath [0x76F4806E+238]

INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 569-178-434
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 569-178-434
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:01:50 KST)" (scheduled at 2025-01-14 19:01:30.966502+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:01:50 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 108, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 38, in load
    driver.get(url) # url 접속
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 347, in execute
    self.error_handler.check_response(response)
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: disconnected: not connected to DevTools
  (failed to check if window was closed: disconnected: not connected to DevTools)
  (Session info: chrome=131.0.6778.265)
Stacktrace:
	GetHandleVerifier [0x007BFD53+23747]
	(No symbol) [0x00747D54]
	(No symbol) [0x0061BE53]
	(No symbol) [0x0060C408]
	(No symbol) [0x0060C329]
	(No symbol) [0x0061E2E0]
	(No symbol) [0x0069AD69]
	(No symbol) [0x00681B96]
	(No symbol) [0x00653F3C]
	(No symbol) [0x00654EBD]
	GetHandleVerifier [0x00A9AC73+3017699]
	GetHandleVerifier [0x00AAB93B+3086507]
	GetHandleVerifier [0x00AA40F2+3055714]
	GetHandleVerifier [0x00855AF0+637536]
	(No symbol) [0x00750A5D]
	(No symbol) [0x0074DA28]
	(No symbol) [0x0074DBC5]
	(No symbol) [0x007407F0]
	BaseThreadInitThunk [0x754CFCC9+25]
	RtlGetAppContainerNamedObjectPath [0x76F4809E+286]
	RtlGetAppContainerNamedObjectPath [0x76F4806E+238]

INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:02:08 KST)" (scheduled at 2025-01-14 19:01:48.975517+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:02:08 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 108, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 38, in load
    driver.get(url) # url 접속
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 347, in execute
    self.error_handler.check_response(response)
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: disconnected: not connected to DevTools
  (failed to check if window was closed: disconnected: not connected to DevTools)
  (Session info: chrome=131.0.6778.265)
Stacktrace:
	GetHandleVerifier [0x007BFD53+23747]
	(No symbol) [0x00747D54]
	(No symbol) [0x0061BE53]
	(No symbol) [0x0060C408]
	(No symbol) [0x0060C329]
	(No symbol) [0x0061E2E0]
	(No symbol) [0x0069AD69]
	(No symbol) [0x00681B96]
	(No symbol) [0x00653F3C]
	(No symbol) [0x00654EBD]
	GetHandleVerifier [0x00A9AC73+3017699]
	GetHandleVerifier [0x00AAB93B+3086507]
	GetHandleVerifier [0x00AA40F2+3055714]
	GetHandleVerifier [0x00855AF0+637536]
	(No symbol) [0x00750A5D]
	(No symbol) [0x0074DA28]
	(No symbol) [0x0074DBC5]
	(No symbol) [0x007407F0]
	BaseThreadInitThunk [0x754CFCC9+25]
	RtlGetAppContainerNamedObjectPath [0x76F4809E+286]
	RtlGetAppContainerNamedObjectPath [0x76F4806E+238]

INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:01:49 KST)" (scheduled at 2025-01-14 19:01:49.436942+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:02:09 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 108, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 38, in load
    driver.get(url) # url 접속
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 347, in execute
    self.error_handler.check_response(response)
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchWindowException: Message: no such window: target window already closed
from unknown error: web view not found
  (Session info: chrome=131.0.6778.265)
Stacktrace:
	GetHandleVerifier [0x007BFD53+23747]
	(No symbol) [0x00747D54]
	(No symbol) [0x0061BE53]
	(No symbol) [0x005FD91B]
	(No symbol) [0x00687EFF]
	(No symbol) [0x0069AD49]
	(No symbol) [0x00681B96]
	(No symbol) [0x00653F3C]
	(No symbol) [0x00654EBD]
	GetHandleVerifier [0x00A9AC73+3017699]
	GetHandleVerifier [0x00AAB93B+3086507]
	GetHandleVerifier [0x00AA40F2+3055714]
	GetHandleVerifier [0x00855AF0+637536]
	(No symbol) [0x00750A5D]
	(No symbol) [0x0074DA28]
	(No symbol) [0x0074DBC5]
	(No symbol) [0x007407F0]
	BaseThreadInitThunk [0x754CFCC9+25]
	RtlGetAppContainerNamedObjectPath [0x76F4809E+286]
	RtlGetAppContainerNamedObjectPath [0x76F4806E+238]

INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:01:49 KST)" (scheduled at 2025-01-14 19:01:49.529942+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:02:09 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 108, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 38, in load
    driver.get(url) # url 접속
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 347, in execute
    self.error_handler.check_response(response)
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchWindowException: Message: no such window: target window already closed
from unknown error: web view not found
  (Session info: chrome=131.0.6778.265)
Stacktrace:
	GetHandleVerifier [0x007BFD53+23747]
	(No symbol) [0x00747D54]
	(No symbol) [0x0061BE53]
	(No symbol) [0x005FD91B]
	(No symbol) [0x00687EFF]
	(No symbol) [0x0069AD49]
	(No symbol) [0x00681B96]
	(No symbol) [0x00653F3C]
	(No symbol) [0x00654EBD]
	GetHandleVerifier [0x00A9AC73+3017699]
	GetHandleVerifier [0x00AAB93B+3086507]
	GetHandleVerifier [0x00AA40F2+3055714]
	GetHandleVerifier [0x00855AF0+637536]
	(No symbol) [0x00750A5D]
	(No symbol) [0x0074DA28]
	(No symbol) [0x0074DBC5]
	(No symbol) [0x007407F0]
	BaseThreadInitThunk [0x754CFCC9+25]
	RtlGetAppContainerNamedObjectPath [0x76F4809E+286]
	RtlGetAppContainerNamedObjectPath [0x76F4806E+238]

INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:02:10 KST)" (scheduled at 2025-01-14 19:01:50.966502+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:02:10 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 108, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 38, in load
    driver.get(url) # url 접속
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 347, in execute
    self.error_handler.check_response(response)
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: disconnected: not connected to DevTools
  (failed to check if window was closed: disconnected: not connected to DevTools)
  (Session info: chrome=131.0.6778.265)
Stacktrace:
	GetHandleVerifier [0x007BFD53+23747]
	(No symbol) [0x00747D54]
	(No symbol) [0x0061BE53]
	(No symbol) [0x0060C408]
	(No symbol) [0x0060C329]
	(No symbol) [0x0061E2E0]
	(No symbol) [0x0069AD69]
	(No symbol) [0x00681B96]
	(No symbol) [0x00653F3C]
	(No symbol) [0x00654EBD]
	GetHandleVerifier [0x00A9AC73+3017699]
	GetHandleVerifier [0x00AAB93B+3086507]
	GetHandleVerifier [0x00AA40F2+3055714]
	GetHandleVerifier [0x00855AF0+637536]
	(No symbol) [0x00750A5D]
	(No symbol) [0x0074DA28]
	(No symbol) [0x0074DBC5]
	(No symbol) [0x007407F0]
	BaseThreadInitThunk [0x754CFCC9+25]
	RtlGetAppContainerNamedObjectPath [0x76F4809E+286]
	RtlGetAppContainerNamedObjectPath [0x76F4806E+238]

INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:02:28 KST)" (scheduled at 2025-01-14 19:02:08.975517+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:02:28 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 108, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 38, in load
    driver.get(url) # url 접속
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 347, in execute
    self.error_handler.check_response(response)
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: disconnected: not connected to DevTools
  (failed to check if window was closed: disconnected: not connected to DevTools)
  (Session info: chrome=131.0.6778.265)
Stacktrace:
	GetHandleVerifier [0x007BFD53+23747]
	(No symbol) [0x00747D54]
	(No symbol) [0x0061BE53]
	(No symbol) [0x0060C408]
	(No symbol) [0x0060C329]
	(No symbol) [0x0061E2E0]
	(No symbol) [0x0069AD69]
	(No symbol) [0x00681B96]
	(No symbol) [0x00653F3C]
	(No symbol) [0x00654EBD]
	GetHandleVerifier [0x00A9AC73+3017699]
	GetHandleVerifier [0x00AAB93B+3086507]
	GetHandleVerifier [0x00AA40F2+3055714]
	GetHandleVerifier [0x00855AF0+637536]
	(No symbol) [0x00750A5D]
	(No symbol) [0x0074DA28]
	(No symbol) [0x0074DBC5]
	(No symbol) [0x007407F0]
	BaseThreadInitThunk [0x754CFCC9+25]
	RtlGetAppContainerNamedObjectPath [0x76F4809E+286]
	RtlGetAppContainerNamedObjectPath [0x76F4806E+238]

INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:02:29 KST)" (scheduled at 2025-01-14 19:02:09.436942+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:02:29 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 108, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 38, in load
    driver.get(url) # url 접속
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 347, in execute
    self.error_handler.check_response(response)
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchWindowException: Message: no such window: target window already closed
from unknown error: web view not found
  (Session info: chrome=131.0.6778.265)
Stacktrace:
	GetHandleVerifier [0x007BFD53+23747]
	(No symbol) [0x00747D54]
	(No symbol) [0x0061BE53]
	(No symbol) [0x005FD91B]
	(No symbol) [0x00687EFF]
	(No symbol) [0x0069AD49]
	(No symbol) [0x00681B96]
	(No symbol) [0x00653F3C]
	(No symbol) [0x00654EBD]
	GetHandleVerifier [0x00A9AC73+3017699]
	GetHandleVerifier [0x00AAB93B+3086507]
	GetHandleVerifier [0x00AA40F2+3055714]
	GetHandleVerifier [0x00855AF0+637536]
	(No symbol) [0x00750A5D]
	(No symbol) [0x0074DA28]
	(No symbol) [0x0074DBC5]
	(No symbol) [0x007407F0]
	BaseThreadInitThunk [0x754CFCC9+25]
	RtlGetAppContainerNamedObjectPath [0x76F4809E+286]
	RtlGetAppContainerNamedObjectPath [0x76F4806E+238]

INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:02:29 KST)" (scheduled at 2025-01-14 19:02:09.529942+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:02:29 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 108, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 38, in load
    driver.get(url) # url 접속
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 347, in execute
    self.error_handler.check_response(response)
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchWindowException: Message: no such window: target window already closed
from unknown error: web view not found
  (Session info: chrome=131.0.6778.265)
Stacktrace:
	GetHandleVerifier [0x007BFD53+23747]
	(No symbol) [0x00747D54]
	(No symbol) [0x0061BE53]
	(No symbol) [0x005FD91B]
	(No symbol) [0x00687EFF]
	(No symbol) [0x0069AD49]
	(No symbol) [0x00681B96]
	(No symbol) [0x00653F3C]
	(No symbol) [0x00654EBD]
	GetHandleVerifier [0x00A9AC73+3017699]
	GetHandleVerifier [0x00AAB93B+3086507]
	GetHandleVerifier [0x00AA40F2+3055714]
	GetHandleVerifier [0x00855AF0+637536]
	(No symbol) [0x00750A5D]
	(No symbol) [0x0074DA28]
	(No symbol) [0x0074DBC5]
	(No symbol) [0x007407F0]
	BaseThreadInitThunk [0x754CFCC9+25]
	RtlGetAppContainerNamedObjectPath [0x76F4809E+286]
	RtlGetAppContainerNamedObjectPath [0x76F4806E+238]

INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:02:30 KST)" (scheduled at 2025-01-14 19:02:10.966502+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:02:30 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 108, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 38, in load
    driver.get(url) # url 접속
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 347, in execute
    self.error_handler.check_response(response)
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: disconnected: not connected to DevTools
  (failed to check if window was closed: disconnected: not connected to DevTools)
  (Session info: chrome=131.0.6778.265)
Stacktrace:
	GetHandleVerifier [0x007BFD53+23747]
	(No symbol) [0x00747D54]
	(No symbol) [0x0061BE53]
	(No symbol) [0x0060C408]
	(No symbol) [0x0060C329]
	(No symbol) [0x0061E2E0]
	(No symbol) [0x0069AD69]
	(No symbol) [0x00681B96]
	(No symbol) [0x00653F3C]
	(No symbol) [0x00654EBD]
	GetHandleVerifier [0x00A9AC73+3017699]
	GetHandleVerifier [0x00AAB93B+3086507]
	GetHandleVerifier [0x00AA40F2+3055714]
	GetHandleVerifier [0x00855AF0+637536]
	(No symbol) [0x00750A5D]
	(No symbol) [0x0074DA28]
	(No symbol) [0x0074DBC5]
	(No symbol) [0x007407F0]
	BaseThreadInitThunk [0x754CFCC9+25]
	RtlGetAppContainerNamedObjectPath [0x76F4809E+286]
	RtlGetAppContainerNamedObjectPath [0x76F4806E+238]

INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:02:48 KST)" (scheduled at 2025-01-14 19:02:28.975517+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:02:48 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 108, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 38, in load
    driver.get(url) # url 접속
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 347, in execute
    self.error_handler.check_response(response)
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: disconnected: not connected to DevTools
  (failed to check if window was closed: disconnected: not connected to DevTools)
  (Session info: chrome=131.0.6778.265)
Stacktrace:
	GetHandleVerifier [0x007BFD53+23747]
	(No symbol) [0x00747D54]
	(No symbol) [0x0061BE53]
	(No symbol) [0x0060C408]
	(No symbol) [0x0060C329]
	(No symbol) [0x0061E2E0]
	(No symbol) [0x0069AD69]
	(No symbol) [0x00681B96]
	(No symbol) [0x00653F3C]
	(No symbol) [0x00654EBD]
	GetHandleVerifier [0x00A9AC73+3017699]
	GetHandleVerifier [0x00AAB93B+3086507]
	GetHandleVerifier [0x00AA40F2+3055714]
	GetHandleVerifier [0x00855AF0+637536]
	(No symbol) [0x00750A5D]
	(No symbol) [0x0074DA28]
	(No symbol) [0x0074DBC5]
	(No symbol) [0x007407F0]
	BaseThreadInitThunk [0x754CFCC9+25]
	RtlGetAppContainerNamedObjectPath [0x76F4809E+286]
	RtlGetAppContainerNamedObjectPath [0x76F4806E+238]

INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:02:49 KST)" (scheduled at 2025-01-14 19:02:29.436942+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:02:49 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 108, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 38, in load
    driver.get(url) # url 접속
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 347, in execute
    self.error_handler.check_response(response)
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchWindowException: Message: no such window: target window already closed
from unknown error: web view not found
  (Session info: chrome=131.0.6778.265)
Stacktrace:
	GetHandleVerifier [0x007BFD53+23747]
	(No symbol) [0x00747D54]
	(No symbol) [0x0061BE53]
	(No symbol) [0x005FD91B]
	(No symbol) [0x00687EFF]
	(No symbol) [0x0069AD49]
	(No symbol) [0x00681B96]
	(No symbol) [0x00653F3C]
	(No symbol) [0x00654EBD]
	GetHandleVerifier [0x00A9AC73+3017699]
	GetHandleVerifier [0x00AAB93B+3086507]
	GetHandleVerifier [0x00AA40F2+3055714]
	GetHandleVerifier [0x00855AF0+637536]
	(No symbol) [0x00750A5D]
	(No symbol) [0x0074DA28]
	(No symbol) [0x0074DBC5]
	(No symbol) [0x007407F0]
	BaseThreadInitThunk [0x754CFCC9+25]
	RtlGetAppContainerNamedObjectPath [0x76F4809E+286]
	RtlGetAppContainerNamedObjectPath [0x76F4806E+238]

INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:02:49 KST)" (scheduled at 2025-01-14 19:02:29.529942+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:02:49 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 108, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 38, in load
    driver.get(url) # url 접속
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 347, in execute
    self.error_handler.check_response(response)
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchWindowException: Message: no such window: target window already closed
from unknown error: web view not found
  (Session info: chrome=131.0.6778.265)
Stacktrace:
	GetHandleVerifier [0x007BFD53+23747]
	(No symbol) [0x00747D54]
	(No symbol) [0x0061BE53]
	(No symbol) [0x005FD91B]
	(No symbol) [0x00687EFF]
	(No symbol) [0x0069AD49]
	(No symbol) [0x00681B96]
	(No symbol) [0x00653F3C]
	(No symbol) [0x00654EBD]
	GetHandleVerifier [0x00A9AC73+3017699]
	GetHandleVerifier [0x00AAB93B+3086507]
	GetHandleVerifier [0x00AA40F2+3055714]
	GetHandleVerifier [0x00855AF0+637536]
	(No symbol) [0x00750A5D]
	(No symbol) [0x0074DA28]
	(No symbol) [0x0074DBC5]
	(No symbol) [0x007407F0]
	BaseThreadInitThunk [0x754CFCC9+25]
	RtlGetAppContainerNamedObjectPath [0x76F4809E+286]
	RtlGetAppContainerNamedObjectPath [0x76F4806E+238]

INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:02:50 KST)" (scheduled at 2025-01-14 19:02:30.966502+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:02:50 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 108, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 38, in load
    driver.get(url) # url 접속
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 347, in execute
    self.error_handler.check_response(response)
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: disconnected: not connected to DevTools
  (failed to check if window was closed: disconnected: not connected to DevTools)
  (Session info: chrome=131.0.6778.265)
Stacktrace:
	GetHandleVerifier [0x007BFD53+23747]
	(No symbol) [0x00747D54]
	(No symbol) [0x0061BE53]
	(No symbol) [0x0060C408]
	(No symbol) [0x0060C329]
	(No symbol) [0x0061E2E0]
	(No symbol) [0x0069AD69]
	(No symbol) [0x00681B96]
	(No symbol) [0x00653F3C]
	(No symbol) [0x00654EBD]
	GetHandleVerifier [0x00A9AC73+3017699]
	GetHandleVerifier [0x00AAB93B+3086507]
	GetHandleVerifier [0x00AA40F2+3055714]
	GetHandleVerifier [0x00855AF0+637536]
	(No symbol) [0x00750A5D]
	(No symbol) [0x0074DA28]
	(No symbol) [0x0074DBC5]
	(No symbol) [0x007407F0]
	BaseThreadInitThunk [0x754CFCC9+25]
	RtlGetAppContainerNamedObjectPath [0x76F4809E+286]
	RtlGetAppContainerNamedObjectPath [0x76F4806E+238]

INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:03:08 KST)" (scheduled at 2025-01-14 19:02:48.975517+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:03:08 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 108, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 38, in load
    driver.get(url) # url 접속
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 347, in execute
    self.error_handler.check_response(response)
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: disconnected: not connected to DevTools
  (failed to check if window was closed: disconnected: not connected to DevTools)
  (Session info: chrome=131.0.6778.265)
Stacktrace:
	GetHandleVerifier [0x007BFD53+23747]
	(No symbol) [0x00747D54]
	(No symbol) [0x0061BE53]
	(No symbol) [0x0060C408]
	(No symbol) [0x0060C329]
	(No symbol) [0x0061E2E0]
	(No symbol) [0x0069AD69]
	(No symbol) [0x00681B96]
	(No symbol) [0x00653F3C]
	(No symbol) [0x00654EBD]
	GetHandleVerifier [0x00A9AC73+3017699]
	GetHandleVerifier [0x00AAB93B+3086507]
	GetHandleVerifier [0x00AA40F2+3055714]
	GetHandleVerifier [0x00855AF0+637536]
	(No symbol) [0x00750A5D]
	(No symbol) [0x0074DA28]
	(No symbol) [0x0074DBC5]
	(No symbol) [0x007407F0]
	BaseThreadInitThunk [0x754CFCC9+25]
	RtlGetAppContainerNamedObjectPath [0x76F4809E+286]
	RtlGetAppContainerNamedObjectPath [0x76F4806E+238]

INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:03:09 KST)" (scheduled at 2025-01-14 19:02:49.436942+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:03:09 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 108, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 38, in load
    driver.get(url) # url 접속
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 347, in execute
    self.error_handler.check_response(response)
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchWindowException: Message: no such window: target window already closed
from unknown error: web view not found
  (Session info: chrome=131.0.6778.265)
Stacktrace:
	GetHandleVerifier [0x007BFD53+23747]
	(No symbol) [0x00747D54]
	(No symbol) [0x0061BE53]
	(No symbol) [0x005FD91B]
	(No symbol) [0x00687EFF]
	(No symbol) [0x0069AD49]
	(No symbol) [0x00681B96]
	(No symbol) [0x00653F3C]
	(No symbol) [0x00654EBD]
	GetHandleVerifier [0x00A9AC73+3017699]
	GetHandleVerifier [0x00AAB93B+3086507]
	GetHandleVerifier [0x00AA40F2+3055714]
	GetHandleVerifier [0x00855AF0+637536]
	(No symbol) [0x00750A5D]
	(No symbol) [0x0074DA28]
	(No symbol) [0x0074DBC5]
	(No symbol) [0x007407F0]
	BaseThreadInitThunk [0x754CFCC9+25]
	RtlGetAppContainerNamedObjectPath [0x76F4809E+286]
	RtlGetAppContainerNamedObjectPath [0x76F4806E+238]

INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:03:09 KST)" (scheduled at 2025-01-14 19:02:49.529942+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:03:09 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 108, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 38, in load
    driver.get(url) # url 접속
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 347, in execute
    self.error_handler.check_response(response)
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchWindowException: Message: no such window: target window already closed
from unknown error: web view not found
  (Session info: chrome=131.0.6778.265)
Stacktrace:
	GetHandleVerifier [0x007BFD53+23747]
	(No symbol) [0x00747D54]
	(No symbol) [0x0061BE53]
	(No symbol) [0x005FD91B]
	(No symbol) [0x00687EFF]
	(No symbol) [0x0069AD49]
	(No symbol) [0x00681B96]
	(No symbol) [0x00653F3C]
	(No symbol) [0x00654EBD]
	GetHandleVerifier [0x00A9AC73+3017699]
	GetHandleVerifier [0x00AAB93B+3086507]
	GetHandleVerifier [0x00AA40F2+3055714]
	GetHandleVerifier [0x00855AF0+637536]
	(No symbol) [0x00750A5D]
	(No symbol) [0x0074DA28]
	(No symbol) [0x0074DBC5]
	(No symbol) [0x007407F0]
	BaseThreadInitThunk [0x754CFCC9+25]
	RtlGetAppContainerNamedObjectPath [0x76F4809E+286]
	RtlGetAppContainerNamedObjectPath [0x76F4806E+238]

INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:03:10 KST)" (scheduled at 2025-01-14 19:02:50.966502+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:03:10 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 108, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 38, in load
    driver.get(url) # url 접속
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 347, in execute
    self.error_handler.check_response(response)
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: disconnected: not connected to DevTools
  (failed to check if window was closed: disconnected: not connected to DevTools)
  (Session info: chrome=131.0.6778.265)
Stacktrace:
	GetHandleVerifier [0x007BFD53+23747]
	(No symbol) [0x00747D54]
	(No symbol) [0x0061BE53]
	(No symbol) [0x0060C408]
	(No symbol) [0x0060C329]
	(No symbol) [0x0061E2E0]
	(No symbol) [0x0069AD69]
	(No symbol) [0x00681B96]
	(No symbol) [0x00653F3C]
	(No symbol) [0x00654EBD]
	GetHandleVerifier [0x00A9AC73+3017699]
	GetHandleVerifier [0x00AAB93B+3086507]
	GetHandleVerifier [0x00AA40F2+3055714]
	GetHandleVerifier [0x00855AF0+637536]
	(No symbol) [0x00750A5D]
	(No symbol) [0x0074DA28]
	(No symbol) [0x0074DBC5]
	(No symbol) [0x007407F0]
	BaseThreadInitThunk [0x754CFCC9+25]
	RtlGetAppContainerNamedObjectPath [0x76F4809E+286]
	RtlGetAppContainerNamedObjectPath [0x76F4806E+238]

INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:03:28 KST)" (scheduled at 2025-01-14 19:03:08.975517+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:03:28 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 108, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 38, in load
    driver.get(url) # url 접속
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 347, in execute
    self.error_handler.check_response(response)
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: disconnected: not connected to DevTools
  (failed to check if window was closed: disconnected: not connected to DevTools)
  (Session info: chrome=131.0.6778.265)
Stacktrace:
	GetHandleVerifier [0x007BFD53+23747]
	(No symbol) [0x00747D54]
	(No symbol) [0x0061BE53]
	(No symbol) [0x0060C408]
	(No symbol) [0x0060C329]
	(No symbol) [0x0061E2E0]
	(No symbol) [0x0069AD69]
	(No symbol) [0x00681B96]
	(No symbol) [0x00653F3C]
	(No symbol) [0x00654EBD]
	GetHandleVerifier [0x00A9AC73+3017699]
	GetHandleVerifier [0x00AAB93B+3086507]
	GetHandleVerifier [0x00AA40F2+3055714]
	GetHandleVerifier [0x00855AF0+637536]
	(No symbol) [0x00750A5D]
	(No symbol) [0x0074DA28]
	(No symbol) [0x0074DBC5]
	(No symbol) [0x007407F0]
	BaseThreadInitThunk [0x754CFCC9+25]
	RtlGetAppContainerNamedObjectPath [0x76F4809E+286]
	RtlGetAppContainerNamedObjectPath [0x76F4806E+238]

INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:03:29 KST)" (scheduled at 2025-01-14 19:03:09.436942+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:03:29 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 108, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 38, in load
    driver.get(url) # url 접속
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 347, in execute
    self.error_handler.check_response(response)
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchWindowException: Message: no such window: target window already closed
from unknown error: web view not found
  (Session info: chrome=131.0.6778.265)
Stacktrace:
	GetHandleVerifier [0x007BFD53+23747]
	(No symbol) [0x00747D54]
	(No symbol) [0x0061BE53]
	(No symbol) [0x005FD91B]
	(No symbol) [0x00687EFF]
	(No symbol) [0x0069AD49]
	(No symbol) [0x00681B96]
	(No symbol) [0x00653F3C]
	(No symbol) [0x00654EBD]
	GetHandleVerifier [0x00A9AC73+3017699]
	GetHandleVerifier [0x00AAB93B+3086507]
	GetHandleVerifier [0x00AA40F2+3055714]
	GetHandleVerifier [0x00855AF0+637536]
	(No symbol) [0x00750A5D]
	(No symbol) [0x0074DA28]
	(No symbol) [0x0074DBC5]
	(No symbol) [0x007407F0]
	BaseThreadInitThunk [0x754CFCC9+25]
	RtlGetAppContainerNamedObjectPath [0x76F4809E+286]
	RtlGetAppContainerNamedObjectPath [0x76F4806E+238]

INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:03:29 KST)" (scheduled at 2025-01-14 19:03:09.529942+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:03:29 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 108, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 38, in load
    driver.get(url) # url 접속
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 347, in execute
    self.error_handler.check_response(response)
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchWindowException: Message: no such window: target window already closed
from unknown error: web view not found
  (Session info: chrome=131.0.6778.265)
Stacktrace:
	GetHandleVerifier [0x007BFD53+23747]
	(No symbol) [0x00747D54]
	(No symbol) [0x0061BE53]
	(No symbol) [0x005FD91B]
	(No symbol) [0x00687EFF]
	(No symbol) [0x0069AD49]
	(No symbol) [0x00681B96]
	(No symbol) [0x00653F3C]
	(No symbol) [0x00654EBD]
	GetHandleVerifier [0x00A9AC73+3017699]
	GetHandleVerifier [0x00AAB93B+3086507]
	GetHandleVerifier [0x00AA40F2+3055714]
	GetHandleVerifier [0x00855AF0+637536]
	(No symbol) [0x00750A5D]
	(No symbol) [0x0074DA28]
	(No symbol) [0x0074DBC5]
	(No symbol) [0x007407F0]
	BaseThreadInitThunk [0x754CFCC9+25]
	RtlGetAppContainerNamedObjectPath [0x76F4809E+286]
	RtlGetAppContainerNamedObjectPath [0x76F4806E+238]

INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:03:30 KST)" (scheduled at 2025-01-14 19:03:10.966502+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:03:30 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 108, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 38, in load
    driver.get(url) # url 접속
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 347, in execute
    self.error_handler.check_response(response)
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: disconnected: not connected to DevTools
  (failed to check if window was closed: disconnected: not connected to DevTools)
  (Session info: chrome=131.0.6778.265)
Stacktrace:
	GetHandleVerifier [0x007BFD53+23747]
	(No symbol) [0x00747D54]
	(No symbol) [0x0061BE53]
	(No symbol) [0x0060C408]
	(No symbol) [0x0060C329]
	(No symbol) [0x0061E2E0]
	(No symbol) [0x0069AD69]
	(No symbol) [0x00681B96]
	(No symbol) [0x00653F3C]
	(No symbol) [0x00654EBD]
	GetHandleVerifier [0x00A9AC73+3017699]
	GetHandleVerifier [0x00AAB93B+3086507]
	GetHandleVerifier [0x00AA40F2+3055714]
	GetHandleVerifier [0x00855AF0+637536]
	(No symbol) [0x00750A5D]
	(No symbol) [0x0074DA28]
	(No symbol) [0x0074DBC5]
	(No symbol) [0x007407F0]
	BaseThreadInitThunk [0x754CFCC9+25]
	RtlGetAppContainerNamedObjectPath [0x76F4809E+286]
	RtlGetAppContainerNamedObjectPath [0x76F4806E+238]

INFO:werkzeug: * Detected change in 'C:\\kd\\ws_java\\team4_v2sbm3c\\src\\main\\python\\exc.py', reloading
INFO:werkzeug: * Detected change in 'C:\\kd\\ws_java\\team4_v2sbm3c\\src\\main\\python\\exc.py', reloading
INFO:werkzeug: * Detected change in 'C:\\kd\\ws_java\\team4_v2sbm3c\\src\\main\\python\\exc.py', reloading
INFO:werkzeug: * Detected change in 'C:\\kd\\ws_java\\team4_v2sbm3c\\src\\main\\python\\exc.py', reloading
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:====== WebDriver manager ======
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:03:48 KST)" (scheduled at 2025-01-14 19:03:28.975517+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:03:48 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 108, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 38, in load
    driver.get(url) # url 접속
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 347, in execute
    self.error_handler.check_response(response)
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: disconnected: not connected to DevTools
  (failed to check if window was closed: disconnected: not connected to DevTools)
  (Session info: chrome=131.0.6778.265)
Stacktrace:
	GetHandleVerifier [0x007BFD53+23747]
	(No symbol) [0x00747D54]
	(No symbol) [0x0061BE53]
	(No symbol) [0x0060C408]
	(No symbol) [0x0060C329]
	(No symbol) [0x0061E2E0]
	(No symbol) [0x0069AD69]
	(No symbol) [0x00681B96]
	(No symbol) [0x00653F3C]
	(No symbol) [0x00654EBD]
	GetHandleVerifier [0x00A9AC73+3017699]
	GetHandleVerifier [0x00AAB93B+3086507]
	GetHandleVerifier [0x00AA40F2+3055714]
	GetHandleVerifier [0x00855AF0+637536]
	(No symbol) [0x00750A5D]
	(No symbol) [0x0074DA28]
	(No symbol) [0x0074DBC5]
	(No symbol) [0x007407F0]
	BaseThreadInitThunk [0x754CFCC9+25]
	RtlGetAppContainerNamedObjectPath [0x76F4809E+286]
	RtlGetAppContainerNamedObjectPath [0x76F4806E+238]

INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\i\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\i\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:03:50 KST)" (scheduled at 2025-01-14 19:03:30.966502+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:03:50 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 108, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 38, in load
    driver.get(url) # url 접속
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 347, in execute
    self.error_handler.check_response(response)
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: disconnected: not connected to DevTools
  (failed to check if window was closed: disconnected: not connected to DevTools)
  (Session info: chrome=131.0.6778.265)
Stacktrace:
	GetHandleVerifier [0x007BFD53+23747]
	(No symbol) [0x00747D54]
	(No symbol) [0x0061BE53]
	(No symbol) [0x0060C408]
	(No symbol) [0x0060C329]
	(No symbol) [0x0061E2E0]
	(No symbol) [0x0069AD69]
	(No symbol) [0x00681B96]
	(No symbol) [0x00653F3C]
	(No symbol) [0x00654EBD]
	GetHandleVerifier [0x00A9AC73+3017699]
	GetHandleVerifier [0x00AAB93B+3086507]
	GetHandleVerifier [0x00AA40F2+3055714]
	GetHandleVerifier [0x00855AF0+637536]
	(No symbol) [0x00750A5D]
	(No symbol) [0x0074DA28]
	(No symbol) [0x0074DBC5]
	(No symbol) [0x007407F0]
	BaseThreadInitThunk [0x754CFCC9+25]
	RtlGetAppContainerNamedObjectPath [0x76F4809E+286]
	RtlGetAppContainerNamedObjectPath [0x76F4806E+238]

INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 569-178-434
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 569-178-434
INFO:werkzeug: * Detected change in 'C:\\kd\\ws_java\\team4_v2sbm3c\\src\\main\\python\\exc.py', reloading
INFO:werkzeug: * Detected change in 'C:\\kd\\ws_java\\team4_v2sbm3c\\src\\main\\python\\exc.py', reloading
INFO:werkzeug: * Detected change in 'C:\\kd\\ws_java\\team4_v2sbm3c\\src\\main\\python\\exc.py', reloading
INFO:werkzeug: * Detected change in 'C:\\kd\\ws_java\\team4_v2sbm3c\\src\\main\\python\\exc.py', reloading
INFO:werkzeug: * Detected change in 'C:\\kd\\ws_java\\team4_v2sbm3c\\src\\main\\python\\exc.py', reloading
INFO:werkzeug: * Detected change in 'C:\\kd\\ws_java\\team4_v2sbm3c\\src\\main\\python\\exc.py', reloading
INFO:werkzeug: * Detected change in 'C:\\kd\\ws_java\\team4_v2sbm3c\\src\\main\\python\\exc.py', reloading
INFO:werkzeug: * Detected change in 'C:\\kd\\ws_java\\team4_v2sbm3c\\src\\main\\python\\exc.py', reloading
INFO:werkzeug: * Detected change in 'C:\\kd\\ws_java\\team4_v2sbm3c\\src\\main\\python\\exc.py', reloading
INFO:werkzeug: * Detected change in 'C:\\kd\\ws_java\\team4_v2sbm3c\\src\\main\\python\\exc.py', reloading
INFO:werkzeug: * Detected change in 'C:\\kd\\ws_java\\team4_v2sbm3c\\src\\main\\python\\exc.py', reloading
INFO:werkzeug: * Detected change in 'C:\\kd\\ws_java\\team4_v2sbm3c\\src\\main\\python\\exc.py', reloading
INFO:werkzeug: * Detected change in 'C:\\kd\\ws_java\\team4_v2sbm3c\\src\\main\\python\\exc.py', reloading
INFO:werkzeug: * Detected change in 'C:\\kd\\ws_java\\team4_v2sbm3c\\src\\main\\python\\exc.py', reloading
INFO:werkzeug: * Detected change in 'C:\\kd\\ws_java\\team4_v2sbm3c\\src\\main\\python\\exc.py', reloading
INFO:werkzeug: * Detected change in 'C:\\kd\\ws_java\\team4_v2sbm3c\\src\\main\\python\\exc.py', reloading
INFO:werkzeug: * Detected change in 'C:\\kd\\ws_java\\team4_v2sbm3c\\src\\main\\python\\exc.py', reloading
INFO:werkzeug: * Detected change in 'C:\\kd\\ws_java\\team4_v2sbm3c\\src\\main\\python\\exc.py', reloading
INFO:werkzeug: * Detected change in 'C:\\kd\\ws_java\\team4_v2sbm3c\\src\\main\\python\\exc.py', reloading
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\i\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\i\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 569-178-434
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 569-178-434
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:04:08 KST)" (scheduled at 2025-01-14 19:03:48.975517+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:04:08 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 108, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 38, in load
    driver.get(url) # url 접속
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 347, in execute
    self.error_handler.check_response(response)
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: disconnected: not connected to DevTools
  (failed to check if window was closed: disconnected: not connected to DevTools)
  (Session info: chrome=131.0.6778.265)
Stacktrace:
	GetHandleVerifier [0x007BFD53+23747]
	(No symbol) [0x00747D54]
	(No symbol) [0x0061BE53]
	(No symbol) [0x0060C408]
	(No symbol) [0x0060C329]
	(No symbol) [0x0061E2E0]
	(No symbol) [0x0069AD69]
	(No symbol) [0x00681B96]
	(No symbol) [0x00653F3C]
	(No symbol) [0x00654EBD]
	GetHandleVerifier [0x00A9AC73+3017699]
	GetHandleVerifier [0x00AAB93B+3086507]
	GetHandleVerifier [0x00AA40F2+3055714]
	GetHandleVerifier [0x00855AF0+637536]
	(No symbol) [0x00750A5D]
	(No symbol) [0x0074DA28]
	(No symbol) [0x0074DBC5]
	(No symbol) [0x007407F0]
	BaseThreadInitThunk [0x754CFCC9+25]
	RtlGetAppContainerNamedObjectPath [0x76F4809E+286]
	RtlGetAppContainerNamedObjectPath [0x76F4806E+238]

INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:04:10 KST)" (scheduled at 2025-01-14 19:03:50.966502+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:04:10 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 108, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 38, in load
    driver.get(url) # url 접속
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 347, in execute
    self.error_handler.check_response(response)
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: disconnected: not connected to DevTools
  (failed to check if window was closed: disconnected: not connected to DevTools)
  (Session info: chrome=131.0.6778.265)
Stacktrace:
	GetHandleVerifier [0x007BFD53+23747]
	(No symbol) [0x00747D54]
	(No symbol) [0x0061BE53]
	(No symbol) [0x0060C408]
	(No symbol) [0x0060C329]
	(No symbol) [0x0061E2E0]
	(No symbol) [0x0069AD69]
	(No symbol) [0x00681B96]
	(No symbol) [0x00653F3C]
	(No symbol) [0x00654EBD]
	GetHandleVerifier [0x00A9AC73+3017699]
	GetHandleVerifier [0x00AAB93B+3086507]
	GetHandleVerifier [0x00AA40F2+3055714]
	GetHandleVerifier [0x00855AF0+637536]
	(No symbol) [0x00750A5D]
	(No symbol) [0x0074DA28]
	(No symbol) [0x0074DBC5]
	(No symbol) [0x007407F0]
	BaseThreadInitThunk [0x754CFCC9+25]
	RtlGetAppContainerNamedObjectPath [0x76F4809E+286]
	RtlGetAppContainerNamedObjectPath [0x76F4806E+238]

INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:04:07 KST)" (scheduled at 2025-01-14 19:04:07.617254+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:04:07 KST)" (scheduled at 2025-01-14 19:04:07.804253+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:04:28 KST)" (scheduled at 2025-01-14 19:04:08.975517+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:04:28 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 108, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 38, in load
    driver.get(url) # url 접속
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 347, in execute
    self.error_handler.check_response(response)
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: disconnected: not connected to DevTools
  (failed to check if window was closed: disconnected: not connected to DevTools)
  (Session info: chrome=131.0.6778.265)
Stacktrace:
	GetHandleVerifier [0x007BFD53+23747]
	(No symbol) [0x00747D54]
	(No symbol) [0x0061BE53]
	(No symbol) [0x0060C408]
	(No symbol) [0x0060C329]
	(No symbol) [0x0061E2E0]
	(No symbol) [0x0069AD69]
	(No symbol) [0x00681B96]
	(No symbol) [0x00653F3C]
	(No symbol) [0x00654EBD]
	GetHandleVerifier [0x00A9AC73+3017699]
	GetHandleVerifier [0x00AAB93B+3086507]
	GetHandleVerifier [0x00AA40F2+3055714]
	GetHandleVerifier [0x00855AF0+637536]
	(No symbol) [0x00750A5D]
	(No symbol) [0x0074DA28]
	(No symbol) [0x0074DBC5]
	(No symbol) [0x007407F0]
	BaseThreadInitThunk [0x754CFCC9+25]
	RtlGetAppContainerNamedObjectPath [0x76F4809E+286]
	RtlGetAppContainerNamedObjectPath [0x76F4806E+238]

INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:04:30 KST)" (scheduled at 2025-01-14 19:04:10.966502+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:04:30 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 108, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 38, in load
    driver.get(url) # url 접속
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 347, in execute
    self.error_handler.check_response(response)
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: disconnected: not connected to DevTools
  (failed to check if window was closed: disconnected: not connected to DevTools)
  (Session info: chrome=131.0.6778.265)
Stacktrace:
	GetHandleVerifier [0x007BFD53+23747]
	(No symbol) [0x00747D54]
	(No symbol) [0x0061BE53]
	(No symbol) [0x0060C408]
	(No symbol) [0x0060C329]
	(No symbol) [0x0061E2E0]
	(No symbol) [0x0069AD69]
	(No symbol) [0x00681B96]
	(No symbol) [0x00653F3C]
	(No symbol) [0x00654EBD]
	GetHandleVerifier [0x00A9AC73+3017699]
	GetHandleVerifier [0x00AAB93B+3086507]
	GetHandleVerifier [0x00AA40F2+3055714]
	GetHandleVerifier [0x00855AF0+637536]
	(No symbol) [0x00750A5D]
	(No symbol) [0x0074DA28]
	(No symbol) [0x0074DBC5]
	(No symbol) [0x007407F0]
	BaseThreadInitThunk [0x754CFCC9+25]
	RtlGetAppContainerNamedObjectPath [0x76F4809E+286]
	RtlGetAppContainerNamedObjectPath [0x76F4806E+238]

ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:04:27 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 129, in crawl_and_insert_data
    url = driver.current_url
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 437, in current_url
    return self.execute(Command.GET_CURRENT_URL)["value"]
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 347, in execute
    self.error_handler.check_response(response)
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchWindowException: Message: no such window: target window already closed
from unknown error: web view not found
  (Session info: chrome=131.0.6778.265)
Stacktrace:
	GetHandleVerifier [0x007BFD53+23747]
	(No symbol) [0x00747D54]
	(No symbol) [0x0061BE53]
	(No symbol) [0x005FD91B]
	(No symbol) [0x00687EFF]
	(No symbol) [0x0069AD49]
	(No symbol) [0x00681B96]
	(No symbol) [0x00653F3C]
	(No symbol) [0x00654EBD]
	GetHandleVerifier [0x00A9AC73+3017699]
	GetHandleVerifier [0x00AAB93B+3086507]
	GetHandleVerifier [0x00AA40F2+3055714]
	GetHandleVerifier [0x00855AF0+637536]
	(No symbol) [0x00750A5D]
	(No symbol) [0x0074DA28]
	(No symbol) [0x0074DBC5]
	(No symbol) [0x007407F0]
	BaseThreadInitThunk [0x754CFCC9+25]
	RtlGetAppContainerNamedObjectPath [0x76F4809E+286]
	RtlGetAppContainerNamedObjectPath [0x76F4806E+238]

ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:04:27 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 222, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2014' in position 80: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:04:47 KST)" (scheduled at 2025-01-14 19:04:27.617254+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:04:47 KST)" (scheduled at 2025-01-14 19:04:27.804253+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:04:47 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 108, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 38, in load
    driver.get(url) # url 접속
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 347, in execute
    self.error_handler.check_response(response)
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchWindowException: Message: no such window: target window already closed
from unknown error: web view not found
  (Session info: chrome=131.0.6778.265)
Stacktrace:
	GetHandleVerifier [0x007BFD53+23747]
	(No symbol) [0x00747D54]
	(No symbol) [0x0061BE53]
	(No symbol) [0x005FD91B]
	(No symbol) [0x00687EFF]
	(No symbol) [0x0069AD49]
	(No symbol) [0x00681B96]
	(No symbol) [0x00653F3C]
	(No symbol) [0x00654EBD]
	GetHandleVerifier [0x00A9AC73+3017699]
	GetHandleVerifier [0x00AAB93B+3086507]
	GetHandleVerifier [0x00AA40F2+3055714]
	GetHandleVerifier [0x00855AF0+637536]
	(No symbol) [0x00750A5D]
	(No symbol) [0x0074DA28]
	(No symbol) [0x0074DBC5]
	(No symbol) [0x007407F0]
	BaseThreadInitThunk [0x754CFCC9+25]
	RtlGetAppContainerNamedObjectPath [0x76F4809E+286]
	RtlGetAppContainerNamedObjectPath [0x76F4806E+238]

INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:04:48 KST)" (scheduled at 2025-01-14 19:04:28.975517+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:04:48 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 108, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 38, in load
    driver.get(url) # url 접속
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 347, in execute
    self.error_handler.check_response(response)
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: disconnected: not connected to DevTools
  (failed to check if window was closed: disconnected: not connected to DevTools)
  (Session info: chrome=131.0.6778.265)
Stacktrace:
	GetHandleVerifier [0x007BFD53+23747]
	(No symbol) [0x00747D54]
	(No symbol) [0x0061BE53]
	(No symbol) [0x0060C408]
	(No symbol) [0x0060C329]
	(No symbol) [0x0061E2E0]
	(No symbol) [0x0069AD69]
	(No symbol) [0x00681B96]
	(No symbol) [0x00653F3C]
	(No symbol) [0x00654EBD]
	GetHandleVerifier [0x00A9AC73+3017699]
	GetHandleVerifier [0x00AAB93B+3086507]
	GetHandleVerifier [0x00AA40F2+3055714]
	GetHandleVerifier [0x00855AF0+637536]
	(No symbol) [0x00750A5D]
	(No symbol) [0x0074DA28]
	(No symbol) [0x0074DBC5]
	(No symbol) [0x007407F0]
	BaseThreadInitThunk [0x754CFCC9+25]
	RtlGetAppContainerNamedObjectPath [0x76F4809E+286]
	RtlGetAppContainerNamedObjectPath [0x76F4806E+238]

INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:04:50 KST)" (scheduled at 2025-01-14 19:04:30.966502+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:04:50 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 108, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 38, in load
    driver.get(url) # url 접속
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 347, in execute
    self.error_handler.check_response(response)
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: disconnected: not connected to DevTools
  (failed to check if window was closed: disconnected: not connected to DevTools)
  (Session info: chrome=131.0.6778.265)
Stacktrace:
	GetHandleVerifier [0x007BFD53+23747]
	(No symbol) [0x00747D54]
	(No symbol) [0x0061BE53]
	(No symbol) [0x0060C408]
	(No symbol) [0x0060C329]
	(No symbol) [0x0061E2E0]
	(No symbol) [0x0069AD69]
	(No symbol) [0x00681B96]
	(No symbol) [0x00653F3C]
	(No symbol) [0x00654EBD]
	GetHandleVerifier [0x00A9AC73+3017699]
	GetHandleVerifier [0x00AAB93B+3086507]
	GetHandleVerifier [0x00AA40F2+3055714]
	GetHandleVerifier [0x00855AF0+637536]
	(No symbol) [0x00750A5D]
	(No symbol) [0x0074DA28]
	(No symbol) [0x0074DBC5]
	(No symbol) [0x007407F0]
	BaseThreadInitThunk [0x754CFCC9+25]
	RtlGetAppContainerNamedObjectPath [0x76F4809E+286]
	RtlGetAppContainerNamedObjectPath [0x76F4806E+238]

ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:04:47 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 222, in crawl_and_insert_data
    print(p_tag.text.strip())
UnicodeEncodeError: 'cp949' codec can't encode character '\u2014' in position 80: illegal multibyte sequence
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:05:07 KST)" (scheduled at 2025-01-14 19:04:47.617254+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:05:07 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 108, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 38, in load
    driver.get(url) # url 접속
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 347, in execute
    self.error_handler.check_response(response)
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchWindowException: Message: no such window: target window already closed
from unknown error: web view not found
  (Session info: chrome=131.0.6778.265)
Stacktrace:
	GetHandleVerifier [0x007BFD53+23747]
	(No symbol) [0x00747D54]
	(No symbol) [0x0061BE53]
	(No symbol) [0x005FD91B]
	(No symbol) [0x00687EFF]
	(No symbol) [0x0069AD49]
	(No symbol) [0x00681B96]
	(No symbol) [0x00653F3C]
	(No symbol) [0x00654EBD]
	GetHandleVerifier [0x00A9AC73+3017699]
	GetHandleVerifier [0x00AAB93B+3086507]
	GetHandleVerifier [0x00AA40F2+3055714]
	GetHandleVerifier [0x00855AF0+637536]
	(No symbol) [0x00750A5D]
	(No symbol) [0x0074DA28]
	(No symbol) [0x0074DBC5]
	(No symbol) [0x007407F0]
	BaseThreadInitThunk [0x754CFCC9+25]
	RtlGetAppContainerNamedObjectPath [0x76F4809E+286]
	RtlGetAppContainerNamedObjectPath [0x76F4806E+238]

INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:05:07 KST)" (scheduled at 2025-01-14 19:04:47.804253+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:05:07 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 108, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 38, in load
    driver.get(url) # url 접속
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 347, in execute
    self.error_handler.check_response(response)
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchWindowException: Message: no such window: target window already closed
from unknown error: web view not found
  (Session info: chrome=131.0.6778.265)
Stacktrace:
	GetHandleVerifier [0x007BFD53+23747]
	(No symbol) [0x00747D54]
	(No symbol) [0x0061BE53]
	(No symbol) [0x005FD91B]
	(No symbol) [0x00687EFF]
	(No symbol) [0x0069AD49]
	(No symbol) [0x00681B96]
	(No symbol) [0x00653F3C]
	(No symbol) [0x00654EBD]
	GetHandleVerifier [0x00A9AC73+3017699]
	GetHandleVerifier [0x00AAB93B+3086507]
	GetHandleVerifier [0x00AA40F2+3055714]
	GetHandleVerifier [0x00855AF0+637536]
	(No symbol) [0x00750A5D]
	(No symbol) [0x0074DA28]
	(No symbol) [0x0074DBC5]
	(No symbol) [0x007407F0]
	BaseThreadInitThunk [0x754CFCC9+25]
	RtlGetAppContainerNamedObjectPath [0x76F4809E+286]
	RtlGetAppContainerNamedObjectPath [0x76F4806E+238]

INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:05:08 KST)" (scheduled at 2025-01-14 19:04:48.975517+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:05:08 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 108, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 38, in load
    driver.get(url) # url 접속
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 347, in execute
    self.error_handler.check_response(response)
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: disconnected: not connected to DevTools
  (failed to check if window was closed: disconnected: not connected to DevTools)
  (Session info: chrome=131.0.6778.265)
Stacktrace:
	GetHandleVerifier [0x007BFD53+23747]
	(No symbol) [0x00747D54]
	(No symbol) [0x0061BE53]
	(No symbol) [0x0060C408]
	(No symbol) [0x0060C329]
	(No symbol) [0x0061E2E0]
	(No symbol) [0x0069AD69]
	(No symbol) [0x00681B96]
	(No symbol) [0x00653F3C]
	(No symbol) [0x00654EBD]
	GetHandleVerifier [0x00A9AC73+3017699]
	GetHandleVerifier [0x00AAB93B+3086507]
	GetHandleVerifier [0x00AA40F2+3055714]
	GetHandleVerifier [0x00855AF0+637536]
	(No symbol) [0x00750A5D]
	(No symbol) [0x0074DA28]
	(No symbol) [0x0074DBC5]
	(No symbol) [0x007407F0]
	BaseThreadInitThunk [0x754CFCC9+25]
	RtlGetAppContainerNamedObjectPath [0x76F4809E+286]
	RtlGetAppContainerNamedObjectPath [0x76F4806E+238]

INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:05:10 KST)" (scheduled at 2025-01-14 19:04:50.966502+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:05:10 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 108, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 38, in load
    driver.get(url) # url 접속
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 347, in execute
    self.error_handler.check_response(response)
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: disconnected: not connected to DevTools
  (failed to check if window was closed: disconnected: not connected to DevTools)
  (Session info: chrome=131.0.6778.265)
Stacktrace:
	GetHandleVerifier [0x007BFD53+23747]
	(No symbol) [0x00747D54]
	(No symbol) [0x0061BE53]
	(No symbol) [0x0060C408]
	(No symbol) [0x0060C329]
	(No symbol) [0x0061E2E0]
	(No symbol) [0x0069AD69]
	(No symbol) [0x00681B96]
	(No symbol) [0x00653F3C]
	(No symbol) [0x00654EBD]
	GetHandleVerifier [0x00A9AC73+3017699]
	GetHandleVerifier [0x00AAB93B+3086507]
	GetHandleVerifier [0x00AA40F2+3055714]
	GetHandleVerifier [0x00855AF0+637536]
	(No symbol) [0x00750A5D]
	(No symbol) [0x0074DA28]
	(No symbol) [0x0074DBC5]
	(No symbol) [0x007407F0]
	BaseThreadInitThunk [0x754CFCC9+25]
	RtlGetAppContainerNamedObjectPath [0x76F4809E+286]
	RtlGetAppContainerNamedObjectPath [0x76F4806E+238]

INFO:werkzeug: * Detected change in 'C:\\kd\\ws_java\\team4_v2sbm3c\\src\\main\\python\\exc2.py', reloading
INFO:werkzeug: * Detected change in 'C:\\kd\\ws_java\\team4_v2sbm3c\\src\\main\\python\\exc2.py', reloading
INFO:werkzeug: * Detected change in 'C:\\kd\\ws_java\\team4_v2sbm3c\\src\\main\\python\\exc2.py', reloading
INFO:werkzeug: * Detected change in 'C:\\kd\\ws_java\\team4_v2sbm3c\\src\\main\\python\\exc2.py', reloading
INFO:werkzeug: * Detected change in 'C:\\kd\\ws_java\\team4_v2sbm3c\\src\\main\\python\\exc2.py', reloading
INFO:werkzeug: * Detected change in 'C:\\kd\\ws_java\\team4_v2sbm3c\\src\\main\\python\\exc2.py', reloading
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\i\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:WDM:Driver [C:\Users\i\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 569-178-434
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 569-178-434
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:05:28 KST)" (scheduled at 2025-01-14 19:05:08.975517+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:05:28 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 108, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 38, in load
    driver.get(url) # url 접속
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 347, in execute
    self.error_handler.check_response(response)
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: disconnected: not connected to DevTools
  (failed to check if window was closed: disconnected: not connected to DevTools)
  (Session info: chrome=131.0.6778.265)
Stacktrace:
	GetHandleVerifier [0x007BFD53+23747]
	(No symbol) [0x00747D54]
	(No symbol) [0x0061BE53]
	(No symbol) [0x0060C408]
	(No symbol) [0x0060C329]
	(No symbol) [0x0061E2E0]
	(No symbol) [0x0069AD69]
	(No symbol) [0x00681B96]
	(No symbol) [0x00653F3C]
	(No symbol) [0x00654EBD]
	GetHandleVerifier [0x00A9AC73+3017699]
	GetHandleVerifier [0x00AAB93B+3086507]
	GetHandleVerifier [0x00AA40F2+3055714]
	GetHandleVerifier [0x00855AF0+637536]
	(No symbol) [0x00750A5D]
	(No symbol) [0x0074DA28]
	(No symbol) [0x0074DBC5]
	(No symbol) [0x007407F0]
	BaseThreadInitThunk [0x754CFCC9+25]
	RtlGetAppContainerNamedObjectPath [0x76F4809E+286]
	RtlGetAppContainerNamedObjectPath [0x76F4806E+238]

INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:05:30 KST)" (scheduled at 2025-01-14 19:05:10.966502+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:05:30 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 108, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 38, in load
    driver.get(url) # url 접속
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 347, in execute
    self.error_handler.check_response(response)
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: disconnected: not connected to DevTools
  (failed to check if window was closed: disconnected: not connected to DevTools)
  (Session info: chrome=131.0.6778.265)
Stacktrace:
	GetHandleVerifier [0x007BFD53+23747]
	(No symbol) [0x00747D54]
	(No symbol) [0x0061BE53]
	(No symbol) [0x0060C408]
	(No symbol) [0x0060C329]
	(No symbol) [0x0061E2E0]
	(No symbol) [0x0069AD69]
	(No symbol) [0x00681B96]
	(No symbol) [0x00653F3C]
	(No symbol) [0x00654EBD]
	GetHandleVerifier [0x00A9AC73+3017699]
	GetHandleVerifier [0x00AAB93B+3086507]
	GetHandleVerifier [0x00AA40F2+3055714]
	GetHandleVerifier [0x00855AF0+637536]
	(No symbol) [0x00750A5D]
	(No symbol) [0x0074DA28]
	(No symbol) [0x0074DBC5]
	(No symbol) [0x007407F0]
	BaseThreadInitThunk [0x754CFCC9+25]
	RtlGetAppContainerNamedObjectPath [0x76F4809E+286]
	RtlGetAppContainerNamedObjectPath [0x76F4806E+238]

INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:05:21 KST)" (scheduled at 2025-01-14 19:05:21.531972+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:05:41 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 108, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 38, in load
    driver.get(url) # url 접속
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 347, in execute
    self.error_handler.check_response(response)
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchWindowException: Message: no such window: target window already closed
from unknown error: web view not found
  (Session info: chrome=131.0.6778.265)
Stacktrace:
	GetHandleVerifier [0x007BFD53+23747]
	(No symbol) [0x00747D54]
	(No symbol) [0x0061BE53]
	(No symbol) [0x005FD91B]
	(No symbol) [0x00687EFF]
	(No symbol) [0x0069AD49]
	(No symbol) [0x00681B96]
	(No symbol) [0x00653F3C]
	(No symbol) [0x00654EBD]
	GetHandleVerifier [0x00A9AC73+3017699]
	GetHandleVerifier [0x00AAB93B+3086507]
	GetHandleVerifier [0x00AA40F2+3055714]
	GetHandleVerifier [0x00855AF0+637536]
	(No symbol) [0x00750A5D]
	(No symbol) [0x0074DA28]
	(No symbol) [0x0074DBC5]
	(No symbol) [0x007407F0]
	BaseThreadInitThunk [0x754CFCC9+25]
	RtlGetAppContainerNamedObjectPath [0x76F4809E+286]
	RtlGetAppContainerNamedObjectPath [0x76F4806E+238]

INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:05:21 KST)" (scheduled at 2025-01-14 19:05:21.620974+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:05:41 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 108, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 38, in load
    driver.get(url) # url 접속
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 347, in execute
    self.error_handler.check_response(response)
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchWindowException: Message: no such window: target window already closed
from unknown error: web view not found
  (Session info: chrome=131.0.6778.265)
Stacktrace:
	GetHandleVerifier [0x007BFD53+23747]
	(No symbol) [0x00747D54]
	(No symbol) [0x0061BE53]
	(No symbol) [0x005FD91B]
	(No symbol) [0x00687EFF]
	(No symbol) [0x0069AD49]
	(No symbol) [0x00681B96]
	(No symbol) [0x00653F3C]
	(No symbol) [0x00654EBD]
	GetHandleVerifier [0x00A9AC73+3017699]
	GetHandleVerifier [0x00AAB93B+3086507]
	GetHandleVerifier [0x00AA40F2+3055714]
	GetHandleVerifier [0x00855AF0+637536]
	(No symbol) [0x00750A5D]
	(No symbol) [0x0074DA28]
	(No symbol) [0x0074DBC5]
	(No symbol) [0x007407F0]
	BaseThreadInitThunk [0x754CFCC9+25]
	RtlGetAppContainerNamedObjectPath [0x76F4809E+286]
	RtlGetAppContainerNamedObjectPath [0x76F4806E+238]

INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\i\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
INFO:werkzeug:[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
INFO:werkzeug:[33mPress CTRL+C to quit[0m
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\i\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
>>>>>>> a6bdad380237258af82a182dad4608eac81e0f1f
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
<<<<<<< HEAD
INFO:werkzeug: * Debugger PIN: 524-771-621
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F8091AE0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/c09b198de9cf0751cb997bc8dc24cabf/url
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:46:39 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 975, in send
    self.connect()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000002C9F8090C40>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\soldesk\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=55482): Max retries exceeded with url: /session/c09b198de9cf0751cb997bc8dc24cabf/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C9F8090C40>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:47:15 KST)" (scheduled at 2025-01-15 16:46:15.152624+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:47:15 KST)" executed successfully
=======
INFO:werkzeug: * Debugger PIN: 569-178-434
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:19:11 KST)" (scheduled at 2025-01-14 19:19:11.045759+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:19:15 KST)" (scheduled at 2025-01-14 19:19:15.907193+09:00)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:00:20], next run at: 2025-01-14 19:19:35 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 108, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 38, in load
    driver.get(url) # url 접속
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 347, in execute
    self.error_handler.check_response(response)
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchWindowException: Message: no such window: target window already closed
from unknown error: web view not found
  (Session info: chrome=131.0.6778.265)
Stacktrace:
	GetHandleVerifier [0x007BFD53+23747]
	(No symbol) [0x00747D54]
	(No symbol) [0x0061BE53]
	(No symbol) [0x005FD91B]
	(No symbol) [0x00687EFF]
	(No symbol) [0x0069AD49]
	(No symbol) [0x00681B96]
	(No symbol) [0x00653F3C]
	(No symbol) [0x00654EBD]
	GetHandleVerifier [0x00A9AC73+3017699]
	GetHandleVerifier [0x00AAB93B+3086507]
	GetHandleVerifier [0x00AA40F2+3055714]
	GetHandleVerifier [0x00855AF0+637536]
	(No symbol) [0x00750A5D]
	(No symbol) [0x0074DA28]
	(No symbol) [0x0074DBC5]
	(No symbol) [0x007407F0]
	BaseThreadInitThunk [0x754CFCC9+25]
	RtlGetAppContainerNamedObjectPath [0x76F4809E+286]
	RtlGetAppContainerNamedObjectPath [0x76F4806E+238]

>>>>>>> a6bdad380237258af82a182dad4608eac81e0f1f
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:WebDriver version 131.0.6778.264 selected
INFO:WDM:Modern chrome version https://storage.googleapis.com/chrome-for-testing-public/131.0.6778.264/win32/chromedriver-win32.zip
INFO:WDM:About to download new driver from https://storage.googleapis.com/chrome-for-testing-public/131.0.6778.264/win32/chromedriver-win32.zip
INFO:WDM:Driver downloading response is 200
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver has been saved in cache [C:\Users\i\.wdm\drivers\chromedriver\win64\131.0.6778.264]
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
INFO:werkzeug:[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
INFO:werkzeug:[33mPress CTRL+C to quit[0m
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\i\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 569-178-434
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:54:40 KST)" (scheduled at 2025-01-15 16:54:40.056877+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:54:45 KST)" (scheduled at 2025-01-15 16:54:45.232364+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:55:40 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:55:45 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:56:40 KST)" (scheduled at 2025-01-15 16:55:40.056877+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:56:40 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:56:45 KST)" (scheduled at 2025-01-15 16:55:45.232364+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:56:45 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:57:40 KST)" (scheduled at 2025-01-15 16:56:40.056877+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:57:40 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:57:45 KST)" (scheduled at 2025-01-15 16:56:45.232364+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:57:45 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:58:40 KST)" (scheduled at 2025-01-15 16:57:40.056877+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:58:40 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:58:45 KST)" (scheduled at 2025-01-15 16:57:45.232364+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:58:45 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:59:40 KST)" (scheduled at 2025-01-15 16:58:40.056877+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:59:40 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:59:45 KST)" (scheduled at 2025-01-15 16:58:45.232364+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:59:45 KST)" executed successfully
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\i\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
INFO:werkzeug:[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
INFO:werkzeug:[33mPress CTRL+C to quit[0m
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\i\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 569-178-434
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\i\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
INFO:werkzeug:[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
INFO:werkzeug:[33mPress CTRL+C to quit[0m
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\i\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:WDM:====== WebDriver manager ======
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
INFO:werkzeug:[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
INFO:werkzeug:[33mPress CTRL+C to quit[0m
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\i\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 569-178-434
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\i\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 569-178-434
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:00:40 KST)" (scheduled at 2025-01-15 16:59:40.056877+09:00)
INFO:WDM:====== WebDriver manager ======
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:00:45 KST)" (scheduled at 2025-01-15 16:59:45.232364+09:00)
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\i\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:00:40 KST)" executed successfully
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
INFO:werkzeug:[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
INFO:werkzeug:[33mPress CTRL+C to quit[0m
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:00:45 KST)" executed successfully
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\i\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 16:59:54 KST)" (scheduled at 2025-01-15 16:59:54.741488+09:00)
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 569-178-434
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:00:54 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:00:00 KST)" (scheduled at 2025-01-15 17:00:00.484357+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:01:00 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:00:26 KST)" (scheduled at 2025-01-15 17:00:26.604499+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:01:26 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:00:30 KST)" (scheduled at 2025-01-15 17:00:30.738027+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:00:32 KST)" (scheduled at 2025-01-15 17:00:32.594516+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:00:36 KST)" (scheduled at 2025-01-15 17:00:36.738369+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:01:40 KST)" (scheduled at 2025-01-15 17:00:40.056877+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:01:45 KST)" (scheduled at 2025-01-15 17:00:45.232364+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:01:40 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:00:48 KST)" (scheduled at 2025-01-15 17:00:48.987824+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:01:45 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:01:54 KST)" (scheduled at 2025-01-15 17:00:54.741488+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:00:55 KST)" (scheduled at 2025-01-15 17:00:55.155013+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:01:36 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:02:00 KST)" (scheduled at 2025-01-15 17:01:00.484357+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:01:54 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:02:00 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:02:26 KST)" (scheduled at 2025-01-15 17:01:26.604499+09:00)
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:01:30 KST)" skipped: maximum number of running instances reached (1)
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:01:32 KST)" skipped: maximum number of running instances reached (1)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:02:26 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:02:36 KST)" (scheduled at 2025-01-15 17:01:36.738369+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:02:40 KST)" (scheduled at 2025-01-15 17:01:40.056877+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:01:55 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:02:36 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:02:45 KST)" (scheduled at 2025-01-15 17:01:45.232364+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:02:40 KST)" executed successfully
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:01:48 KST)" skipped: maximum number of running instances reached (1)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:02:32 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:02:45 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:02:30 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:02:54 KST)" (scheduled at 2025-01-15 17:01:54.741488+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:02:48 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:02:55 KST)" (scheduled at 2025-01-15 17:01:55.155013+09:00)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:03:00 KST)" (scheduled at 2025-01-15 17:02:00.484357+09:00)
INFO:WDM:====== WebDriver manager ======
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:02:55 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:02:54 KST)" executed successfully
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\i\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
INFO:werkzeug:[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
INFO:werkzeug:[33mPress CTRL+C to quit[0m
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:03:00 KST)" executed successfully
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\i\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 569-178-434
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:03:26 KST)" (scheduled at 2025-01-15 17:02:26.604499+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:03:26 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:03:30 KST)" (scheduled at 2025-01-15 17:02:30.738027+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:03:32 KST)" (scheduled at 2025-01-15 17:02:32.594516+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:03:36 KST)" (scheduled at 2025-01-15 17:02:36.738369+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:03:40 KST)" (scheduled at 2025-01-15 17:02:40.056877+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:03:30 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:03:36 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:03:32 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:03:45 KST)" (scheduled at 2025-01-15 17:02:45.232364+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:03:40 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:03:48 KST)" (scheduled at 2025-01-15 17:02:48.987824+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:03:45 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:03:48 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:03:54 KST)" (scheduled at 2025-01-15 17:02:54.741488+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:03:55 KST)" (scheduled at 2025-01-15 17:02:55.155013+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:04:00 KST)" (scheduled at 2025-01-15 17:03:00.484357+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:03:54 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:03:55 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:04:00 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:03:07 KST)" (scheduled at 2025-01-15 17:03:07.825891+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:04:07 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:03:14 KST)" (scheduled at 2025-01-15 17:03:14.428325+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:04:14 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:04:26 KST)" (scheduled at 2025-01-15 17:03:26.604499+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:04:30 KST)" (scheduled at 2025-01-15 17:03:30.738027+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:04:32 KST)" (scheduled at 2025-01-15 17:03:32.594516+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:04:26 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:04:36 KST)" (scheduled at 2025-01-15 17:03:36.738369+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:04:40 KST)" (scheduled at 2025-01-15 17:03:40.056877+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:04:32 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:04:45 KST)" (scheduled at 2025-01-15 17:03:45.232364+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:04:48 KST)" (scheduled at 2025-01-15 17:03:48.987824+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:04:30 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:04:40 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:04:36 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:04:54 KST)" (scheduled at 2025-01-15 17:03:54.741488+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:04:55 KST)" (scheduled at 2025-01-15 17:03:55.155013+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:05:00 KST)" (scheduled at 2025-01-15 17:04:00.484357+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:05:07 KST)" (scheduled at 2025-01-15 17:04:07.825891+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:05:00 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:05:14 KST)" (scheduled at 2025-01-15 17:04:14.428325+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:04:45 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:04:48 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:05:26 KST)" (scheduled at 2025-01-15 17:04:26.604499+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:05:30 KST)" (scheduled at 2025-01-15 17:04:30.738027+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:05:32 KST)" (scheduled at 2025-01-15 17:04:32.594516+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:04:54 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:05:36 KST)" (scheduled at 2025-01-15 17:04:36.738369+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:05:26 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:05:40 KST)" (scheduled at 2025-01-15 17:04:40.056877+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:04:55 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:05:45 KST)" (scheduled at 2025-01-15 17:04:45.232364+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:05:30 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:05:36 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:05:14 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:05:40 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:05:32 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:05:48 KST)" (scheduled at 2025-01-15 17:04:48.987824+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:05:54 KST)" (scheduled at 2025-01-15 17:04:54.741488+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:05:55 KST)" (scheduled at 2025-01-15 17:04:55.155013+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:06:00 KST)" (scheduled at 2025-01-15 17:05:00.484357+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:05:48 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:05:54 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:05:55 KST)" executed successfully
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:05:07 KST)" skipped: maximum number of running instances reached (1)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:06:00 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:05:45 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:06:07 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:06:14 KST)" (scheduled at 2025-01-15 17:05:14.428325+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:06:14 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:06:26 KST)" (scheduled at 2025-01-15 17:05:26.604499+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:06:26 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:06:30 KST)" (scheduled at 2025-01-15 17:05:30.738027+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:06:32 KST)" (scheduled at 2025-01-15 17:05:32.594516+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:06:30 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:06:36 KST)" (scheduled at 2025-01-15 17:05:36.738369+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:06:32 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:06:40 KST)" (scheduled at 2025-01-15 17:05:40.056877+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:06:36 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:06:40 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:06:45 KST)" (scheduled at 2025-01-15 17:05:45.232364+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:06:48 KST)" (scheduled at 2025-01-15 17:05:48.987824+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:06:48 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:06:45 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:06:54 KST)" (scheduled at 2025-01-15 17:05:54.741488+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:06:55 KST)" (scheduled at 2025-01-15 17:05:55.155013+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:07:00 KST)" (scheduled at 2025-01-15 17:06:00.484357+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:06:55 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:06:54 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:07:00 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:07:07 KST)" (scheduled at 2025-01-15 17:06:07.825891+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:07:07 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:07:14 KST)" (scheduled at 2025-01-15 17:06:14.428325+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:07:14 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:07:26 KST)" (scheduled at 2025-01-15 17:06:26.604499+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:07:26 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:07:30 KST)" (scheduled at 2025-01-15 17:06:30.738027+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:07:32 KST)" (scheduled at 2025-01-15 17:06:32.594516+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:07:30 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:07:36 KST)" (scheduled at 2025-01-15 17:06:36.738369+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:07:32 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:07:40 KST)" (scheduled at 2025-01-15 17:06:40.056877+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:07:36 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:07:40 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:07:45 KST)" (scheduled at 2025-01-15 17:06:45.232364+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:07:48 KST)" (scheduled at 2025-01-15 17:06:48.987824+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:07:45 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:07:48 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:07:54 KST)" (scheduled at 2025-01-15 17:06:54.741488+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:07:55 KST)" (scheduled at 2025-01-15 17:06:55.155013+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:07:55 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:07:54 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:08:00 KST)" (scheduled at 2025-01-15 17:07:00.484357+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:08:00 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:08:07 KST)" (scheduled at 2025-01-15 17:07:07.825891+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:08:07 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:08:14 KST)" (scheduled at 2025-01-15 17:07:14.428325+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:08:14 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:08:26 KST)" (scheduled at 2025-01-15 17:07:26.604499+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:08:30 KST)" (scheduled at 2025-01-15 17:07:30.738027+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:08:32 KST)" (scheduled at 2025-01-15 17:07:32.594516+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:08:26 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:08:30 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:08:36 KST)" (scheduled at 2025-01-15 17:07:36.738369+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:08:32 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:08:40 KST)" (scheduled at 2025-01-15 17:07:40.056877+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:08:36 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:08:45 KST)" (scheduled at 2025-01-15 17:07:45.232364+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:08:40 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:08:48 KST)" (scheduled at 2025-01-15 17:07:48.987824+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:08:45 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:08:48 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:08:54 KST)" (scheduled at 2025-01-15 17:07:54.741488+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:08:55 KST)" (scheduled at 2025-01-15 17:07:55.155013+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:09:00 KST)" (scheduled at 2025-01-15 17:08:00.484357+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:08:55 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:08:54 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:09:00 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:09:07 KST)" (scheduled at 2025-01-15 17:08:07.825891+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:09:07 KST)" executed successfully
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:09:14 KST)" (scheduled at 2025-01-15 17:08:14.428325+09:00)
INFO:WDM:====== WebDriver manager ======
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:09:14 KST)" executed successfully
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\i\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
INFO:werkzeug:[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
INFO:werkzeug:[33mPress CTRL+C to quit[0m
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:09:26 KST)" (scheduled at 2025-01-15 17:08:26.604499+09:00)
INFO:WDM:====== WebDriver manager ======
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:09:26 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:09:30 KST)" (scheduled at 2025-01-15 17:08:30.738027+09:00)
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\i\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:09:32 KST)" (scheduled at 2025-01-15 17:08:32.594516+09:00)
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 569-178-434
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:09:30 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:09:36 KST)" (scheduled at 2025-01-15 17:08:36.738369+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:09:40 KST)" (scheduled at 2025-01-15 17:08:40.056877+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:09:45 KST)" (scheduled at 2025-01-15 17:08:45.232364+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:09:32 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:09:36 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:09:48 KST)" (scheduled at 2025-01-15 17:08:48.987824+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:09:45 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:09:54 KST)" (scheduled at 2025-01-15 17:08:54.741488+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:09:55 KST)" (scheduled at 2025-01-15 17:08:55.155013+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:09:48 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:10:00 KST)" (scheduled at 2025-01-15 17:09:00.484357+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:09:55 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:09:40 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:09:54 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:10:07 KST)" (scheduled at 2025-01-15 17:09:07.825891+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:10:00 KST)" executed successfully
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:10:14 KST)" (scheduled at 2025-01-15 17:09:14.428325+09:00)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:10:07 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:10:14 KST)" executed successfully
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\i\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:09:24 KST)" (scheduled at 2025-01-15 17:09:24.406309+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:10:26 KST)" (scheduled at 2025-01-15 17:09:26.604499+09:00)
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
INFO:werkzeug:[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
INFO:werkzeug:[33mPress CTRL+C to quit[0m
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:10:30 KST)" (scheduled at 2025-01-15 17:09:30.738027+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:10:32 KST)" (scheduled at 2025-01-15 17:09:32.594516+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:09:34 KST)" (scheduled at 2025-01-15 17:09:34.341309+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:10:36 KST)" (scheduled at 2025-01-15 17:09:36.738369+09:00)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:10:40 KST)" (scheduled at 2025-01-15 17:09:40.056877+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:10:45 KST)" (scheduled at 2025-01-15 17:09:45.232364+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:10:48 KST)" (scheduled at 2025-01-15 17:09:48.987824+09:00)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:10:54 KST)" (scheduled at 2025-01-15 17:09:54.741488+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:10:55 KST)" (scheduled at 2025-01-15 17:09:55.155013+09:00)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:11:00 KST)" (scheduled at 2025-01-15 17:10:00.484357+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:10:55 KST)" executed successfully
INFO:werkzeug: * Detected change in 'C:\\Users\\i\\AppData\\Roaming\\Python\\Python310\\site-packages\\apscheduler\\executors\\base.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\i\\AppData\\Roaming\\Python\\Python310\\site-packages\\apscheduler\\executors\\base.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\i\\AppData\\Roaming\\Python\\Python310\\site-packages\\apscheduler\\executors\\base.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\i\\AppData\\Roaming\\Python\\Python310\\site-packages\\apscheduler\\executors\\base.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\i\\AppData\\Roaming\\Python\\Python310\\site-packages\\apscheduler\\executors\\base.py', reloading
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:10:34 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 126, in crawl_and_insert_data
    url = driver.current_url.strip()
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 437, in current_url
    return self.execute(Command.GET_CURRENT_URL)["value"]
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 347, in execute
    self.error_handler.check_response(response)
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: target frame detached: received Inspector.detached event
  (Session info: chrome=131.0.6778.265)
Stacktrace:
	GetHandleVerifier [0x00A3FD53+23747]
	(No symbol) [0x009C7D54]
	(No symbol) [0x0089BE53]
	(No symbol) [0x0088E998]
	(No symbol) [0x0088DBA7]
	(No symbol) [0x0088D503]
	(No symbol) [0x0088D43D]
	(No symbol) [0x0088B8BB]
	(No symbol) [0x0088BF2D]
	(No symbol) [0x008981BA]
	(No symbol) [0x008A78B5]
	(No symbol) [0x008AC726]
	(No symbol) [0x0088C535]
	(No symbol) [0x008A74E1]
	(No symbol) [0x0091B33D]
	(No symbol) [0x00901B96]
	(No symbol) [0x008D3F3C]
	(No symbol) [0x008D4EBD]
	GetHandleVerifier [0x00D1AC73+3017699]
	GetHandleVerifier [0x00D2B93B+3086507]
	GetHandleVerifier [0x00D240F2+3055714]
	GetHandleVerifier [0x00AD5AF0+637536]
	(No symbol) [0x009D0A5D]
	(No symbol) [0x009CDA28]
	(No symbol) [0x009CDBC5]
	(No symbol) [0x009C07F0]
	BaseThreadInitThunk [0x754CFCC9+25]
	RtlGetAppContainerNamedObjectPath [0x76F4809E+286]
	RtlGetAppContainerNamedObjectPath [0x76F4806E+238]

INFO:werkzeug: * Detected change in 'C:\\Users\\i\\AppData\\Roaming\\Python\\Python310\\site-packages\\apscheduler\\executors\\base.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\i\\AppData\\Roaming\\Python\\Python310\\site-packages\\selenium\\webdriver\\remote\\webdriver.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\i\\AppData\\Roaming\\Python\\Python310\\site-packages\\selenium\\webdriver\\remote\\webdriver.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\i\\AppData\\Roaming\\Python\\Python310\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\i\\AppData\\Roaming\\Python\\Python310\\site-packages\\selenium\\webdriver\\remote\\webdriver.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\i\\AppData\\Roaming\\Python\\Python310\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\i\\AppData\\Roaming\\Python\\Python310\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\i\\AppData\\Roaming\\Python\\Python310\\site-packages\\selenium\\webdriver\\remote\\webdriver.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\i\\AppData\\Roaming\\Python\\Python310\\site-packages\\selenium\\webdriver\\remote\\webdriver.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\i\\AppData\\Roaming\\Python\\Python310\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\i\\AppData\\Roaming\\Python\\Python310\\site-packages\\selenium\\webdriver\\remote\\webdriver.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\i\\AppData\\Roaming\\Python\\Python310\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\i\\AppData\\Roaming\\Python\\Python310\\site-packages\\selenium\\webdriver\\remote\\webdriver.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\i\\AppData\\Roaming\\Python\\Python310\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\i\\AppData\\Roaming\\Python\\Python310\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py', reloading
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:11:07 KST)" (scheduled at 2025-01-15 17:10:07.825891+09:00)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\i\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\i\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:WDM:====== WebDriver manager ======
INFO:WDM:====== WebDriver manager ======
INFO:WDM:====== WebDriver manager ======
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\i\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\i\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\i\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:10:30 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:10:36 KST)" executed successfully
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:10:24 KST)" skipped: maximum number of running instances reached (1)
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:10:26 KST)" skipped: maximum number of running instances reached (1)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:11:28 KST)" (scheduled at 2025-01-15 17:10:28.160312+09:00)
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 569-178-434
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 569-178-434
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:11:26 KST)" executed successfully
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:10:45 KST)" executed successfully
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:11:30 KST)" (scheduled at 2025-01-15 17:10:30.738027+09:00)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:10:32 KST)" skipped: maximum number of running instances reached (1)
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:11:00 KST)" raised an exception
Traceback (most recent call last):
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 126, in crawl_and_insert_data
    url = driver.current_url.strip()
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 437, in current_url
    return self.execute(Command.GET_CURRENT_URL)["value"]
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 347, in execute
    self.error_handler.check_response(response)
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: target frame detached: received Inspector.detached event
  (Session info: chrome=131.0.6778.265)
Stacktrace:
	GetHandleVerifier [0x00A3FD53+23747]
	(No symbol) [0x009C7D54]
	(No symbol) [0x0089BE53]
	(No symbol) [0x0088E998]
	(No symbol) [0x0088DBA7]
	(No symbol) [0x0088D503]
	(No symbol) [0x0088D43D]
	(No symbol) [0x0088B8BB]
	(No symbol) [0x0088BF2D]
	(No symbol) [0x008981BA]
	(No symbol) [0x008A78B5]
	(No symbol) [0x008AC726]
	(No symbol) [0x0088C535]
	(No symbol) [0x008A74E1]
	(No symbol) [0x0091B33D]
	(No symbol) [0x00901B96]
	(No symbol) [0x008D3F3C]
	(No symbol) [0x008D4EBD]
	GetHandleVerifier [0x00D1AC73+3017699]
	GetHandleVerifier [0x00D2B93B+3086507]
	GetHandleVerifier [0x00D240F2+3055714]
	GetHandleVerifier [0x00AD5AF0+637536]
	(No symbol) [0x009D0A5D]
	(No symbol) [0x009CDA28]
	(No symbol) [0x009CDBC5]
	(No symbol) [0x009C07F0]
	BaseThreadInitThunk [0x754CFCC9+25]
	RtlGetAppContainerNamedObjectPath [0x76F4809E+286]
	RtlGetAppContainerNamedObjectPath [0x76F4806E+238]

INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\i\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:WDM:====== WebDriver manager ======
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:11:30 KST)" executed successfully
INFO:WDM:====== WebDriver manager ======
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:10:48 KST)" executed successfully
WARNING:apscheduler.scheduler:Execution of job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:10:40 KST)" skipped: maximum number of running instances reached (1)
INFO:werkzeug: * Detected change in 'C:\\Users\\i\\AppData\\Roaming\\Python\\Python310\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py', reloading
INFO:werkzeug: * Detected change in 'C:\\Users\\i\\AppData\\Roaming\\Python\\Python310\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py', reloading
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\urllib3\\request.py', reloading
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\urllib3\\request.py', reloading
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\urllib3\\poolmanager.py', reloading
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\urllib3\\poolmanager.py', reloading
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py', reloading
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\urllib3\\util\\retry.py', reloading
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\urllib3\\packages\\six.py', reloading
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\urllib3\\packages\\six.py', reloading
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\http\\client.py', reloading
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\http\\client.py', reloading
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\socket.py', reloading
ERROR:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:11:28 KST)" raised an exception
Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 449, in _make_request
    six.raise_from(e, None)
  File "<string>", line 3, in raise_from
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 444, in _make_request
    httplib_response = conn.getresponse()
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1374, in getresponse
    response.begin()
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 318, in begin
    version, status, reason = self._read_status()
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 279, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "C:\ProgramData\Anaconda3\lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [WinError 10054] 현재 연결은 원격 호스트에 의해 강제로 끊겼습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 105, in crawl_and_insert_data
    load('https://www.bbc.com//', 1)
  File "C:\kd\ws_java\team4_v2sbm3c\src\main\python\crawling.py", line 37, in load
    driver.get(url) # url 접속
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 356, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\webdriver.py", line 345, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 302, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\i\AppData\Roaming\Python\Python310\site-packages\selenium\webdriver\remote\remote_connection.py", line 322, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 550, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\packages\six.py", line 769, in reraise
    raise value.with_traceback(tb)
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 449, in _make_request
    six.raise_from(e, None)
  File "<string>", line 3, in raise_from
  File "C:\ProgramData\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 444, in _make_request
    httplib_response = conn.getresponse()
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 1374, in getresponse
    response.begin()
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 318, in begin
    version, status, reason = self._read_status()
  File "C:\ProgramData\Anaconda3\lib\http\client.py", line 279, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "C:\ProgramData\Anaconda3\lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
urllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(10054, '현재 연결은 원격 호스트에 의해 강제로 끊겼습니다', None, 10054, None))
INFO:werkzeug: * Detected change in 'C:\\ProgramData\\Anaconda3\\Lib\\socket.py', reloading
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:werkzeug: * Restarting with watchdog (windowsapi)
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts
INFO:apscheduler.scheduler:Added job "crawl_and_insert_data" to job store "default"
INFO:apscheduler.scheduler:Scheduler started
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 569-178-434
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000179F82A3DC0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/6052896cbb80a7be4e12ca11c240f513/element
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\i\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Get LATEST chromedriver version for google-chrome
INFO:WDM:Driver [C:\Users\i\.wdm\drivers\chromedriver\win64\131.0.6778.264\chromedriver-win32/THIRD_PARTY_NOTICES.chromedriver] found in cache
INFO:WDM:====== WebDriver manager ======
INFO:WDM:====== WebDriver manager ======
INFO:apscheduler.executors.default:Running job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:11:48 KST)" (scheduled at 2025-01-15 17:10:48.987824+09:00)
INFO:apscheduler.executors.default:Job "crawl_and_insert_data (trigger: interval[0:01:00], next run at: 2025-01-15 17:11:07 KST)" executed successfully
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000179F82A3E80>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다')': /session/6052896cbb80a7be4e12ca11c240f513/element
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
INFO:numexpr.utils:NumExpr defaulting to 4 threads.
